# RLHF

## 前言

最近DeepMind的大模型ChatGPT火遍全网，其主要原因是InstructGPT，来自论文《Aligning Language Models to Follow Instructions》，它比传统的大模型方法GPT-3，GPT-2有一些重要改进。InstructGPT的主要改进点是引入了RLHF,来自另外一篇论文《Deep Reinforcement Learning from Human Preferences》。把难以定义的Reward函数替换为人类反馈（human feedback）,本质是一种弱监督学习。

以下是论文《Deep Reinforcement Learning from Human Preferences》翻译：

## 摘要

对于复杂RL系统，需要设定复杂的目标。在本论文中，我们探索了非专家反馈（HF），结果表明无需Reward函数（屏蔽了Atari游戏的奖励函数接口），交互次数降低到1%，仍然可以有效解决复杂的RL问题。因为引入的是非专家反馈，可以大大节约算法成本，是目前的SOTA。为了证明方法有效性，我们尝试在1小时内训练出全新的复杂RL目标。

### 一、 简介

强化学习取得了一些进展，但是要明确的奖励，假如我们要训练机器人清洁桌子或者炒鸡蛋，目前没有方法构建Reward函数，如果我们能想一种方法把我们的实际目标传递给Agent，这将是重要的一步。学术界有两种办法：Inverse RL或者Imatation RL。但是复杂目标还是很难做到。

还有一种方法就是使用HF，但如果直接把HF当作Reward函数来用，训练成本会非常高（数百或者数千小时的人类经验）。我们的方法是从HF中学习Reward函数并且迭代优化这个函数，任务是：

1. 某些无法演示（demonstrate）的RL
1. 允许非专家HF
1. 大RL问题
1. HF成本要经济

我们让非专家比较Agent的短视频片段，而不是打分。因为我们发现人类比较Agent短视频比判别states更有效率。论文作者在两个环境中做了实验：ALE for atari、 MuJoCo。尽管被实验者不会写Reward函数，但是他们通过一小时左右的HF就可以训练出Reward函数。

#### 1.1 相关工作

略

### 二、先决条件和方法

#### 2.1 设定和目标

假定Agent和环境交互了多步：在时间t，Agent接收到了观察ot属于O，采取了动作at属于A。

传统RL，环境需要给出奖励函数rt属于R，Agent的目标是最大化累积奖励函数Rt。不同于依赖环境提供奖励信号，我们假定有一个观察者对这几步交互片段（轨迹片段）进行反馈。通俗讲，Agent的目标是创造观者者倾向的轨迹，同时尽量少向观察者提问（queries）。

轨迹片段：一系列的o和a，$σ=（（o_0,a_0）,（o_1,a_1）, … ,（o_{k-1},a_{k-1}）,）$ 属于 $（ O x A ）^k$

$σ^1 > σ^2$ 代表观察者倾向 $σ^1$ 到 $σ^2$ 的轨迹片段

更精确的定性和定量的表达如下：

略（截图）

定性：有时候无法提供定量评估函数，我们用定性的人类偏好，在本论文中我们用提问的方式询问：Agent实现目标的几个视频的HF。

#### 2.2 我们的方法
在任意时刻我们维护一个策略π ： O –> A 和一个奖励函数r^: O x A –> R,两个函数的参数都用神经网络训练。

网络参数如下更新：

1. 策略π和环境交互生成序列{τ1,…, τi},其中π的参数用传统RL训练，最大化r的sum
1. 从上一步序列{τ1,…, τi}中截取一个片段（σ1，σ2），让人类评估
1. r^的参数由HF反馈做监督学习

这些过程都是异步的：轨迹流p1 –> p2, 人类反馈p2 –> p3, 奖励函数r^从p3 –> p1，下面部分会详细说明。

##### 2.2.1 优化策略函数

使用r^来计算奖励后，我们只剩一个传统的RL问题，用任何RL算法都行，一个细节是r^是非稳定函数，因此我们倾向用稳定的PG方法，这篇文章中我们用了A2C玩Atari游戏，用了TRPO玩MuJoCo。我们使用了传统RL常用的超参数，唯一修改的是TRPO算法的熵奖励。因为TRPO依靠信任区域确保充足的探索，而我们这里改变了奖励函数。

我们把奖励的norm的方差设置为0，标准差设置为常量。这是一个常见的处理方法，因为我们的奖励position（？？）不确定。

##### 2.2.2 生成HF

给人播放两个1～2秒的片段，人指出哪个片段更好，同样好，无法判断。这个三元判断会记录在数据库中（σ1，σ2，μ），其中σ1 和σ2是两个片段，μ是f1上的分布。2g代表人类偏好哪个片段，如果人类选其中一个片段，那么μ的分布重心就落在该片段上，如果人类标记同样好，则μ均匀分布，如果标记无法判断，就不存数据库。

##### 2.2.3 适配奖励函数

我们将r^视为隐含因子，用下面公式所示指数和的形式表示：

略（截图）

用r^来最小化交叉熵顺势函数

略（截图）

类似国际象棋中的Elo等级评分系统

针对第3.3节的实际情况，我们对过去基础的方法做了有用的修改：

1. 从数据库总采样三元组，通过norm方法和平均值来估计r^
1. 拿出一部分数据做验证集，使用L2正则保证验证loss比训练loss差距不大（1.1~1.5倍范围内），有些情况下我们还用了dropout
1. 先不直接在等式1上做softmax，我们假定HF有10%的选择是随机的（人类判断不是100%可靠）

##### 2.2.4 选择查询

我们基于奖励函数估计的不确定性来查询HF，我们采样大量轨迹，并使用奖励函数评估哪个片段更倾向。然后选择评估出来差异最大的轨迹。这个方法比较粗，有些情况（第三节）会影响性能。理想情况我们希望查询是基于信息的期望(论文 Akrour et al. 2012; Krueger et al. 2016)。

### 附录

代码：[2]

## 测验与习题

1. 人类反馈强化学习如何用来训练GPT？

[1]: http://www.abilitygame.cn/2022/12/12/rlhf-%E4%B8%80%E7%A7%8D%E5%BC%B1%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95/
[2]: https://github.com/HarderThenHarder/transformers_tasks/tree/main/RLHF
