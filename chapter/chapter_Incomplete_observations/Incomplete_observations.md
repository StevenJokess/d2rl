

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-03-22 01:27:06
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-04-01 03:06:37
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 对状态的不完全观测

之前章节中的 DQN $Q(s, a ; \boldsymbol{w})$, 策略网络 $\pi(a \mid s ; \boldsymbol{\theta}) 、 \boldsymbol{\mu}(s ; \boldsymbol{\theta})$, 价值网络 $q(s, a ; \boldsymbol{w})$ 、 $v(s ; \boldsymbol{w})$ 都需要把当前状态 $s$ 作为输入。之前我们一直假设可以完全枧测到状态 $s$; 在围棋、象棋、五子棋等简单的游戏中，棋盘上当前的格局就是完整的状态, 符合完全观测的假设。但是在很多实际应用中, 完全观测假设往往不符合实际。比如在星际争霸、英雄联盟等电子游戏中, 屏幕上当前的画面并不能完整反映出游戏的状态, 因为观测只是地图的一小部分; 甚至最近的 100 帧也无法反映出游戏真实的状态。

把 $t$ 时刻的状态记作 $s_t$, 把观测记作 $o_t$ 。观测 $o_t$ 可以是当前游戏屏幕上的画面, 也 可以是最近 100 帧出面。我们无法用 $\pi\left(a_t \mid s_t ; \boldsymbol{\theta}\right)$ 做决策, 因为我们不知道 $s_t$ 。最简单的 解决仦法就是用当前炑测 $o_t$ 代替状态 $s_t$, 用 $\pi\left(a_t \mid o_t ; \boldsymbol{\theta}\right)$ 做决策。同理, 对于 DQN 和价 值网络, 也用 $o_t$ 代替 $s_t$ 。虽然这种简单的方法可行, 但是效果恐怕不好。

图11.1: 在迷宫问题中，智能体可能知道迷宫的整体格局，也可能只知道自己附近的格局。

图11.1 的例子是让智能体走迷宫。图11.1(a) 中智能体可以完整观测到迷宫s；这种问题最容易解决。图11.1(b) 中智能体只能观测到自身附近一小块区域 $o_t$，这属于不完全观测问题，这种问题较难解决。如果仅仅靠当前观测 $o_t$ 做决策，智能体做出的决策是非常盲目的，很难走出迷宫。

---

真实的生物天天生活在非完全信息环境下， 到处都是刚刚图中的灰色方格。 它们还得学到正确的行为， 那么它们是如何解决这个问题呢？

主要有三种方法：一种就是策略梯度的方法，虽然所知状态和信息是不全面的，我们可以利用概率的方法来学习。当不知道该往左走还是该往右走时，随便走出一步，这样有百分之五十的概率得到最后要的奖励（一个驴子在沙漠里， 怎么都要选择一个方向走，不走一定渴死），利用直接学习的策略函数也就是Policy Gradient解决掉这个问题。这个方法大家可以看到效率是比较低的， 如果时间有限那就死了。

---

一种更合理的办法是让智能体记住过去的观测，这样就能对状态的观测越来越完整，做出越来越理性的决策；如图11.1(c) 所示。对于不完全观测的强化学习问题, 应当记忆过去的枧测, 用所有已知的信息做决策。 这正是人类解决不完全观测问题的方式。对于星际争霸、扑克牌、林将等不完全枧测的 游戏, 人类玩家也需要记忆; 人类玩家的决策不止依赖于当前时刻的枧测 $o_t$, 而是依赖于过去所有的观测 $o_1, \cdots, o_t$ 。把从初始到 $t$ 时刻为止的所有观测记作:

$$
o_{1: t}=\left[o_1, o_2, \cdots, o_t\right]
$$

可以用 $o_{1: t}$ 代替状态 $s$, 作为策略网络的输入 那么策略网络就记作：

$$
\pi\left(a_t \mid \boldsymbol{o}_{1: t} ; \boldsymbol{\theta}\right)
$$

该如何实现这样一个策略网络呢? 请注意, $o_{1: t}$ 的大小是变化的。如果 $o_1, \cdots, o_t$ 都是 $d \times 1$ 的向量, 那么 $o_{1: t}$ 是 $d \times t$ 的矩阵或 $d t \times 1$ 的向量, 它的大小随 $t$ 增长。卷积层和全连接层都要求输入大小固定, 因此不能简单地用卷积层和全连接层实现策略网络。一种可行的妙法是将卷积层、全连接层与循环层结合, 这样就能处理不固定长度的输入。

[1]: https://www.math.pku.edu.cn/teachers/zhzhang/drl_v1.pdf
[2]: https://zhuanlan.zhihu.com/p/41464298
