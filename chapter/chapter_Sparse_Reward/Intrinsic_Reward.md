# 一般强化学习问题

多臂赌博机作为简化版的强化学习环境, 叮以为探索与利用问题的研 究提供更简单和清晰的形式。有一些在多臂赌博机中得到研究的算法也可 以应用到一般的强化学习问题上来。例如 $\epsilon$-greedy, 基于 UCB 的 UCT 算法等。强化学习问题是一个马尔科夫链 (MDP), 较单一状态下多次进行动作选择的多臂赌博机问题相比, 强化学习要学习的是一个最优映射, 可以 从不同状态中选择最优的动作, 并使得这样的状态动作序列以获得更高的回报。

在强化学习框架下, 问题将更复杂。相较于多臂赌博机, 强化学习问题对于最优解和状态动作值 $Q(s, a)$ (类似于多臂赌博机中的 $Q(a)$ ) 的估计将更困难。而在很多强化学习问题中, 更多时候面对的问题甚至不在于哪个算 法可以更快达到收玫, 而是是否可以收敛的问题。有时候是因为任务过于复杂, 导致状态空间和动作空间过于庞大, 而在这种情况下, 仅靠增强策略的探索能力仍旧无法收玫; 此外, 在某些场景中环境中的回报是稀疏的, 甚至有时没有外部回报, 在这种情况下, 无法靠回报或者 $V(S)$ 及 $Q(s, a)$ 来指导探索和利用。

在复杂的环境中, 如果仍然采用抖动策略进行随机探索, 效果将会很差。因为抖动策略依赖的实际上是随机行为。如果偶然导致奖励, 则这些对应的行为就会被强化, 并且智能体未来会更倾向于采取这些有利的行为。

当奖励足够密集或者状态空间和动作空间不太大时，随机行动可以带来合理概率的奖励，因而比较奏效。但是，复杂的环境往往需要很长的特定动作的序列才能获取奖励，这样的序列随机发生的可能性非常低，仅靠随机策略难以发现较优解。此外，在一般的强化学习问题中往往无法找到最优解，很多时候只能找到次优解，而更多时候都会陷入局部最优。在这样的情况下，我们需要策略拥有不依赖外界的更好的探索能力。

在强化学习的问题下，对动作的探索变成了对动作和状态的探索，或者是对轨迹的探索。在多臂赌博机中，探索的根本思路在于以更高的概率选取在当前更有可能获得更高回报的动作：一种方案是直接对策略，也即对选择动作的概率进行调整，另一种方式则像 UCT 一样，将当前对动作价值的估计加上额外项，并采用贪心策略。我们同样可以将这一的思想借鉴过来，鼓励智能体探索更有可能获得更高回报的轨迹。由于轨迹不仅包含了动作 a，也包含了状态 s，因此在探索时，要考虑两方面的探索。二者可以分别考虑，也可以同时考虑。通常情况下，由于状态空间的维度往往远大于动作空间，因此状态数量也往往要大于动作数量，因此探索问题的重心通常放在对状态 s 的探索中。

## 基于内在奖励的方法

我们首先要知道什么是内在奖励。我们知道在强化学习的问题定义下，存在 r 作为环境给智能体的反馈或回报，而智能体学习的目标正是最大化这样的累计回报，因此这样的回报是促使智能体学习的重要信号。这样的从环境中获得的奖励信号通常被称为是外在奖励，而内在奖励则是由智能体自身给出。本质上，基于内在奖励的方法可以看做是一种区间估计的方式，给出当前动作可能成为最优动作的奖励，因此早期很多工作都深受此想法的影响。

基于内在奖励的方法的研究由来已久，并且已经有详细的综述文献给予了总结Oudeyer and Kaplan (2009); Schmidhuber (2010)。基于内在奖励的方法的核心在于如何计算或者生成内在奖励，例如希望其与状态的不确定性/惊喜程度/新鲜程度相关，这通常要借助一些手段或者技术来实现。因此本节将主要从内在奖励的不同组成来对这些方法进行分类简述。


### 基于计数的方法

基于计数的方法思路较为简单, 即通过维护访问过的状态的数量, 并利用计数值计算内在奖励。

最早, Wiering and Schmidhuber (1998) 提出利用计数来提供额外的奖 励, 称作“Directed Exploration”, 这样的奖励函数定义了“有趣 (interesting)” 的状态。其定义了两种奖励函数, 一种称作 recency-based, 定义为 $R=\frac{-t}{K_T}$, 其 中, $t$ 是当前的时间步, $K_T$ 代表常数, 则该奖励将会鼓励智能体选择最近没有 被选择的动作; 另一种奖励函数称作 frequency-based, 定义为 $R=\frac{-C_{s_t}\left(a_t\right)}{K_C}$, 其中, $C_{s_t}\left(a_t\right)$ 代表在当前状态下执行各动作的次数, $K_C$ 代表常数, 因此 该奖励会鼓励智能体探索使用最不频繁的动作。该研究基于探索奖励建立 了模型, 并利用模型计算动作价值的区间估计, 进而求解最优策略。此工 作即是著名的 MBIE (model based internal estimation)。后来 Strehl and Littman $(2004,2008)$ 重新描述了 MBIE 方法, 重新给出了信赖区间的定义, 在该定义中, 信赖域的公式与 $n(s, a)$, 即状态-动作对的访问次数有关, 进而 在Strehl and Littman (2005) 中证明了 MBIE 是 PAC 的, 并给出了 MBIE 估计的回报上界为 $\tilde{R}(s, a)=\hat{R}(s, a)+\sqrt{\frac{\ln 2 / \delta_R}{2 n(s, a)}}$, 其中, $\delta_R$ 是根据 Hoeffding bound 不等式, 至少有概率 $1-\delta_R$ 的概率使回报落在指定区间。Strehl and Littman (2008) 中还提出了 MBIE-EB 方法, 其对价值函数的估计在回报的 基础上加入了探索奖励项即 $R(s, a)+\frac{\beta}{\sqrt{n(s, a)}}$ 。这些工作和 $R_{\text {max }}$ Brafman and Tennenholtz (2002)， $E^3$ Kearns and Singh (2002) 等方法被称为 PAC-MDP 方法。这些方法的主要思路是, 如果一个智能体已经观察到某个状态动作 对足够多次, 那么可以使用例如 Hoeffding bound 等偏差不等式, 确保经验估计可以接近真实环境的动力学模型。但是, 如果没有观察到状态-动作对足够多次, 则假设它具有非常高的价值, 这将鼓励智能体尝试没有观察到足够多次的状态动作对, 直到最终我们有一个适当准确的系统模型, 这种技 术一般被称为面对不确定性的乐观主义。

和 Gittin Indices 类似, 在强化学习问题中同样有人采用贝叶斯方法来 对环境建模。这些方法提出了信念状态 (belief state) 的概念。最优贝叶斯策 略选择的动作不仪基于它们将如何影响环境的下一个状态, 还基于它们将如 何影响下一个信念状态; 而且, 由于更好地了解 MDP 通常会带来更大的未 来回报, 贝叶斯策略很自然会在平衡探索和利用上进行权衡。但是这些方法 通常是难以求解的。这些方法通常被称为 Bayesian Reinforcement Learning 方法。Kolter and Ng (2009) 结合了贝叶斯方法与 PAC-MDP 方法, 对贝 叶斯价值函数的估计在回报的基础上加入了探索奖励项即 $R(s, a)+\frac{\beta}{1+n(s, a)}$, 并且证明了尽管这样的算法无法求解出最优策略, 但可以接近最优策略。
这样基于计数的方法明显只适用于低维离散动作和状态空间, 无法解决 高维或者连续空间下的强化学习问题。基于此, Tang et al. (2017) 提出了利 用哈希函数 $\phi\left(s_t\right)$ 缩小状态空间, 以计算内在奖励, 计算 $R(s, a)+\frac{\beta}{\sqrt{n(\phi(s))}}$ 。 而Bellemare et al (2016) 则利用概率分布计算的密度函数 (density model) 推出伪计数 (pseudo-count), 计算内在奖励, 其形式为 $R(x, a)+\frac{\beta}{\sqrt{n(x)+0.01}}$, 这里的 $x$ 是游戏画面的某个像素点, 则 $\hat{n}(x)$ 计算的是杂处像素点的伪计数。 并通过证明其和信息增益. (information gain) 的关联证明了基于计数方法中 采用的信赖域和传统基于内在奖励方法的关联性, 本质是统一的。Ostrovsk et al. (2018) 借鉴前面工作的思想, 利用 PixelCNN 来计算伪计数, 其使用 的奖励形式为 $R(x, a)+\frac{1}{\sqrt{\hat{n}_n(x)}}$ 。


### 基于信息论的方法

很多基于内在奖励的探索方法从信息论的角度出发, 用各种指标衡量 新状态或者新动作对信息的贡献程度, 或者降低不确定性的程度, 鼓励智能 体探索包含更多信息的轨迹, 很多方法和变分推断相结合。Mohamed and Rezende (2015) 提出使用信息论中的互信息 (mutual information) 来计算 所谓的 empowerment, 通过选择可以最大化 empowerment 的动作, 智能体 希望达到可以到达更大数量的术来状态。作者提出, 智能体叮以选择只最大 化该指标, 因为环境中的奖励不一定够好; 智能体也可以将 empowerment 作为 reward shaping 的组成部分。empowerment 的计算公式可以写作:

$$
\epsilon(s)=\max _w \mathcal{I}^w\left(a, s^{\prime} \mid s\right)=\max _w \mathbb{E}_{p\left(s^{\prime} \mid a, s\right) w(a \mid s)}\left[\log \left(\frac{p\left(a, s^{\prime} \mid s\right)}{w(a \mid s) p\left(s^{\prime} \mid s\right)}\right)\right] 。
$$

Still and Precup (2012) 同样使用互信息作为探索奖励, 该研究提出, 可以将动作视为状态的一种表示。因此可以将状态映射到动作视为一种有损压缩, 若一组状态共宁相同的最优动作, 则该动作可以被视为这组该状态“聚类”的压缩表示, 因此在具有㥵同回报的策略中, 要找到最紧凑的策略, 即压缩了最多状态信息的动作, 因此要最小化动作与状态的互信息, 目标是在最大化收益的同时,最小化 $I_q^\pi\left(A_t, X_t\right)=\sum_{x \in \mathcal{X}} \sum_{a \in \mathcal{A}} \pi(a \mid x) p^\pi(x) \log \left[\frac{\pi(a \mid x)}{p^\pi(x)}\right]$ 。

Houthooft et al. (2016) 利用智能体对动力学模型的信心 (belief) 的信息增益 (information gain) 来进行探索, 同样是基于好奇心驱动的思想, 鼓励智能体选择会带来更惊奇 (surprise) 的状态的动作, 其计算的是在已知历史轨迹 $\xi_t$ 后加入动作 $a_t$ 和状态 $s_{t+1}$ 后的信息增益作为内在奖励, 即 $R\left(s_t, a_t\right)+\eta D_{K L}\left[p\left(\theta \mid \xi_t, a_t, s_{t+1} \| p\left(\theta \mid \xi_t\right)\right]\right.$ 。


### 基于预测误差的方法

基于预测误差的方法通常会建立环境的一步预测模型, 来对未来状态 进行估计, 并以实际情况与估计的差别程度作为对智能体的探索奖励, 由此 鼓励智能体探索更少见的状态。Pathak et al. (2017) 使得好奇心驱动的探索 再次成为研究热点, 该研究提出使用深度网络建立环境的一步预测模型, 对 环境状态的特征表示进行预测, 并利用其与真实的环境状态特征表示的差 值范数作为惊喜的奖励, 其对内在奖励的定义为: $\frac{\eta}{2}\left\|\hat{\phi}\left(s_{t+1}\right)-\phi\left(s_{t+1}\right)\right\|_2^2$, 其中 $\hat{\phi}\left(s_{t+1}\right)=f\left(\phi\left(s_t\right), a ; \theta_F\right)$ 。
目前大部分探索的 Benchmark 环境是以 MonteZuma 为代表的一系列 稀疏回报的游戏环境, 而在这些环境中, 目前公认的 Sota 是Burda et al. (2019) 提出的 RND 方法。该方法同样是由好奇心驱动, 但其通过最小化 一个由状态作为输入的可训练的神经网络与另一个随机初始化的固定网络 的差值, 并由该差值提供内在奖励, 其观点在于少见的状态将不会训练的很 好, 因此会带来很大的差值。该方法的内在奖励可以'与为 $\hat{f}(x ; \theta)-f(x)$ 。

[1]: [探索利用困境综述](../../papers_PDF/exploration-talk.pdf)
