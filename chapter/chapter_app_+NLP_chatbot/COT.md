1.思维链定义
背景



在 2017-2019 年之间，随着 Transformer 模型的提出，计算资源与大规模语料库不断出现，自然语言处理领域发生了翻天覆地的变化，传统的全监督学习的范式逐渐达到了瓶颈，很难在传统的训练方式上取得大幅度提升。这时大规模预训练模型的如 Bert、RoBERTa 等模型的出现使得研究方向转向了以预训练模型为基础 + 下游任务 Fine-tune 的范式。



然而随着语言模型规模的不断增大，Fine-tune 的成本变得越来越高，以 GPT-3 为例，其参数量已经达到了惊人的 175B，对于这样大规模的参数，仅依靠传统 Fine-Tune 已经很难对模型起到有效的迁移，且如此大规模的参数量使得梯度的反向传播的代价也急剧增加。在这样的背景下，提示学习应运而生。提示学习通过改造下游任务、增加专家知识等形式，使得目标任务的输入输出更加贴合原始语言模型训练时的数据。



2021 年，提示学习经历了以离散提示学习（提示词的组合）为开始，连续化提示学习（连续空间表示）为复兴的多个阶段，逐步达到高潮。但基于连续空间的提示学习同样存在较多的局限性，比如资源消耗与训练不稳定等多种问题。这一时期，虽然大多数研究者普遍认同提示学习将会带来自然语言处理领域下一代革命，但这一时期大多数研究工作主要还是与模型训练或新的语言模型结构相关。



直到 2022 年，大规模语言模型的效果 “肉眼可见” 的变好，同时随着模型规模的不断增大，模型也变得更好“提示”，尤其是之前一些没有办法做很好的任务不断取得突破。但是大模型在做算术推理、常识推理和符号推理时的表现还不够好。 大模型的 in-context few shot 能力是极强的，但是创建很多的中间步骤用来做监督 finetune 是非常耗时的，而且传统的 prompt 方式在数学计算、常识推理等做的又不好，怎么结合 in-context few shot 和 中间步骤来改善算术推理、常识推理和符号推理等能力是一个问题。思维链的一系列工作就是在这样的大环境下诞生的。



定义



思维链 (Chain-of-thought，CoT) 的概念是在 Google 的论文 "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models" 中被首次提出。思维链（CoT）是一种改进的提示策略，用于提高 LLM 在复杂推理任务中的性能，如算术推理、常识推理和符号推理。CoT 没有像 ICL 那样简单地用输入输出对构建提示，而是结合了中间推理步骤，这些步骤可以将最终输出引入提示。简单来说，思维链是一种离散式提示学习，更具体地，大模型下的上下文学习（即不进行训练，将例子添加到当前样本输入的前面，让模型一次输入这些文本进行输出完成任务），相比于之前传统的上下文学习（即通过x1​,y1​,x2​,y2​,....xtest​作为输入来让大模型补全输出ytest​），思维链多了中间的中间的推导提示，以下图为例：






可以看到，类似的算术题，思维链提示会在给出答案之前，还会自动给出推理步骤：



“罗杰先有 5 个球，2 罐 3 个网球等于 6 个，5 + 6 = 11”



“食堂原来有 23 个苹果，用 20 个做午餐，23-20=3；又买了 6 个苹果，3+6=9”



思维链提示给出了正确答案，而直接给出答案的传统提示学习，结果是错的，连很基本的数学计算都做不好。简单来说，语言模型很难将所有的语义直接转化为一个方程，因为这是一个更加复杂的思考过程，但可以通过中间步骤，来更好地推理问题的每个部分。



一个有效的思维链应该具有以下特点：



逻辑性：思维链中的每个思考步骤都应该是有逻辑关系的，它们应该相互连接，从而形成一个完整的思考过程。

全面性：思维链应该尽可能地全面和细致地考虑问题，以确保不会忽略任何可能的因素和影响。

可行性：思维链中的每个思考步骤都应该是可行的，也就是说，它们应该可以被实际操作和实施。

可验证性：思维链中的每个思考步骤都应该是可以验证的，也就是说，它们应该可以通过实际的数据和事实来验证其正确性和有效性。

2.思维链用于上下文学习的方法(In-context learning)
2.1 Few-shot CoT
Few-shot CoT 是 ICL 的一种特殊情况，它通过融合 CoT 推理步骤，将每个演示〈input，output〉扩充为〈input，CoT，output〉。



【CoT prompt 的设计】

作为一种直接的方法，研究表明，使用不同的 CoT（即每个问题的多个推理路径）可以有效地提高它们的性能。

另一个直观的想法是，具有更复杂推理路径的提示更有可能引发 LLM 的推理能力，这可以导致生成正确答案的准确性更高。然而，这两种方法都依赖于带标注的 CoT 数据集，这限制了在实践中的应用。为了克服这一限制，Auto-CoT 建议利用 Zero-shot-CoT，通过专门提示 LLM 来生成 CoT 推理路径，从而消除了手动操作。为了提高性能，Auto-CoT 进一步将训练集中的问题划分为不同的聚类，然后选择最接近每个聚类中心的问题，这应该很好地代表训练集中的提问。尽管 Few-shot CoT 可以被视为 ICL 的一种特殊提示情况，但与 ICL 中的标准提示相比，演示的顺序似乎影响相对较小：在大多数任务中，重新排序演示只会导致小于 2% 的性能变化。

【增强的 CoT 策略】

除了丰富上下文信息外，CoT 提示还提供更多选项来推断给定问题的答案。现有的研究主要集中在生成多条推理路径，并试图在得出的答案中找到共识。例如，在生成 CoT 和最终答案时，提出了 self-consistency 作为一种新的解码策略。它首先生成几个推理路径，然后对所有答案进行综合（例如，通过在这些路径中投票来选择最一致的答案）。self-consistency 在很大程度上提高了 CoT 推理的性能，甚至可以改进一些 CoT 提示通常比标准提示差的任务。此外，将自一致性策略扩展到更通用的集成框架（扩展到提示上的集成），发现不同的推理路径是提高 CoT 推理性能的关键。

2.2 Zero-shot CoT
与 Few-shot CoT 不同，Zero-shot CoT 在 prompt 中不包括人工标注的任务演示。相反，它直接生成推理步骤，然后使用生成的 CoT 来导出答案。其中 LLM 首先由 “Let's think step by step” 提示生成推理步骤，然后由 “Therefore, the answer is” 提示得出最终答案。他们发现，当模型规模超过一定规模时，这种策略会大大提高性能，但对小规模模型无效，显示出显著的涌现能力模式。



为了在更多的任务上解锁 CoT 能力，Flan-T5 和 Flan-PaLM 进一步在 CoT 标注上执行指令调优，并且改进了在不可见任务上的零样本性能。

3. 结论
CoT 对小模型作用不大，模型参数至少达到 10B 才有效果，达到 100B 效果才明显。并且，从小模型的输出可以看出，它们大部分是输出了流畅但不合逻辑的 CoT，因此得到错误的结果。

CoT 对复杂的问题的性能增益更大，例如 GSM8K（更难，因为基线最低）上 GPT-3 和 PaLM 的性能增加了一倍多。而对于 MAWPS-SingleOp（更简单的任务），性能改进非常小甚至是负面的。

加上 CoT 的 PaLM 540B 超过了任务特定的用监督学习训练的模型的最优结果。不加 CoT 的话 GSM8K 和 MAWPS 任务上 LLM 的结果比不过最优的监督学习模型。



思维链是解决推理任务时人类思维过程遵循的一系列典型步骤。 它可以帮助我们将一个问题分解成一系列的子问题，然后逐个解决这些子问题，从而得出最终的答案。在大型语言模型中，思维链可以用来引出推理。思路链方法带来以下好处：



CoT 允许模型将多步推理问题分解为中间步骤，这意味着额外的计算可以分配到需要推理的复杂问题上；

CoT 使大语言模型更具可解释性，更加可信，并提供了调试推理路径错误的机会；

CoT 推理能够被用于数学应用题、常识推理和符号操作等任务，并且可能适用任何人类需要通过语言解决的问题；

CoT 可以通过将其加入到 few-shot prompting 示例中，从而在足够大的语言模型中引导出推理能力。



当前的思维链也存在着许多局限性：



首先，尽管设计的思维链是在模拟人类的推理过程，但模型是否真正的学会了推理仍需进一步进行验证。

人工设计思维链仍然是代价过大，大规模的人工标注思维链是不可行的。

思维链只在大规模模型上有效（10B 以上）

4.未来对思维链的思考
（1）什么时候 CoT 对 LLMs 有用



由于 CoT 是一种涌现能力，只对足够大的模型（例如，通常包含 10B 或更多的参数）有积极影响，但对小模型没有影响。此外，由于 CoT 通过中间推理步骤增强了标准提示，因此它主要有效地改进了需要逐步推理的任务，如算术推理、常识推理和符号推理。然而，对于不依赖于复杂推理的其他任务，它可能显示出比标准提示更差的性能，例如 GLUE 的 MNLI-m/mm、SST-2 和 QQP。



（2）为什么 LLMs 可以执行 CoT 推理



关于 CoT 能力的来源，人们普遍假设它可以归因于对代码的训练，因为在代码上训练的模型显示出强大的推理能力。从直觉上讲，代码数据通过算法逻辑和编程流程进行了良好的组织，这可能有助于提高 LLM 的推理性能。然而，这一假设仍然缺乏消融实验的公开报道证据。此外，指令调优似乎不是获得 CoT 能力的关键原因，因为经验表明，对非 CoT 数据的指令调优并不能提高保持的 CoT 基准的性能。



总之，CoT 提示为诱导 LLM 的推理能力提供了一种通用而灵活的方法。也有一些初步尝试将该技术扩展到解决多模态任务和多语言任务。除了将 LLM 与 ICL 和 CoT 直接结合使用外，最近的一些研究还探讨了如何将 LLM 的能力专门化到特定任务，这被称为模型专门化。例如，研究人员通过微调 LLM 生成的 CoT 推理路径上的小规模 Flan-T5，专门研究 LLM 的数学推理能力。模型专业化也可用于解决各种任务，如问答、代码合成和信息检索。

5.关键知识点
有效的思维链应具备的特点是：逻辑性、全面性、可行性

思维链只能在大语言模型中起作用。

Few-shot CoT 是 ICL 的一种特殊情况。

Zero-shot CoT 在 prompt 中不包括人工标注的任务演示。

CoT 使大语言模型更具可解释性，更加可信。[1]



[1]: https://xie.infoq.cn/article/736cd66decbb874093b9c1b61

---

chain of thought 也就是 CoT ，一经提出就引发了社区对它的热烈讨论，CoT 能够帮助大规模语言模型解决复杂的算术、常识及字符推理等任务。


背景知识

语言模型

语言模型的本质是对任意一段文本序列的概率进行建模
如果将语言模型看成一个大黑盒的话，它的输入是一段文本序列，输出也是一段文本序列，通过训练语言模型，就能使得给定的文本序列和输出的文本序列拼接起来所组成的一整段文本序列的概率尽可能比较大

Large Language Model: GPT-3






为了得到更好的泛化模型，以及提高模型在执行多种通用任务上的能力，基于大数据的预训练模型越来越多。 给定更大的预训练数据，Transformer 架构在增加模型大小和训练中计算量的情况下表现更好，展示了卓越的 scalability。具体来说，基于 transformer 的语言模型的性能与模型参数、数据量和训练计算量的呈幂律关系（Kaplan 等人，2020 年）。transformer 在语言模型上的架构有三种不同的设计模式：encoder-only(Bert)\encoder-decoder(Bart\T5)\decoder-only(GPT)。
大规模语言模型所采用的都是 decoder-only 架构，并采用世界上丰富的未标签文本进行自监督训练。预训练任务通常是Next word prediction，这种方式又被称为 Causal language modeling。这个Causal就是“因果”的意思，对于decoder，它在训练时是无法看到全文的，只能看到前面的信息。因此这类模型适合做文本生成任务.使用decoder transformer的 GPT 预训练，其中目标序列是移动了一个标记的输入序列。请注意，transformer 解码器中的因果注意力模式强制每个 token 只能关注其过去的 token。GPT 有 1 亿个参数，需要针对下游任务进行微调。一年后引入的更大的语言模型 GPT-2（Radford 等人，2019 年）。与 GPT 中的原始 transformer decoder 相比，GPT-2 中采用了pre-normalization以及改进的初始化和权重缩放。在 40 GB 的文本上进行预训练，15 亿参数的 GPT-2 在语言建模基准上获得了 sota，并在不需要更新参数和架构的情况下在多个其他任务上获得了可观的结果。





GPT-2 展示了在不更新模型的情况下将相同的语言模型用于多个任务的潜力。而不需要再加入梯度计算进行微调。GPT-3 没有采用微调而是将输入文本序列作为任务的描述，输入输出样例作为任务的说明，提示作为任务的输入。GPT-3 可以被分类为 3 种学习范式：





少样本与零样本的唯一区别就是中间多出了一些参考样例，它们其实都是在续写前缀（只是零样本的输入没有任何参考，而少样本的输入有一些参考样例来帮助语言模型推断如何根据任务输入生成相应的任务输出）
用一个训练好的大语言模型来求解推理任务的几种范式

这里以需要推理的数学题举例
1、Zero-shot






文献：Large Language Models are Zero-Shot Reasoners(https://arxiv.org/abs/2205.11916)
语言模型的输入是一道数学题连接一个字符串“The answer is”，然后让语言模型进行续写
2、Zero-Shot-CoT











语言模型的输入还是一道数学题连接一个字符串“Let's think step by step”，然后让语言模型进行续写
这种情况下，语言模型会续写出中间推理步骤，并最终生成答案
3、Manual-CoT











文献：Chain of Thought Prompting Elicits Reasoning in Large Language Models（https://arxiv.org/abs/2201.11903）
这种情况下使用到了少样本学习，在输入问题之前，手动设计一些问题和答案的样例（样例的答案给出中间推理步骤），这些问题和答案都需要手动构造，所以叫 Manual-CoT
语言模型的输入是一些手动设计的问题和答案的参考样例连接一个真正需要求解的问题，然后让语言模型进行续写
Manual-CoT 比 Zero-Shot-CoT 的性能要好，因为它采用的是 few shot ，在输入中提供了一些问题、中间推理步骤以及答案的样例给语言模型进行参考。但是，提供这些样例需要进行人工设计，这就需要一定的人工成本
4、Auto-CoT






文献：Automatic Chain of thought Prompting in Large Language Models（https://arxiv.org/abs/2210.03493）
auto-CoT 分为两个步骤
通过多样性选取有代表性的问题
对于每一个采样的问题拼接上“Let's think step by step”（类似于 Zero-Shot-CoT ）输入到语言模型，让语言模型生成中间推理步骤和答案，然后把这些所有采样的问题以及语言模型生成的中间推理步骤和答案全部拼接在一起，构成少样本学习的样例，最后再拼接上需要求解的问题一起输入到语言模型中进行续写，最终模型续写出了中间的推理步骤以及答案
在十个数据集上 Auto-CoT 是可以匹配甚至超越 Manual-CoT 的性能，也就说明自动构造的 CoT 的问题、中间推理步骤和答案样例比人工设计的还要好，而且还节省了人工成本


CoT 这个领域刚发展不久，所以还有很多未解之谜，尽管刚提出才几个月，但是已经受到了社区的广泛关注



Abstract

现在语言模型的规模越来越大，但是即便是现在最大的语言模型，它们也往往很难在涉及到推理方面的任务取得很好的表现，也就是说，他们通常很难在数学，符号，以及常识的推理上取得尚佳的表现

这篇文章主要是针对大语言模型在遇到语言推理任务时的局限性，提出了 chain of thought，也就是思维链

文中也给出了 CoT 的定义：人类在遇到一系列问题时所产生的推理步骤，而它们的表现形式就是一系列的短句子（比如说在背景介绍中所提到的遇到数学问题时所产生的中间推理步骤）
最终的实验效果非常好，比如说在使用谷歌内部的 540B 参数量的 PaLM 大语言模型，CoT 能够在像 GSM8K 这样比较难一点的数学问题数据集上取得新的 state of art








1.Introduction

在此前关于大规模语言模型的推理任务中，有两种方法：针对下游任务对模型进行微调；为模型提供少量的输入输出样例进行学习。但是这两种方法都有着局限性，前者微调计算成本太高，后者采用传统的输入输出样例在推理任务上效果很差，而且不会随着语言模型规模的增加而有实质性的改善。






语言模型的规模达到 100B 的参数量之后，就能够在像 sentiment analysis and topic classification 这种分类任务上取得非常好的结果

作者将这类任务归纳为 system-1，也就是能够人类很快很直观地理解的任务
还有一类任务需要很慢而且是很仔细的考虑，作者将其归纳为 system-2 （比如一些设计逻辑、常识的推理任务）
作者发现，即便语言模型的规模达到了几百B的参数量，也很难在 system-2 这类任务上获得很好的表现

作者将这种现象称为 flat scaling curves：如果将语言模型参数量作为横坐标，在 system-2 这类任务上的表现作为纵坐标，则折线就会变得相当平缓，不会像在 system-1 这类任务上那么容易就实现模型的性能随着模型参数量的增长而提升，也就是说，在 system-2 这类任务上语言模型就很难大力出奇迹了
针对这个问题，作者提出了 chain of thought （CoT）这种方法来利用大语言模型求解推理任务








上图展示了在 CoT 诞生之前是怎样使用标准的 prompting 方法来求解推理任务的
首先这是一个少样本学习的方法，需要给出一些问题和答案的样例，然后拼接这正想要求解的问题，最后再拼接一个字符串“A:”之后输入到大语言模型中，让大语言模型进行续写
大语言模型会在所提供的问题和答案的样例中学习如何求解，结果发现很容易出错，也就是上面提到的大语言模型在 system-2 上很容易遇到瓶颈





上图展示了 CoT 的做法，CoT 与 Standard prompting 唯一的区别就是，CoT 在样例中在给出问题的同时，不仅给出了答案，在答案之前还给出了人为写的中间推理步骤
在把问题、中间推理步骤和答案的若干样例拼接上所想要求解的问题和字符串“A”，再输入到语言模型之后，语言模型会自动地先续写中间推理步骤，有了这些推理步骤之后，它就会更容易地给出正确答案，也就是能够更好地解决 system-2 这类的问题
2.Chain of thought Prompting

因为 CoT 是作者所提出的一个新事物，所以作者强调了 CoT 中几个比较有意思的地方

首先，CoT 原则上能够让模型把一个多步的问题分解出各种中间步骤，使那些具有更多推理步的问题有机会分配到更多的计算量（如果是从最后的将拼接好的问题、答案样例以及所要求解的问题和前缀输入到语言模型中产生最后的答案这一步来看，对于一个更难的问题，在续写的时候，CoT就使得语言模型能够产生更多的中间推理步骤，因为语言模型在生成输出的时候是一个一个 token 进行生成的，那么如果问题越难，CoT 又使得生成的中间步骤越多，那么整体上生成的 token 的数量也会越多，自然而然在求解更难的问题的时候就会使用到更多的计算量。就好比人类在遇到更难得问题的时候，可能就会耗费更多的脑力，这样 CoT 也能够让计算机能够对更难的问题分配更多的计算资源）
CoT 提供了可解释性，也就是在不知道答案的情况下，也能够知道答案是怎样得来的，也就是所谓的中间推理步骤
作者认为 CoT 在原则上能够适用于任何人类能够用语言所能解决的问题，而不仅仅是数学、逻辑、常识这类的问题。因为 CoT 本身的载体就是一系列的短句子，本身也是人类语言
当一个语言模型训练好之后，就能够通过 few-shot prompting 这种范式，在每个样例中写上中间推理步骤，再拼接好所要求解的问题输入到语言模型，就能够引发语言模型续写中间推理步骤，再得出最后的答案（像 Zero-Shot CoT 就发现，甚至都不需要在 few-shot 这些样例中添加 CoT ，可以仅凭“let's think step by step”作为 CoT 的推理；而 Auto CoT ，也就是“Let's think not just step by step but one by one”使用了多个“let's think step by step”就可以自动地构造 few-shot 的样例，从而弥补了 Zero-shot 和 Few-shot 之间的性能差异）


3.Arithmetic Reasoning

这里的算数推理所考虑的问题范围集中在小学数学问题，也就是 6-10 岁的小朋友所能解决的数学问题

实验设计

benchmarks: 五个数学问题的数据集

standard prompting: baseline

chain of thought prompting:

作者人工设计了一套 8 个带有 CoT 推理链条的 few-shot 样例，而且作者在五个数据集中统一使用了这 8 个带有 CoT 推理链条的 few-shot 样例

其中的一个原因是因为人工构造 CoT 推理链条的 few-shot 样例的成本是很高的，因为不仅要找到具有代表性的问题，还要为每个问题设计中间推理步骤以及答案，而最后的性能对这些人工设计非常敏感，所以需要反复进行调试。
language model: GPT-3\LaMDA\PaLM\UL2 20B\Codex.








CoT 会随着模型规模的提高会涌现出更强大的能力。也就是说 CoT 对于小规模模型没有太大影响，只有在使用参数为 100B 以上的模型时才会产生性能提升。并且作者发现，在较小规模的模型中产生了流畅但不符合逻辑的 CoT，导致了比 Standard prompt 更低的表现。
其次，CoT 对于更复杂的问题有更大的性能提升。例如，对于 GSM8K（baseline 性能最低的数据集），最大的 GPT 和 PaLM 模型的性能提高了一倍以上。而对于 SingleOp（MAWPS中最简单的子集，只需要一个步骤就可以解决），性能的提高要么是负数，要么是非常小。
在 GPT 和 PaLM 模型下，CoT 超越了之前的 sota（之前的 sota 采用的是在特定任务下对模型进行微调的模式）
消融实验

Equation Only：把 CoT 替换成只包含 CoT 中的算式部分。
Variable compute only：把 CoT 替换成与中间 CoT 长度相等的...
结果发现上述两种方法都比 CoT 差了很多，这就说明了 CoT 虽然简单，但是不能再简单了，而且这也更能体现出 CoT 中自然语言所起的作用
Thought after answer：把中间推理步骤放在答案的后面。
实验结果显示：把中间推理步骤放在答案的后面所得到的结果也不是很好，这就说明，在训练数据集中大部分情况下依然还是先给出中间推理步骤再给出答案，而不是先给出答案再给出中间推理步骤









鲁棒性实验

在本节的最后，作者也提到 CoT 的性能可能也会对人工设计的 prompt 比较敏感，因此有必要评测所提出的方法的鲁棒性






different annotator 表示有三个作者分别设计了一套带有 CoT 的样例
concise style 表示作者专门又设计了一套更加简单的带有 CoT 的样例
exemplars 从带有中间推理步骤的数据集中随机地选取一些问题，并附上这些数据集中自带的中间推理步骤构成 CoT 的样例，再拼接上答案来进行评测
最后的结果显示：带 CoT 的方法都要比不带 CoT 的 standard prompting 要带来更加显著的性能提升，尤其是在图三中的两个更加困难的数学推理数据集中，这种性能提升更加明显


4.Symbolic Reasoning

文章中在本节和下一节中还提到了一些其他类型的任务

符号推理任务











5. Commonsense Reasoning

常识推理任务













上图展示了 CoT 模型在这些任务的性能，和之前类似，尤其是当你的语言模型的规模很大的时候，CoT 相对普通的 prompting 方法能够带来更加显著的性能提升
6.Discussion

在文章的末尾，作者强调，在 CoT 诞生之前的标准的 prompting 只是大语言模型语言能力的一个下限
虽然 CoT 模拟了人类推理的思维过程，但这并不能回答神经网络是否真的“推理”，我们将其作为一个悬而未决的问题
在大规模语言模型上使用 CoT 推理使得在实际应用中服务的成本很高；进一步的研究可以探索如何在较小的模型中进行推理
参考文献

Chain of Thought Prompting Elicits Reasoning in Large Language Models
Chain of Thought论文、代码和资源【论文精读】

[2]: https://zhuanlan.zhihu.com/p/582758381
