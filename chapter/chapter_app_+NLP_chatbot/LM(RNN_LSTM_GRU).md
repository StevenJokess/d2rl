

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-11-03 07:51:05
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-11-09 06:33:12
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请资助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 语言模型（RNN、LSTM、GRU）

## 语言模型

### 定义

一、语言模型（LM）的任务就是预测一段文字接下来会出现什么词。[1]

即一个语言模型应该有能力计算下面这个公式的值:

$$
P\left(x^{(t+1)} \mid x^{(t)}, \ldots, x^{(1)}\right)
$$

翻译过来就是，在已知一句话的前 $\mathrm{t}$ 个词的基础上，通过LM可以计算出下一个词是某个词的概率。

二、语言模型（LM）给一段文本赋予一个概率。

即对于一段文字 $x^{(1)}, x^{(2)}, \ldots, x^{(T)}$ ，LM可以计算出这句话出现的概率:

$$
\begin{aligned}
& P\left(x^{(1)}, \ldots, x^{(T)}\right) \\
& =P\left(x^{(1)}\right) \cdot P\left(x^{(2)} \mid x^{(1)}\right) \cdot P\left(x^{(3)} \mid x^{(2)}, x^{(1)}\right) \ldots \\
& =\Pi_{t=1}^T P\left(x^{(t)} \mid x^{(t-1)}, \ldots, x^{(1)}\right)
\end{aligned}
$$

其中

$$
P\left(x^{(t)} \mid x^{(t-1)}, \ldots, x^{(1)}\right)
$$

就是LM可以计算出来的。


### 应用

- 输入法联想。比如我在搜狗输入法里写一个“我”，输入法会给我推荐几个词：“女朋友”、“刚刚”、“的”等等。这会暴露我们个人的一些习惯。
- 搜索联想。基于神经网络的语言模型演进历程
-

### 如何学习语言模型：n-gram模型

所谓的N-gram，就是指一堆连续的词。根据这一堆词的个数，我们可以分为unigram,bigram,trigram等等。

如果我们需要得到一个N-gram的LM，它的意思就是希望我们可以通过N-1个词预测第N个词的概率。

那么如何学习得到一个N-gram的LM呢？一个直接的思路就是，我们可以收集关于语料中的各个N-gram出现的频率信息。

对于一个N-gram的LM，我们需要做一个假设：某个词出现的概率只由其前N-1个词决定。

比如我们想得到一个3-gram的LM，那么就是说想预测一段文本的下一个词是什么的话，只用看这个词前面2个词即可。



## RNN的必要性

在时间序列数据中，当前的观察依赖于之前的观察，因此观察之间不是相互独立的。然而，传统的神经网络将每个观察视为独立的，这就导致了循环神经网络(RNN)的兴起，它通过包含数据点之间的依赖关系将记忆的概念引入神经网络。



但是RNN是如何实现这种记忆的呢？

RNN通过神经网络中的反馈回路实现记忆，这其实是RNN与传统神经网络的主要区别。反馈回路**允许信息在层内传递**，而前馈神经网络的信息仅在层之间传递。

为此，演化出了不同类型的RNN：

循环神经网络(RNN)
长短期记忆网络(LSTM)
门控循环单元网络(GRU)

本文将介绍RNN、LSTM和GRU的概念和异同点，以及它们的一些优点和缺点。

循环神经网络(RNN)
通过反馈回路，一个RNN单元的输出也被同一单元用作输入。因此，每个RNN都有两个输入：过去和现在。使用过去的信息会产生短期记忆。

为了更好地理解，可以展开RNN单元的反馈循环。展开单元格的长度等于输入序列的时间步数。

可以看到过去的观察结果是如何作为隐藏状态通过展开的网络传递的。在每个单元格中，当前时间步的输入、前一时间步的隐藏状态和偏置组合，然后通过激活函数限制以确定当前时间的隐藏状态步。

RNN可用于一对一、一对多、多对一和多对多预测。

### RNN的优点

由于其短期记忆，RNN可以处理顺序数据并识别历史数据中的模式。此外，RNN能够处理不同长度的输入。

### RNN的缺点

RNN存在梯度下降消失的问题。在这种情况下，用于在反向传播期间更新权重的梯度变得非常小。将权重与接近于零的梯度相乘会阻止网络学习新的权重。停止学习会导致RNN忘记在较长序列中看到的内容。梯度下降消失的问题随着网络层数的增加而增加。

由于RNN仅保留最近的信息，所以该模型在考虑过去的观察时会出现问题。因此，RNN只有短期记忆而没有长期记忆。

此外，由于RNN使用反向传播及时更新权重，网络也会遭受梯度爆炸的影响，如果使用ReLu激活函数，则会受到死亡ReLu单元的影响。前者可能会导致收敛问题，而后者会导致停止学习。

## 长短期记忆(LSTM)

长短期记忆网络（LSTM）是一种 RNN 变体，使模型能够扩展其内存容量，适应更长的时间线需要。RNN 只能记住近期输入。无法使用来自前几个序列的输入来改善其预测。[3]

LSTM的关键是单元状态，它从单元的输入传递到输出。单元状态允许信息沿着整个链流动，仅通过三个门进行较小的线性动作。因此，单元状态代表LSTM的长期记忆。这三个门分别称为遗忘门、输入门和输出门。这些门用作过滤器并控制信息流并确定保留或忽略哪些信息。

遗忘门决定了应该保留多少长期记忆。为此，使用了一个sigmoid函数来说明单元状态的重要性。输出在0和1之间变化，0即不保留任何信息；1则保留单元状态的所有信息。

输入门决定将哪些信息添加到单元状态，从而添加到长期记忆中。

输出门决定单元状态的哪些部分构建输出。因此，输出门负责短期记忆。

总的来说，状态通过遗忘门和输入门更新。

TODO:

### 优缺点

- 优点：类似于RNN，主要优点是它们可以捕获序列的长期和短期模式。因此，它们是最常用的RNN。
- 缺点：由于结构更复杂，LSTM的计算成本更高，从而导致训练时间更长。由于LSTM还使用时间反向传播算法来更新权重，因此LSTM存在反向传播的缺点，如死亡ReLu单元、梯度爆炸等。

## 门控循环单元(GRU)

门控循环单元（GRU）是支持选择性内存保留的 RNN。该模型添加了更新，并遗忘了其隐藏层的门，隐藏层可以在内存中存储或删除信息。[3]

与LSTM类似，GRU解决了简单RNN的梯度消失问题。然而，与LSTM的不同之处在于GRU使用较少的门并且没有单独的内部存储器，即单元状态。因此，GRU完全依赖隐藏状态作为记忆，从而导致更简单的架构。

重置门负责短期记忆，因为它决定保留和忽略多少过去的信息。

更新门负责长期记忆，可与LSTM的遗忘门相媲美。

当前时间步的隐藏状态是基于两个步骤确定的：

首先，确定候选隐藏状态。候选状态是当前输入和前一时间步的隐藏状态以及激活函数的组合。前一个隐藏状态对候选隐藏状态的影响由重置门控制。

第二步，将候选隐藏状态与上一时间步的隐藏状态相结合，生成当前隐藏状态。先前的隐藏状态和候选隐藏状态如何组合由更新门决定。

如果更新门给出的值为0，则完全忽略先前的隐藏状态，当前隐藏状态等于候选隐藏状态。如果更新门给出的值为1，则相反。

GRU的优势：由于与LSTM相比有着更简单的架构，GRU的计算效率更高，训练速度更快，只需要更少的内存。此外，GRU已被证明对于较小的序列更有效。

GRU的缺点：由于GRU没有单独的隐藏状态和细胞状态，因此它们可能无法像LSTM那样考虑过去的观察结果。与RNN和LSTM类似，GRU也可能遭受反向传播及时更新权重的缺点，即死亡ReLu单元、梯度爆炸。


[1]: https://zhuanlan.zhihu.com/p/147322049
[2]: https://developer.aliyun.com/article/174256
[3]: https://aws.amazon.com/cn/what-is/recurrent-neural-network/
