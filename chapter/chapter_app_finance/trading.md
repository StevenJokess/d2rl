

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-05-14 01:21:12
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-06-11 17:02:08
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 交易

根据彭博社的消息，WorldQuant 公司正在使用深度学习做小规模的交易。


## 交易游戏

实际上，交易本身也是一种游戏。下面来看我如何像pacman一样，把交易分解为强化学习的五个基本元素。

宏观意义上的环境就是我们的金融交易市场，里面有成千上万的交易对手、有做市商、有各种可交易的金融标的、以及各种经济、金融的资讯/信息。当然，在强化学习里，尤其是基于工程视角，这里的环境可以是一个交易回测引擎，也可以是在线交易平台。

![交易环境](../../img/trade_env.png)

如上图所示，交易回测引擎首先包含了一个订单撮合引擎，它用来模拟真实交易指令获得成交的过程，在这个过程中，可能包含的场景比如要拒绝无效交易订单，如涨跌停股票不能交易买入；还比如中国股市的T+1机制，T+0的订单不能被执行；还不如限价单、市价单的执行机制。然后就是行情回放或者行情模拟，这是核心的一部分，我们的资产标的价格以及相关信息会随着时间t不断更新，背后至少会有一个庞大复杂的行情数据库，可能是EOD、分钟行情、tick行情、order book等，可能还会包含衍生的指数数据、财务数据、宏观经济数据等。最后就是账户盈亏统计，其中包含对各种交易的计算汇总、头寸汇总、手续费计算、账户净值、账户现金等。

当然，对于agent，上述所有的规则并不一定要完全已知，agent不一定要知道交易成本的具体计算公式、交易限制如涨跌停/成交量限制。

### State

State是agent对可观测环境的建模或者编码。首先很明显的就是agent需要知道当前的资产价格，很可能它还需要知道历史价格。不过大家要记住，不管是监督学习还是强化学习，都绝对不能包含仍和未来数据，在这里的state也是一样。也可以把一些技术指标、因子等加入state，有些人也会结合监督模型，把基于当前state（行情或因子）对股票价格或者其他特征的预测值编码为state的一部分。同时，state还比如包含至少当前的持仓信息，如每个资产的组合权重。

### Action

交易agent的动作建模对接近实际场景的，就是每个时间t，给出一个所有资产的权重向量（包含现金）。根据这个权重向量，结合当前持仓（包含在当前的state里），就能知道当前还有哪些票需要换仓，换仓量是多少，于是变成了当前的交易指令传送给回测引擎或者environment。

### Agent

我们的交易agent可以理解为就是我们的自动下单机器人，它每个时间只做三件事：分别是观察、决策和学习。 观察，就是在每个时间点能够从环境中获取到state信息，这几乎就是它能直接观察的全部信息了；决策就是在每个时间点决定agent要执行的action，比如建/换仓的股票列表以及权重，然后在agent执行action的同时，它能马上得到reward，这是我们agent与环境交互得到的另一个重要的信息。最后agent能够根据得到的信息，来学习、更新它的模型和参数。

### Reward

Reward是能够让agent学习的核心，可以类比监督学习里的标注Label，监督学习里每一个example模型都能知道它做对了没有，label马上就能告诉它，然后通过损失函数让模型学习。在强化学习里，我们优化的是最终目标，比如游戏通关、交易财富净值最大等，但只有等到一个episode完成了过后，或者一次模型完成后，我们才能确定我们做的好不好，但是如果我们只在“游戏”结束时才对agent进行奖励和惩罚，很可能让agent学习得太慢，效果不佳，这就是强化学习里所谓的延迟奖励。但如果我们能在每一步都对agent进行奖惩，就更容易学习。在我们的交易游戏里，可以在每个时间点上用当时的交易损益来作为reward，也就是我们把交易浮赢/亏作为奖惩机制。这么做的原因，也是因为这样的reward的设计和最终的组合净值是在设计上是一致的，每一步step的汇总就是最终的组合净值（可以考虑资本时间成本）。还有一种设计是在reward里加入风险的因素，不过这种设计我没尝试过，研究它的论文也不多，没有证明这种reward一定具有优势。

[1]: http://pg.jrj.com.cn/acc/Res/CN_RES/INDUS/2017/10/20/bff2daa6-042b-41f8-837c-4b8575431726.pdf
[2]: https://bigquant.com/wiki/doc/-xoRs2BYj3r
