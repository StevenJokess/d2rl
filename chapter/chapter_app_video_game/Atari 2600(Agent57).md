# Atari

基本内容在[Game](Game.md)已经介绍过了，[DQN](../chapter_DQN-algs)也讲过了，

Atari 2600游戏环境是很多DRL算法的测试环境。Atari 2600包含57款游戏，可以用来模拟真实环境中遇到的情况，且游戏环境具有多样性。DQN较早应用于Atari 2600的算法，它在22款游戏中取得了人类玩家的平均水平。

之后，DQN 的各种改进版本（如 DDQN、优先经验回放的DQN、Dueling DQN，以及分布式DQN）分别在Atari 2600游戏中取得了不同程度的提升。然而，很难有一种算法能够在所有Atari 2600 游戏中都达到人类的水平，这主要有两个原因：一个是长期的信度分配，另一个是探索困境。信度分配在DRL中用来解决动作-奖励值分配的问题，DRL中存在奖励延时，因此很难将奖励值分配到具体的行动中，而且游戏运行时间越长，信度分配问题就越难解决。探索困境是指当环境状态的维数很大并且奖励值的设置比较稀疏时，智能体需要很多探索行为才能得到积极的反馈，导致算法很难收敛。

为了解决这些问题，Badia A P等人提出了Agent57算法，并成功地在所有Atari 2600游戏中超越了人类平均水平。针对长期的信度分配问题，Agent57 算法**动态地对折扣因子进行调整，权衡未来奖励对当前状态-动作的重要性**。同时Agent57 算法加入内部奖励值来解决探索困境，内部奖励值的大小由未探索过状态的新奇程度来决定。

Agent57 算法采用两种内部的奖励值：一种是在一个学习周期（episode）内根据新奇的状态得到的奖励值；另一种是长期的，在学习周期之间根据状态的新奇程度得到的奖励值。最后的内部奖励值由这两部分组成。通过这些方法，Agent57算法成功地在所有Atari 2600游戏中超越人类玩家的平均水平，提高了DRL算法的通用性。[3]

## 经典环境

下面再介绍几个经典环境：

- 倒立摆（Cart Pole）：这是一个经典控制问题。一个杆一个小车，杆的一端连接到小车，连接处自由，杆可以摆来摆去。小车前后两个方向移动，移动取决于施加的前后作用力，大小为1。目标是控制力的方向，进而控制小车，让杆保持站立。注意小车的移动范围是有限制的。
- 月球登陆者（Lunar  Lander）：这个游戏构建在Box2D模拟器上。Box2D是一款2D游戏世界的物理引擎，可处理二维物体的碰撞、摩擦等力学问题。本游戏的场景是让月球车顺利平稳地着陆在地面上的指定区域，接触地面一瞬间的速度最好为0，并且消耗的燃料越少越好。
- 双足行走者（Bipedal  Walker）：同样基于Box2D模拟器，这个游戏中玩家可以控制双足行走者的步进姿态。具体地说，是控制腿部膝关节处的马达扭力，尽量让行走者前进得更远，同时避免摔倒。本环境提供的路面包括台阶、树桩和陷坑，同时给行走者提供10个激光测距值。另外，环境的状态信息包括水平速度、垂直速度、整体角速度和关节处角速度等。[2]

## 伪代码

下面介绍如何利用DQN解决Atari。

![DQN](../../img/DQN_atari.png)

直接处理原始 Atari 帧，即 $210 \times 160$ 像素图像和 128 色，在计算上要求很高，因此作者应用 了一个旨在降低输入维度的基本预处理步骤。对原始帧进行预处理通过首先将其 RGB 表示转换为 灰度并将其下采样为 $110 \times 84$ 图像。最终的输入表示是通过裁剪图像的 $84 \times 84$ 区域来获得 的，该区域粗略地捕获了播放区域。最后的裁剪阶段之所以需要是因为作者使用了中 $2 D$ 卷积的 GPU 实现，它需要方形输入。对于本文中的实验，算法 1 中的函数 $\phi$ 将此预处理应用于历史的最后 4 帧，并将它们堆叠起来以生成 $Q$ 函数的输入。

有几种使用神经网络参数化 $Q$ 的可能方法。由于 $Q$ 将历史-动作对（ history-action pairs）映射到其 $Q$ 值的估计， 因此历史和动作已被某些先前的方法用作神经网络的输入。这种架构的主要缺 点是需要单独的前向传播来计算每个动作的 $Q$ 值，导致成本与动作数量成线性比例。作者改为使 用一种架构，其中每个可能的动作都有一个单独的输出单元，只有状态表示是神经网络的输入。输出对应于输入状态的单个动作的预测 $Q$ 值。这种架构的主要优点是能够计算给定状态下所有可能动作的 $\mathrm{Q}$ 值，只需通过网络进行一次前向传递。

作者现在描述用于所有七款 Atari 游戏的具体架构。神经网络的输入是由 $\phi$ 生成的 $84 \times 84 \times 4$ 图像。第一个隐藏层将 16 个步长为 4 的 $8 \times 8$ 滤波器与输入图像进行卷积，并应用激活函数。 第二个隐藏层卷积 32 个 $4 \times 4$ 的滤波器，步长为 2 ，再次跟随激活函数。最后的隐藏层是全连接层，由 256 个神经元组成。输出层是一个全连接层，每个有效动作都有一个输出。在作者考虑的游戏中，有效动作的数量在 4 到 18 之间。作者将使用该方法训练的卷积网络称为 Deep Q Networks (DQN)。

[1]: https://zhuanlan.zhihu.com/p/441314394?utm_campaign=&utm_medium=social&utm_oi=772887009306906624&utm_psn=1628228840898924544&utm_source=qq
[2]: https://pdf-1307664364.cos.ap-chengdu.myqcloud.com/%E6%95%99%E6%9D%90/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E3%80%8A%E7%99%BE%E9%9D%A2%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%B8%A6%E4%BD%A0%E5%8E%BB%E9%9D%A2%E8%AF%95%E3%80%8B%E4%B8%AD%E6%96%87PDF.pdf
[3]: http://www.infocomm-journal.com/znkx/article/2020/2096-6652/2096-6652-2-4-00314.shtml
