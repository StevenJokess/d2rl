

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-05-14 23:48:17
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-10-17 01:01:19
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 王者荣耀（绝悟）

腾讯的AI Lab利用DRL研究了多人在线战术竞技（multi-player online battle arena，MOBA）游戏的1v1模式，该游戏具有很复杂的环境，并且需要很多的控制量。以MOBA中的王者荣耀（Honor of Kings）为例，它具有比围棋更大的状态和动作空间，给策略搜索带来了巨大的挑战。Ye D等人提出了一种包含人工智能服务器、调度模块、记忆池以及强化学习学习者的 DRL 架构来处理该游戏环境，其中人工智能服务器负责与环境进行交互来产生经验数据；调度模块负责将人工智能服务器产生的经验数据进行压缩、打包，送到记忆池中；记忆池用于存储数据，并支持各种长度的样本数据，以及基于生成时间的数据采样；强化学习学习者采用分布式训练，并行地从记忆池中采样数据得到梯度，在同步梯度取均值后，对策略参数进行更新，并且将更新的策略参数传到人工智能服务器中。经过训练后，提出的算法在2 100场Honor of Kings的1v1竞赛中的获胜率为99.81%。[3]

级会议。随后，每年一届的“腾讯开悟多智能体强化学习大赛”在全球顶尖高校院所办得风风火火，世界著名电竞游戏《王者荣耀》团队和腾讯ALab近年来向学术界开放其深度强化学习研究经验及测试资源，刘渝教授带领优秀大学生获得第二届决赛前十名：整个过程精彩纷呈，影响深远，促进了深度强化学习在广袤无垠的前沿议题的进展，例如通过深度强化学习能力[4]

## 新闻

腾讯 AI Lab 与王者荣耀共同探索的前沿研究项目 - 策略协作型 AI 「绝悟」今天在吉隆坡举办的王者荣耀最高规格电竞赛事——世界冠军杯半决赛的特设环节中，在职业选手赛区联队带来的5v5水平测试中获胜，升级至王者荣耀电竞职业水平。

而「绝悟」的 1v1 版本今天也在上海举办的国际数码互动娱乐展览会ChinaJoy首次对公众亮相，向顶级业余玩家开放为期四天的体验测试。首日的504场测试中，「绝悟」测试胜率为99.8%，仅输1场（对方为王者荣耀国服第一后羿）。

两次技术水平测试结果代表腾讯在深度强化学习、多智能体决策智能课题上的国际级 AI 研究水准，也标志着公司在攻坚通用人工智能（ Artificial General Intelligence）难题上更进一步。腾讯 AI Lab 将通过论文等形式进一步分享技术细节，通过开放研究，帮助和启发更多研究者。应用上，「绝悟」背后的研发经验，可在探索 AI 结合电竞、农业、医疗及智慧城市等广阔领域展现巨大潜力。

自动播放开关
自动播放
「绝悟」职业水平测试完整视频（从3分55秒开始）
[ 错误码:61101.13080.18, 1adfe687aecab45dec3f46a44e37d7d9_70201 ] 我要反馈
<>

「绝悟」职业水平测试完整视频（从3分55秒开...
「绝悟」名字寓意绝佳领悟力，其技术研发始于2017年12月，并在2018年12月通过了顶尖业余水平测试（前职业选手与主播联队带来）。

此次测试的「绝悟」版本建立了基于“观察-行动-奖励”的深度强化学习模型，无需人类数据，从白板学习（Tabula Rasa）开始，让 AI 自己与自己对战，一天的训练强度高达人类 440 年。AI 从0到1摸索成功经验，勤学苦练，既学会了如何站位、打野、辅助保护和躲避伤害等游戏常识。更惊喜的是，AI 也探索出了不同于人类常规做法的全新策略。团队还创建One Model模型提升训练效率，优化通信效率提升 AI 的团队协作能力，使用零和奖惩机制让 AI 能最大化团队利益，使其打法果断，有舍有得。

- 探索全新策略：开局时「绝悟」没选择传统人类对线走位策略，而是由双C位英雄虞姬和王昭君先一起清理中路第一波兵线，压制敌方中辅。之后又转上路压制曹操血线。
- 长线策略：对线期，赛区联队三人压迫下路，「绝悟」果断选择用三个 AI 反压赛区联队的上路，最终双方互换一塔，维持均势。
- 团队协作：比赛中期，「绝悟」四人追击娜可露露，AI 达摩一脚将娜可露露反踢入 AI 群中，再由四个 AI 完美配合拿下自己的首杀。
- 即时策略：一对一时，赛区联队实力较强的曹操追击「绝悟」虞姬，虞姬在残血状态退至高地。看到曹操抗塔血量大减后，把握机会绝地反杀。
- 即时策略+团队协作：比赛后期在赛区联队的上路高地塔团战，AI 王昭君先手被对方秒杀，「绝悟」果断选择反打，以漂亮的一波团战全歼对手。
- 即时策略+团队协作：在赛区联队全队覆灭后， 「绝悟」的兵线尚未到达，下路高地塔还有过半血量，「绝悟」果断选择四人轮流抗塔，无兵线强拆塔。* 注：赛事尾声，赛区联队团灭后，「绝悟」未直接推水晶，而是计算整体收益后，选择先推最后一个高地塔，再推水晶直至胜利。

游戏中测试的难点，是 AI 要在不完全信息、高度复杂度的情况作出复杂快速的决策。在庞大且信息不完备的地图上，10位参与者要在策略规划、英雄选择、技能应用、路径探索及团队协作上面临大量、不间断、即时的选择，这带来了极为复杂的局面，预计有高达10的20000次方种操作可能性，而整个宇宙原子总数也只是10的80次方。

腾讯策略协作型AI「绝悟」

若 AI 能在如此复杂的环境中，学会人一样实时感知、分析、理解、推理、决策到行动，就可能在多变、复杂的真实环境中发挥更大作用。因此业界认为下一个 AI 里程碑，可能会在复杂策略游戏中诞生。世界顶级科技公司均在推进此类研究，如Google Deepmind（星际争霸2）、Facebook（星际争霸2） 及 Open AI（Dota 2）等。

腾讯 AI Lab 一直是此类智能体研究的先行者。2016年起，研发的围棋 AI “绝艺”（Fine Art），现担任中国国家围棋队训练专用 AI ；2017年，启动“绝悟”研发；2018 年，“绝悟”达到业余顶尖水平，腾讯还在射击类顶级 AI 竞赛 VizDoom 夺冠，并在《星际争霸2》首先研发出击败内置 AI 的智能体。

腾讯副总裁姚星介绍，“电子竞技”将成为策略协作型AI“绝悟”未来短期内的主要应用场景。作为数字时代最受年轻人欢迎的运动，电竞已于2018年成为亚运会表演项目，中国队参赛获两金一银的佳绩。与传统体育项目一样，电竞职业选手也需要手眼脑协调、策略和操作快速反应、团队协作精神及大量刻苦训练。借助在算法和数据方面的优势， AI 可为职业选手提供数据、战略与协作类实时分析与建议，及不同强度与级别的专业陪练。以前沿科技推动电竞专业化发展，AI 将继续推动中国电竞在全球范围内保持领先。

而长期应用上，“绝悟”将是腾讯攻克 AI 终极研究难题——通用人工智能的关键一步。AGI 代表研发能在通用系统中执行多种复杂命令，达到或超越人类水平的 AI ，从‘绝艺’到‘绝悟’，不断让 AI 从0到1去学习进化，并发展出一套合理的行为模式，这中间的经验、方法与结论，长期来看，有望在大范围内，如医疗、制造、无人驾驶、农业到智慧城市管理等领域带来更深远影响。[5]

## 具体论文



Mastering Complex Control in MOBA Games with Deep Reinforcement Learning

作者：Deheng Ye、Zhao Liu、Mingfei Sun 等
论文链接：https://arxiv.org/abs/1912.09729

- Mage的中文是法师，
- Marksman的中文是射手，
- Warrior的中文是战士
- Assassin的中文是刺客。

推荐：腾讯王者荣耀人工智能「绝悟」的论文入选 AAAI 2020。在研究测试中，AI 玩露娜和顶级选手单挑时也赢了个 3：0。

想要达到战胜人类职业选手的水平，光靠人类数据通过监督学习的方式是不可能的，就像人不可能揪住自己的头发把自己提起来一样，监督学习强依赖于数据的质量，它的天花板就是与人类选手平齐。所以要想超越人类，只能另辟蹊径，自己训练自己，探索出新的策略和打法。绝悟通过庞大的算力、改进的算法及针对性的建模设计，最终训练出了击败职业选手的超强AI。

强化学习的流程是一个生产消费模式，生产端一般是选用CPU，通过self play的形式，不断地产生数据，并发送到数据池，数据的格式一般包括特征（feature）、动作（action）、奖惩（reward）等。消费端选用GPU，从数据池中拉取数据进行训练，更新模型，再将模型同步到生产端，生产端按一定的规则从多个历史模型中更新模型，再产生新的数据，以此循环。在训练过程中，可以通过各项指标的监控来实时反应学习的效果，比如reward、loss，或其他业务指标等。

图1 绝悟强化训练流程图

在绝悟团队发表的论文中可知，训练资源有60w核CPU及1064张GPU，每个英雄每天的训练对局堪比人类玩家无眠无休500年的游戏数据，需要如此庞大规模的训练数据，与王者这款游戏的状态空间和动作空间之大，及游戏本身的特性之复杂是正相关的。

![王者这款游戏的状态空间和动作空间](../../img/MOBA_state_space.png)

以此庞大规模的计算资源产生的训练数据，是强化学习的根基，除此之外，绝悟团队对算法的改进，也是助于提升效果的重要一环。算法的作用是引导模型向更高更强的方向收敛，proximal policy algorithm（PPO）是最早由DeepMind提出（其实是借鉴了OpenAI的TRPO），被广泛应用于机器学习的算法。绝悟团队在PPO算法的基础上进行改进，确保收敛，并结合了attention机制优化目标选择，以及LSTM网络增强记忆等多方面提升效果，细节可能后期会出一篇专门的解读。


图2 绝悟网络结构图

除了资源及算法方面，还有一个比较重要的方面就是建模设计。在机器学习中，特征设计要尽量将状态表达全面，且有差异性，绝悟团队除了用image的特征表达地图信息，还加了vector的特征表达敌我方英雄的状态信息，以及游戏的全局信息。如果说算法影响模型的收敛程度，那么特征是否表达充分，则会影响模型的效果上限。动作建模的设计将控制端输出解耦，形成多标签的优化目标，主要分为两大类，一个是目标选择，另一个是基于这个目标的移动及技能释放的偏移量，这样的设计大大减少了动作的探索空间，更利于模型的收敛速率。除此之外，奖惩也是针对游戏属性的设计，包括血量、经济、击杀、命中、死亡等等。总之特征、动作、奖惩的设计颇需要对游戏的深刻了解和游戏经验，正所谓，要想征服他，必须先成为他。



[1]: https://aijishu.com/a/1060000000100723
[2]: https://zhuanlan.zhihu.com/p/473216291
[3]: http://www.infocomm-journal.com/znkx/article/2020/2096-6652/2096-6652-2-4-00314.shtml
[4]: http://www.deeprlhub.com/d/1357-5
[5]: https://tech.qq.com/a/20190803/002062.htm
