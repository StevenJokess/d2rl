

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-03-21 23:48:20
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-04-04 21:49:08
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# AlphaGo

## 简述

AlphaGo 是第一个打败围棋世界冠军的程序，共18个冠军。[5]

AlphaGo 具备深度学习能力，可结合人类专家比赛中学到的棋谱，以及在和自己下棋（Self-Play）中进行强化学习。2016年3月，人工智能AlphaGo 在围棋竞技中以4：1[6]击败李世石，标志着AI 具备了自我迭代和学习强化的能力。AlphaGo 的深度学习模型可分为策略网络和价值网络，分别解决了围棋中的“下一手最佳策略”和“下一手后的胜率”两个问题，并以此为核心通过学习人类棋谱和自我对弈，不断调整策略提升棋力。AlphaGo 较好地模拟了人类棋手对于围棋落子的直觉判断，即，由于围棋存在的广度（每一步的可能性）和深度（棋局从开始到结束的步数）导致计算机无法模拟计算所有的可能性，AlphaGo 通过策略网络减少搜索广度，即每一步搜索少量节点，而通过价值网络减少搜索深度，无需将棋局下到底即可得出当前局势的判断。[3]

## 计算机围棋的发展历史与现状(Development history and present situation of computer Go)

计算机围棋起源于20世纪60年代, 长期以来, 它被认为是人工智能领域的一大挑战, 并为智能学习算法的研究提供了一个很好的测试平台. 计算机围棋通过计算一个大约含 $b^d$ 个落子情况序列的搜索树上的最优
值函数来评估棋局和选择落子位置, 其中b是搜索的宽度, d是搜索的深度。与象棋等具有有限搜索空间的棋类不同, 围棋的计算复杂度约为$250^150$. 如果采用传统的暴力搜索方式, 按照现有的计算能力是远远无法解决围棋问题的。早期计算机围棋通过专家系统和模糊匹配缩小搜索空间, 减轻计算强度, 但受限于计算资源和硬件能力, 实际效果并不理想。

2006年, 蒙特卡罗树搜索的应用标志着计算机围棋进入了崭新的阶段。 现代计算机围棋的主要算法是基于蒙特卡罗树的优化搜索. Coulom采用这种方法开发的CrazyStone在2006年计算机奥运会上首次夺得九路(9 × 9的棋盘)围棋的冠军。2008年, 王一早开发的MoGo在9路围棋中达到段位水平。2012年, 加藤开发的Zen在19路(19 × 19的全尺寸棋盘) 围棋上以3:1击败二段棋手约翰特朗普. 2014年, 职业棋手依田记基九段让四子不敌razyStone, 这在围棋界引起了巨大的轰动. 赛后依田记基表示此时的Crazy-Stone大概有业余六七段的实力, 但是他依然认为数年内计算机围棋很难达到职业水准. 与此同时, 加藤英树也表示计算机围棋需要数十年的时间才能达到职业水准, 这也是当时大多数围棋领域和人工智能领域的专家持有的观点. 然而, 随着深度学习和蒙特卡罗树搜索方法的结合, 这一观点开始受到挑战. 2015年,Facebook人工智能研究院的Tian结合深度卷积神经网络和蒙特卡罗树搜索开发出的计算机围棋DarkForest表现出了与人类相似的下棋风格和惊人的实力, 这预示着计算机围棋达到职业水准的时间可能会提前。而2016年3月AlphaGo的横空出世彻底宣告基于人工智能算法的计算机围棋达到了人类顶尖棋手水准。



## 原理分析

AlphaGo 创新性地结合深度强化学习和蒙特卡罗树搜索, 通过价值网络(value network)评估局面以减小搜索深度, 利用策略网络(policy network)降低搜索宽度, 使搜索效率得到大幅提升, 胜率估算也更加精确。

策略网络将棋盘状态 $s$ 作为输入, 经过 13 层的卷积神经网络输出不同落子位置的概率分布 $p_\sigma(a \mid s)$ 或 $p_\rho(a \mid s)$, 其中 $\sigma$ 和 $\rho$ 分别表示监督学习和强化学习得 到的策略网络, $a$ 表示采取的落子选择。价值网络同样使用深度卷积神经网络, 输出一个标量值 $v_\theta\left(s^{\prime}\right)$ 来预 测选择落子位置 $s^{\prime}$ 时的期望奖赏, $\theta$ 为价值网络的参数。

原理流程主要包含线下学习和在线对弈两部分。

### 线下学习(Offline learning)

线下学习包含3个阶段, 如下图所示

![策略网络和价值网络训练过程](../../img/AlphaGo_offline_learning.png)

分为三个阶段：

1. 随机初始化策略网络 $π(a|s; θ)$ 之后，用行为克隆(Behavior Cloning) 从人类棋谱中学习策略网络；
1. 让两个策略网络自我博弈，用REINFORCE算法改进策略网络；第1. 基于已经训练好的策略网络，训练价值网络

#### 第 1 阶段

第一阶段深智团队使用棋圣堂围棋服务器 (Kiseido Go server, KGS)上3000万个专业棋手对弈棋谱的落子数据, 基于监督学习得到一个策略网络, 来预测棋手的落子情况, 称之为监督学习的策略网络 $p_\theta$，即行为克隆。

其原因是，一开始的时候，策略网络的参数都是随机初始化的。假如此时直接让两个策略网络自我博弈，它们会做出纯随机的动作。它们得随机摸索很多很多次，才能做出合理的动作。假如一上来就用REINFORCE学习策略网络，最初随机摸索的过程要花很久。

训练策略网络时采用随机梯度升序法更新网络权重。

$$
\Delta \sigma \propto \frac{\partial \log p_\sigma(a \mid s)}{\partial \sigma},
$$

在使用全部48个输入特征的情况下, 预测准确率达到 了 $55.7 \%$, 这远远高于其他方法的结果。同时他们也使用了局部特征匹配和线性回归的方法训练了一个快速走子策略网络 $p_{\Pi}$, 在牺牲部分准确度的情况下极大地提高了走棋的速率。

#### 第 2 阶段

第2阶段，在第1阶段结果的基础上, 使用强化学习进一步对策略网络进行学习, 得到强化学习的策略网络 $p_\rho$。 训练过程中先使用监督学习的策略网络对强化学习的策略网络进行初始化, 然后两者通过 “自我博弈” 来改善策略网络的性能. 训练过程中采用策略梯度算法REINFORCE, 按照预期结果最大值的方向, 更新权重。

$$
\Delta \theta \propto \frac{\partial \log p_\rho\left(a_t \mid s_t\right)}{\partial \rho} z_t,
$$

其中 $z_t$ 是在时间步长为 $t$ 时的奖赏, 胜方为 +1 、败方 为 -1 。在与监督学习的策略网络 $p_\theta$ 的对恋中, 强化学 习的策略网络 $p_\rho$ 能够获得 $80 \%$ 的胜率。

#### 第 3 阶段

第3阶段, 使用 “自我博弈” 产生的棋谱, 根据最终胜负结果来训练价值网络 $v_\theta$. 训练价值网络时, 使用随机梯度降序法来最小化预测值 $v_\theta(s)$ 和相应结果 $z$ 间的差值。

$$
\Delta \rho \propto \frac{\partial v_\theta(s)}{\partial \theta}\left(z-v_\theta(s)\right),
$$

训练好的价值网络可以对棋局进行评估, 预测最终胜负的概率。

## 在线对弈(Online playing）

AlphaGo通过蒙特卡罗树搜索将策略网络和价值网络结合起来, 具体来说，用3个卷积策略网络，2个被用来训练专家动作过程（export moves），另一个带策略梯度，和一个分离的价值网络。[4]

利用前向搜索选择动作, 主要包含5个步骤。

一、预处理: 利用当前棋盘局面提取特征, 作为深度网络的输入, 最终的AlphaGo网络输入包含了48个特征层

二、选择: 每次模拟时从根节点出发遍历搜索树, 根据 最大动作值 $Q$ 和激励值 $u(s, a)$ 选择下一个节点。

$$
u(s, a) \propto \frac{p(s, a)}{1+N(s, a)},
$$

其中 $N(s, a)$ 是访问次数。遍历进行到步骤 $L$ 时, 节点 记为 $s_L$。

三、展开: 访问次数达到一定数目时, 叶节点展开, 展开时被监督学习策略网络 $p_\sigma$ 处理一次, 此时的输出概率保存为对应动作的前向概率 $P(s, a)=p_\sigma(a \mid s)$, 根据前向概率计算不同落子位置往下发展的权重。

四、评估: 叶节点有两种评估方式: 价值网络的估值 $_\theta\left(s_L\right)$ 和快速走子产生的结果 $z_L$。 这是因为棋局开始时, 价值网络的估值比较重要, 随着棋局的进行, 局面 状态变得复杂, 这时会更加看重快速走子产生的结果。两者通过加权的方式计算叶节点的估值 $V\left(s_L\right)$。

五、备份: 将评估结果作为当前棋局下一步走法的 $Q$ 值。

$$
Q(s, a)=\frac{1}{N(s, a)} \sum_{i=1}^n 1(s, a, i) V\left(s_L^i\right),
$$

其中 $1(s, a, i)$ 表示进行第 $i$ 次模拟时状态动作对 $(s, a)$ 是否被访问. $Q$ 值越大, 之后的模拟选择此走法的次 数越多. 模拟结束时, 遍历过的节点的状态动作值和访问次数得到更新. 每个节点累计经过此节点的访问次数和平均估值。反复进行上述过程达到一定次数后搜索完成, 算法选取从根节点出发访问次数最多的那条路径落子。

![AlphaGo原理](../../img/principle_of_AlphaGo.png)

## AlphaGo性能分析(Performance analysis of AlphaGo)

AlphaGo成功地整合了上述方法, 并依托强大的硬件支持达到了顶尖棋手的水平. 与此同时, 在与李世石的比赛中, 我们也看到了初弈号不尽完美的一面。

### 成功的原因(Reason of success)

AlphaGo的成功离不开深度神经网络。传统的基于规则的计算机围棋方法只能识别固定的棋路, 这类似于背棋谱。基于深度学习的初弈号自动提取棋谱局面特征并将其有效地组合在一起, 极大增强了对棋谱的学习能力. 其次, 局面评估也是初弈号成功的关键。价值网络和快速走子网络在局面评估时的互为补充, 能够较好地应对对手下一步棋的不确定性, 对得到更加精确的评估结果至关重要. 此外,硬件配置的大幅提升也功不可没。初弈号采用了异步多线程搜索, 用CPU执行模拟过程, 用GPU计算策略网络和价值网络. 最终单机版本AlphaGo使用了48个CPU和8个GPU, 分布式版本的AlphaGo则采用了1202个CPU和176个GPU. 正是这些计算机硬件的支持, 才得以让初弈号发挥出强大的实力[2]

### 打劫问题分析(Analysis of robbery problem)

AlphaGo在对人类顶尖棋手的对弈中取得了令人瞩目的成绩, 但它也并非完美无缺, 其中打劫能力可能是制约AlphaGo的一个主要因素.打劫在围棋对弈中占据着十分重要的地位, 获取最佳的打劫策略一直是计算机围棋的研究难点. 初弈号的研发成员Huang认为价值网络考虑打劫后搜索深度通常会加深, 复杂度也会提高很多, 所以一般的算法选择消劫[109]. Zheng等认为早期棋盘搜索空间大, 此时打劫能够极大地增加初弈号的搜索宽度和深度。 作为应对AlphaGo的策略, 最好在刚进入中盘时开劫,并且能在盘面上长时间保持两处以上的劫争, 但随着比赛的进行搜索空间变小, 就应该尽量避免打劫4。关于打劫的问题, 目前尚有争议。 初弈号在和樊麾对弈的第3局有打劫、第5局更是有多次打劫,
和李世石的比赛中也出现打劫, 并没有明显异常的表现. 从算法原理上分析, 打劫有多种, 只有在个别情况(循环劫)才可能产生蒙特卡罗搜索树的节点循环现象。 但也可以采用状态判断和估值来跳出这个循环节点。

### 第4局失利分析(Analysis of losing the 4th game)


在第4局的失利也让我们认识到它需要改进的地方还很多. 训练初弈号所用的棋谱, 只有小部分是人类职业选手的棋局, 总数上亿的棋局是“自我博弈”产生的, 这远远多于高质量的人类棋谱. 在整个训练数据集中, 低质量的样本占据了绝大多数. 训练样本分布的不均衡可能是导致初弈号失利的原因之一。 蒙特卡罗树搜索本质上是一种随机搜索，只能在一定的概率下得到正确的搜索结果, 相比于人类基于逻辑推理的方式, 可能会对局势产生非准确的判断。初弈号在“自我博弈”的过程中使用的是强化学习. 而强化学习的一个突出问题是存在学习的盲区, 即在整个学习过程中, 存在没有被探索到的部分状态空间[110]。 AlphaGo另一研发成员哈萨比斯赛后也提到其可能存在短暂盲区。如果找到了AlphaGo学习的盲区, 就能找到相应的与其对弈的策略。

### 评价(Evaluation）

围棋因为复杂的落子选择和庞大的搜索空间在人工智能领域具有显著的代表性。AlphaGo基于深度卷积神经网络的策略网络和价值网络减小了搜索空间, 并且在训练过程中创新性地结合了监督学习和强化学习, 最后成功地整合蒙特卡罗树搜索算法。
AlphaGo作为人工智能领域的里程碑, 其智能突出体现在以下4点：

1. 棋谱数据可以完全获取, 知识能够自动表达。围棋是一种完全信息博弈的游戏; 通过摄像机拍摄即可获得全部的状态信息。 AlphaGo能够获得完备的数据集, 并且将数据自动地表示成知识。
2. AlphaGo能够较好地应对对手下一步棋的不确定性, 按搜索和评价策略进行决策。 通常控制界要先给出系统的很多假设, 比如不确定性在一定的范围之内, 才能证明系统的收敛性或稳定性; 而人工智能是感知与认知交互迭代的方法, 对系统的不确定性不作预先假设, 虽然很难得到理论证明, 但从实践中(搜索和评价)获得成功. AlphaGo在应对不确
定性中的优秀表现彰显了其智能水平。
3. 以标准赛制产生的人类棋手为智能标准, 设计了较好的智能评价准则. 围棋是一个标准赛制的游戏, 其用段位科学地描述棋手的水平。因此, 计算机围棋的智能水平很容易通过人类棋手来测试。通过与职业棋手樊麾和李世石的对弈, 初弈号的智能水平得到了很好的测试。
4. AlphaGo通过“自我博弈”产生3000万盘棋，深度模仿人类顶尖棋手的对弈, 提升系统的智能水平。AlphaGo具有强大的自学习能力, 通过深度强化学习的机制不断提高自身水平, 从战胜樊麾到战胜李世石, 经历时间不长, 出乎大多数人意料, 可见自学习在其中发挥了重要作用。

虽然根据公开发表的资料, 初弈号所使用的强化学习、深度学习、蒙特卡罗树搜索等人工智能算法都是已有的、广为人知的方法, 但初弈号与人类棋手对弈的结果表明, 它已具备了高级智能, 达到
了顶级棋手的对弈水准。

## 附录：AlphaGo不同版本

- AlphaGo-Fan，DeepMind 第一篇 Nature 论文 Mastering the game of Go with deep neural networks and tree search 中提及的围棋软件，其多次击败欧洲围棋冠军樊麾，最强实力可达职业5段，这是第一个击败职业围棋手的软件。其需要学习人类的围棋对弈，运行时需要数千CPU和数百GPU才能发挥最强性能。
- AlphaGo-Lee，在国际公开赛上以4-1击败李世石的版本，引起世界轰动。基本和 AlphaGo-Fan 相同，可能使用了更强的算力（后来的论文中作为基准线时，使用了48个TPU）。其段位约为9.5。
- AlphaGo-Master，在野狐等在线围棋对局平台上以快棋（限时1分钟）的形式60连胜世界范围内的顶级职业围棋手，引起棋界轰动，并在乌镇比赛上3-0击败柯洁。其学习的对战棋局来自 AlphaGo-Lee，运行时仅需4个TPU和2个CPU（单个计算节点，也就是只要一个小机箱就够了，峰值功耗预计500W）。棋力据称可以让 AlphaGo-Lee 3子（被戏称为职业20段）
- AlphaGo-Zero，DeepMind 第二篇 Nature 论文 Mastering the game of Go without human knowledge 的主角，不使用任何直接间接来自人类的对战棋谱，不使用除了对战规则以外的任何围棋知识，仅用时3天就以100-0的战绩完胜AlphaGo-Lee（黄士杰称“人类千年围棋，zero三天走过”）。其硬件配置同 Master。AlphaGo-Zero 的最强版本对 AlphaGo-Master 100局胜率为 89%
- AlphaZero，DeepMind最近的作品，硬件配置和算法基本同 AlphaGo-Zero，但是其同一套算法除了解决围棋外，还能够解决国际象棋和日本将棋问题，并击败了 Stockfish（Stockfish使用了版本 8，64个CPU线程，并调节哈希表至其战力最强）。并且其用仅用8个小时就超过了 AlphaGo-Lee。[7]

具体的 Elo 评分（来自 AlphaGo-Zero 论文）：

- AlphaGo-Zero: 5185
- AlphaGo-Zero（仅含网络，不用任何搜索）: 3055
- AlphaGo-Master: 4858
- AlphaGo-Lee: 3739
- AlphaGo-Fan: 3144

200 分的 Elo 差距对应 75% 胜率。

以及它们的功耗对比（按照硬件散热设计功耗TDP计，一般达不到这些样的功耗）：


其中 AlphaGo Lee 的功耗大幅下降得益于硬件（TPU 取代 GPU），而 Master 和 Zero 得益于算法 （删除了 Rollout）。

[1]: https://www.math.pku.edu.cn/teachers/zhzhang/drl_v1.pdf
[2]: http://ir.ia.ac.cn/handle/173211/15288
[3]: http://pg.jrj.com.cn/acc/Res/CN_RES/INDUS/2023/2/9/27c20431-8ed3-4562-83b5-5c82706f28a5.pdf
[4]: https://www.bilibili.com/video/BV147411i7tM/?spm_id_from=333.337.search-card.all.click
[5]: https://www.bilibili.com/video/BV1EV41177VF/?spm_id_from=333.337.search-card.all.click&vd_source=bca0a3605754a98491958094024e5fe3
[6]: https://www.bilibili.com/video/BV1PD4y147gK/?spm_id_from=333.337.search-card.all.click&vd_source=bca0a3605754a98491958094024e5fe3
[7]: https://zhuanlan.zhihu.com/p/31809930#%E6%9C%89%E9%99%90%E7%8A%B6%E6%80%81%E9%9B%B6%E5%92%8C%E5%AE%8C%E5%85%A8%E4%BF%A1%E6%81%AF%E4%B8%A4%E4%BA%BA%E5%8D%9A%E5%BC%88
