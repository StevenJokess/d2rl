

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-03-23 22:54:02
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-04-10 01:12:58
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 机器人

## 传统的机器人控制的缺陷与机器人学习的必要性

传统的机器人控制大多基于定制化程序解决固定任务, 只适合简单的结构化环境。而对于各种复杂环境下的复杂任务, 机器人本身的最优行为并不唯一, 缺乏固定范式, 因此无法对机器人策略进行预编程。近年来, 学习算法的快速发展使解决此类问题成为可能。机器人学习(Robot Learning)是机器学习和机器人技术的交叉领域, 目的是研究让机器人通过与环境自主交互和学习, 获得新技能以适应环境, 使机器人能完成复杂任务。机器人学习涉及计算机视觉、自然语言处理、机器人控制等多个领域的研究。

## 深度强化学习的“深度”必要性

在强化学习(Reinforcement Learning, RL)中, 机器人可通过与世界环境交互, 从环境中获得反馈并优化自己的行动策略。但在3D仿真环境中，机器人手脚弯曲的状态是完全不可数的。所以需要结合深度学习的深度强化学习(Deep RL, DRL), 使用神经网络表征策略和状态, 对复杂的环境更鲁棒, 更适用于机器人学习中的复杂任务.而基于模型的强化学习(Model-Based RL, MBRL)作为强化学习的一个主要分支, 通常会基于交互信息学习一个环境动力学模型(Dynamics Model), 并基于该模型生成数据优化行动策略, 或利用模型进行规划.无模型的强化学习(Model-Free RL, MFRL)不需要建模, 简单直观, 样本效率较低, 但是渐进性能较高, 适用于游戏领域.而基于模型的强化学习需要学习模型, 学到模型后会有较高的样本效率, 但缺陷是对于有些复杂任务, 模型不容易学到。在机器人控制领域, 模型都有确定的物理规律作为指导, 动作空间相对较小, 模型容易学到, 因此基于模型的强化学习算法更适用。

## 重要进展

目前在机器人控制领域, 强化学习算法取得重要进展, OpenAI等[3]通过深度强化学习和虚拟到现实(Sim to Real)的方法, 训练一只可玩魔方的机械手, 灵活性甚至超过人类。

相比无模型算法, 基于模型的强化学习算法在机器人上应用更广泛.Levine等[4]基于引导策略搜索(Guided Policy Search), 可让机器人直接从摄像机原始像素中学习灵巧操作, 实现端到端的机器人操作控制.Fazeli等[5]让机器人学习融合视觉和触觉的多模态的推理模型, 结合规划使机器人学会玩叠叠乐(Jenga).Fisac等[6]利用系统动力学模型的近似知识结合贝叶斯机制, 构建机器人控制的通用安全框架, 有效提升机器人操作中的安全性.基于模型的机器人学习算法可从样本中学习一个环境模型, 基于模型学习操作技能, 更适合高维图像输入, 更接近人类的学习方式, 有助于构建人类期望中的机器人.

2019年，Google Brain的Andy Zeng团队，通过强化学习算法与常规控制器结合的方式（Residual Reinforcement）设计的机械臂控制系统Tossingbot，使得机械臂可以完成抓取物体和抛掷物体的能力，戳视频。

##

本文首先从机器人学习问题的建模入手, 介绍马尔科夫决策过程的相关概念, 并将机器人学习问题形式化.详细介绍机器人学习中基于模型的强化学习方法, 包括主流的模型学习及模型利用的方法.主流的模型学习方法具体介绍前向动力学模型、逆向动力学模型和隐式模型.模型利用的方法具体介绍基于模型的规划、基于模型的策略学习和隐式规划, 并对其中存在的问题进行探讨.最后, 结合现实中机器人学习任务面临的问题, 介绍基于模型的强化学习在其中的应用, 并展望基于模型的强化学习未来的研究方向.

1 机器人学习问题的形式化
机器人的长时决策问题[7]通常可表示为一个马尔科夫决策过程(Markov Decision Process, MDP), 用一个五元组表示为(S, A, P, R, γ ).S⊆Rn为状态空间(State Space), 是环境状态(State)构成的集合; A⊆Rn为动作空间(Action Space), 是机器人动作(Action)构成的集合; P(s'|s, a)为状态转移概率(Transition Probability), 描述系统的环境动力学模型, 即机器人在状态s时执行动作a到达下一个状态s'的概率分布; R(s, a, s')为奖励函数(Reward Function), 表示机器人在状态s时执行动作a到达状态s'获得的即时奖励; γ ∈ [0, 1]为折扣因子, 计算累计奖励时降低未来回报对当前的影响.学习的目的是找到一个从状态到动作的映射π, 称为策略(Policy), 使累积折扣奖励 ∑k=0∞
γ krk最大化.这样建模可概括机器人学习的任务, 研究人员也可据此研究通用算法.机器人操作的学习问题涉及到学习定义状态空间、学习环境动力学模型、学习运动技能.在机器人强化学习算法中, 状态空间是给定的, 主要学习运动控制策略.环境动力学模型通常是未知的.基于模型的强化学习会通过与环境交互学习环境动力学模型, 以此为基础再进行策略的学习.

将具有相似结构的任务建模为一个MDP的集合(MDPs), 称为一个任务族.使用P(M)表示任务的分布, 每个MDP都是一个特定的任务.任务族中所有任务的动作空间都是相同的, 由机器人决定.但是任务之间的状态空间可能有所不同.第i个任务的状态空间可表示为机器人自身的状态空间和第i个任务环境的状态空间的笛卡尔积:

Si=Sr× Sie
,

其中, Sr表示机器人自身的状态空间, Sie
表示第i个任务对应环境的状态空间, 可以是原始图像、传感器的数值或预处理的相关任务变量的集合.相应地, 任务族中每个任务的奖励函数也由两部分组成:

Ri=C+Gi,

其中, C表示任务族中公共环境背景的损失函数, Gi表示第i个任务的奖励函数.任务族中的多数任务环境可视为物体集合, 因此也可由描述任务相关物体的变量集合描述.可将环境模型进一步分解为物体状态的集合:

Sie
= Siw
× Ωi1
× …× Ωiki
,

其中, Siw
为公共的环境背景状态, Ωij
为第i个任务中第j个相关物体的状态, ki表示第i个任务环境中包含ki个相关物体.这样建模任务有一个明显的优点, 针对某些物体学到的模型和策略在包含这些物体的新环境中可重复利用[7].



[1]: http://manu46.magtech.com.cn/Jweb_prai/article/2022/1003-6059-35-1-1.html
[2]: https://paddlepedia.readthedocs.io/en/latest/tutorials/reinforcement_learning/DQN.html#id1
[3]: https://www.guyuehome.com/21403
