# 不完全信息博弈

## NFSP

Neural Fictitious Self-Play (NFSP)

Silver大神[2]16年的Fictitious Self-Play Deep Reinforcement Learning from Self-Play in Imperfect-Information Games[3]等

Neural Fictitious Self-Play (NFSP) [paper] end-to-end approach to solve card games with deep reinforcement learning. NFSP has an inner RL agent and a supervised agent that is trained based on the data generated by the RL agent. In the toolkit, we use DQN as RL agent.[4]

### 虚拟自我对局 Fictitious Self Play, FSP

虚拟对局是从自我对局中学习的博恋论模型。自我对局的参与人选择针对其对手的平均行为的最佳反应。自我对局 参与人的平均策略在某些类型的博栾 (如二人零和博栾和多人势力场博恋) 中都会收玫到纳什均衡。Leslie 和 Collins 在 2006 年给出了推广的弱化自我对局。这种模型和通常的自我博恋有着类似的收玫保证，但是允许有近似 最优反应和扰动的平均策略更新，也让这个扩展模型对机器学习尤其合适。
自我对局通常使用规范式博恋定义，这与扩展式博栾在有效性上存在指数级差距。Heinrich 等人在 2005 年引入了 Full-Width Extensive Form FSP (XSP)，可以让自我对局参与人使用行为式，扩展式进行策略更新，得到了线性 的时间和空间复杂性。这里一个关键的洞察是对规范式策略的凸组合， $\hat{\sigma}=\lambda_1 \hat{\pi}_1+\lambda_2 \hat{\pi}_2$ ，我们可以达到一个实 现等价的行为策略 $\sigma$  ， 通过设置该值为成比例于对应的实现概率的凸组合，

$$
\sigma(s, a) \propto \lambda_1 x_{\pi_1}(s) \pi_1(s, a)+\lambda_2 x_{\pi_2}(s) \pi_2(s, a) \forall s, a
$$

其中 $\lambda_1 x_{\pi_1}(s)+\lambda_2 x_{\pi_2}(s)$ 是在信息状态 $s$ 的策略的规范化常量。除了在行为策略下定义自我对局的 full-width 平 均策略更新外，公式 (1) 给出了从这样的策略的凸组合中采样数据集的一种方法。Heinrich 等人在 2015 提出了 Fictitious Self Play (FSP) 的基于采样和机器学习的算法来近似 XFP。FSP 分别用强化学习和监督学习来替换最 优反应计算和平均策略更新。特别地，FSP agent 产生他们在自我对局中的经验转换的数据集。每个 agent 存放了 自身的转移元组 $\left(s_t, a_t, r_{t+1}, s_{t+1}\right)$ 在记忆 $\mathcal{M}_{R L}$ 中，这个为强化学习设计。而 agent 自身的行为 $\left(s_t, a_t\right)$ 被存放在另一个分开的记忆 $\mathcal{M}_{S L}$ 中，这为监督学习设计。自我对局的采样通过 agent 的强化学习的记忆来近似用其他 参与人的平均策略组合定义的 MDP 的数据。因此，通过强化学习的 MDP 的近似解会产生一个近似的最优反应。类似地，agent 的监督学习记忆近似了 agent 本身的平均策略，这可以通过监督式的分类方法学习。

###


## CFR

Counterfactual Regret Minimization (CFR)

Counterfactual Regret Minimization (CFR) [paper] is a regret minimizaiton method for solving imperfect information games.[4]

[1]: https://rlcard.org/
[2]: https://zhuanlan.zhihu.com/p/25239682
[3]: https://arxiv.org/abs/1603.01121
[4]: https://rlcard.org/algorithms.html#nfsp
[5]: http://tigerneil.github.io/2016/05/30/nsfp/
