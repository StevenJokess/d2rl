

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-04-25 21:56:45
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-05-16 03:01:28
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 不完美信息的机器博弈

## 博弈论与计算机科学

M(·冯·诺依曼：现代计算机之父+现代博弈论之父

博弈论与计算机科学的交叉领域非常多：

- 理论计算机科学：算法博弈论·
- 人工智能：多智能体系统、A游戏玩家、人机交互、机器学习、广告推荐
- 互联网：互联网经济、共享经济·
- 分布式系统：区块链
人工智能与博弈论相互结合，形成了两个主要研究方向·博弈策略的求解博弈规则的设计

人工智能与博弈论相互结合，形成了两个主要研究方向：

- 博弈策略的求解
- 博弈规则的设计

## 博弈策略求解

动机：
- 博弈论提供了许多问题的数学模型。
- 纳什定理确定了博弈过程问题存在解
- 人工智能的方法可用来求解均衡局面或者最优策略

主要问题：如何高效求解博弈参与者的策略以及博弈的均衡局势？

应用领域：
- 大规模搜索空间的问题求解：围棋
- 非完全信息博弈问题求解：德州扑克
- 网络对战游戏智能：Dota、星球大战
- 动态博弈的均衡解：厂家竞争、信息安全

## NFSP

Neural Fictitious Self-Play (NFSP)

Silver大神[2]16年的Fictitious Self-Play Deep Reinforcement Learning from Self-Play in Imperfect-Information Games[3]等

Neural Fictitious Self-Play (NFSP) [paper] end-to-end approach to solve card games with deep reinforcement learning. NFSP has an inner RL agent and a supervised agent that is trained based on the data generated by the RL agent. In the toolkit, we use DQN as RL agent.[4]

### 虚拟自我对局 Fictitious Self Play, FSP

虚拟对局是从自我对局中学习的博弈论模型。自我对局的参与人选择针对其对手的平均行为的最佳反应。自我对局 参与人的平均策略在某些类型的博栾 (如二人零和博栾和多人势力场博弈) 中都会收玫到纳什均衡。Leslie 和 Collins 在 2006 年给出了推广的弱化自我对局。这种模型和通常的自我博弈有着类似的收玫保证，但是允许有近似 最优反应和扰动的平均策略更新，也让这个扩展模型对机器学习尤其合适。
自我对局通常使用规范式博弈定义，这与扩展式博栾在有效性上存在指数级差距。Heinrich 等人在 2005 年引入了 Full-Width Extensive Form FSP (XSP)，可以让自我对局参与人使用行为式，扩展式进行策略更新，得到了线性 的时间和空间复杂性。这里一个关键的洞察是对规范式策略的凸组合， $\hat{\sigma}=\lambda_1 \hat{\pi}_1+\lambda_2 \hat{\pi}_2$ ，我们可以达到一个实 现等价的行为策略 $\sigma$  ， 通过设置该值为成比例于对应的实现概率的凸组合，

$$
\sigma(s, a) \propto \lambda_1 x_{\pi_1}(s) \pi_1(s, a)+\lambda_2 x_{\pi_2}(s) \pi_2(s, a) \forall s, a
$$

其中 $\lambda_1 x_{\pi_1}(s)+\lambda_2 x_{\pi_2}(s)$ 是在信息状态 $s$ 的策略的规范化常量。除了在行为策略下定义自我对局的 full-width 平 均策略更新外，公式 (1) 给出了从这样的策略的凸组合中采样数据集的一种方法。Heinrich 等人在 2015 提出了 Fictitious Self Play (FSP) 的基于采样和机器学习的算法来近似 XFP。FSP 分别用强化学习和监督学习来替换最 优反应计算和平均策略更新。特别地，FSP agent 产生他们在自我对局中的经验转换的数据集。每个 agent 存放了 自身的转移元组 $\left(s_t, a_t, r_{t+1}, s_{t+1}\right)$ 在记忆 $\mathcal{M}_{R L}$ 中，这个为强化学习设计。而 agent 自身的行为 $\left(s_t, a_t\right)$ 被存放在另一个分开的记忆 $\mathcal{M}_{S L}$ 中，这为监督学习设计。自我对局的采样通过 agent 的强化学习的记忆来近似用其他 参与人的平均策略组合定义的 MDP 的数据。因此，通过强化学习的 MDP 的近似解会产生一个近似的最优反应。类似地，agent 的监督学习记忆近似了 agent 本身的平均策略，这可以通过监督式的分类方法学习。

###


## 遗憾最小化算法(Regret Minimization)

- 假设一共有 $N$ 个玩家。玩家 $i$ 所采用的策略表示为 $\sigma_i$ 。[6]
- 对于每个信息集 $I_i \in \xi_i, \sigma_i\left(I_i\right): A\left(I_i\right) \rightarrow[0,1]$ 是在动作集 $A\left(I_i\right)$ 上的概率分布函数。玩家 $i$ 的策略空间用 $\Sigma_i$ 表示。
- 一个策略组包含所有玩家策略，用 $\sigma=\left(\sigma_1, \sigma_2, \ldots, \sigma_{|N|}\right)$ 。
- $\sigma_{-i}$ 表示 $\sigma$ 中除了 $\sigma_i$ 之外的策略 (即除去玩家 $i$ 所采用的策略)
- 在博弈对决中, 不同玩家在不同时刻会采取相应策略以及行动。策略 $\sigma$ 下对应的行动序列 $h$ 发生的概率表示为 $\pi^\sigma(h)$ 。于是, $\pi^\sigma(h)=\prod_{i \in N} \pi_i^\sigma(h)$, 这里 $\pi_i^\sigma(h)$ 表示玩家 $i$ 使用策略 $\sigma_i$ 促 使行动序列 $h$ 发生的概率。除玩家 $i$ 以外, 其他玩家通过各自策略促使行动序列 $h$ 发生的概率 可表示为: $\pi_{-i}^\sigma(h)=\prod_{j \in N \backslash\{i\}} \pi_j^\sigma(h)$
- 对于每个玩家 $i \in N, u_i: Z \rightarrow R$ 表示玩家 $i$ 的收益函数, 即在到达终止序列集合 $Z$ 中某个终 止序列时, 玩家 $i$ 所得到的收益。
- 玩家 $i$ 在给定策略 $\sigma$ 下所能得到的期望收益可如下计算: $u_i(\sigma)=\sum_{h \in Z} u_i(h) \pi^\sigma(h)$

### $\varepsilon$-纳什均衡与平均遗憾值

- $\varepsilon$-纳什均衡 :
  - 对于给定的正实数 $\varepsilon$, 策略组 $\sigma$ 是 $\varepsilon$ 一纳什均衡当且仅当对于每个玩家 $i \in N$, 满足如下 条件:
  - $u_i(\sigma)+\varepsilon \geq \max _{\sigma_i^{\prime} \in \Sigma_i} u_i\left(\sigma_i^{\prime}, \sigma_{-i}\right)$

- 平均遗憾值(average overall regret)：假设博弈能够重复地进行（如围棋等），令第 $t$ 次博弈时的策略组为 $\sigma^t$, 若博弈已经进行了 $M$ 次, 则这 $M$ 次博弈对于玩家 $i \in N$ 的平均遗憾值 定义为:
$$
\overline{\operatorname{Regret}_i^M}=\frac{1}{M} \max _{\sigma_i \in \Sigma_i} \sum_{i=1}^M\left(u_i\left(\sigma_i^*, \sigma_{-i}^t\right)-u_i\left(\sigma^t\right)\right)
$$

### 策略选择

遗憾最小化算法是一种根据过去博弈中的遗憾程度来决定将来动作选择的方法

在博弈中, 玩家 $i$ 在第 $T$ 轮次 (每一轮表示一次博弈完成）采取策略 $\sigma_i$ 的遗憾值定义如 下（累加遗憾）:

$$
\operatorname{Regret}_i^T\left(\sigma_i\right)=\sum_{t=1}^T\left(\mu_i\left(\sigma_i, \sigma_{-i}^t\right)-\mu_i\left(\sigma^t\right)\right)
$$

通常遗憾值为负数的策略被认为不能提升下一时刻收益, 所以这里考虑的遗憾值均为 正数或 0

计算得到玩家 $i$ 在第 $T$ 轮次采取策略 $\sigma_i$ 的遗憾值后, 在第 $T+1$ 轮次玩家 $i$ 选择策略 $a$ 的概 率如下（悔值越大、越选择，即亡羊补牢）

$$
P(a)=\frac{\operatorname{Regret}_i^T(a)}{\bar{L}_{b \in \in \text { 所有可选择策略\} }} \text { Regret }_i^T(b)}
$$


### 遗憾最小化算法: 石头-剪刀-布的例子

- 假设两个玩家A和B进行石头-剪刀-布（Rock-Paper-Scissors, RPS）的游戏, 获胜 玩家收益为 1 分, 失败玩家收益为 1 分, 平局则两个玩家收益均为零分
- 第一局时，若玩家 $A$ 出石头（R）, 玩家 $B$ 出布（P），则此时玩家 $A$ 的收益 $\mu_A(R, P)=-1$, 玩家 $\mathrm{B}$ 的收益为 $\mu_B(P, R)=1$
- 对于玩家A来说, 在玩家B出布 (P) 这个策略情况下, 如果玩家A选择出布
$(\mathrm{P})$ 或者剪刀 (S) ，则玩家A对应的收益值 $\mu_A(P, P)=0$ 或者 $\mu_{\mathrm{A}}(S, P)=1$
- 所以第一局之后, 玩家A没有出布的遗憾值为 $\mu_A(P, P)-\mu_A(R, P)=0-(-1)= 1$ , 没有出剪刀的遗憾值为 $\mu_A(S, P)-\mu_A(R, P)=1-(-1)=2$
- 所以在第二局中, 玩家 $\mathrm{A}$ 选择石头、剪刀和布这三个策略的概率分别为 0 、 2 / 3、1/3。因此，玩家A趋向于在第二局中选择出剪刀这个策略

玩家 $i$ 每一轮悔值计算公式: $\quad \mu_i\left(\sigma_i, \sigma_{-i}^t\right)-\mu_i\left(\sigma^t\right)$

在第一轮中玩家 $A$ 选择石头和玩家B选择布、在第二局中玩家 $A$ 选择剪刀和玩家 $B$ 选择石 头情况下，则玩家A每一轮遗憾值及第二轮后的累加遗憾取值如下:

| 每轮悔值 策略 | 石头 | 剪刀 | 布 |
| :---: | :---: | :---: | :---: |
| 第一轮悔值 | 0 | 2 | 1 |
| 第二轮悔值 | 1 | 0 | 2 |
| $\operatorname{Regret}_A^2$ | 1 | 2 | 3 |

- 从上表可知, 在第三局时, 玩家A选择石头、剪刀和布的概率分别为 $1 / 6 、 2 / 6 、 3 / 6$
- 在实际使用中, 可以通过多次模拟迭代累加遗憾值找到每个玩家在每一轮次的最优策略
- 但是当博弈状态空间呈指数增长时, 对一个规模巨大的博恋树无法采用最小遗憾算法

## 虚拟遗憾最小化算法 Counterfactual Regret Minimization (CFR)

Counterfactual Regret Minimization (CFR) [paper] is a regret minimizaiton method for solving imperfect information games.[4]

- 如果不能遍历计算所有节点的遗憾值, 那么可以采用虚拟遗憾最小化算法来进行模拟计算
- 假设：
  - 集合 $A$ 是博恋中所有玩家所能采用的行为集 (如在石头-剪刀-布游戏中出石头、出剪刀或出布三种行为)
  - $I$ 为信息集, 包含了博弈的规则以及玩家采取的历史行动, 在信息集 下所能采取的行为集合记为 $A(I)$
- 玩家 $i$ 在第 $t$ 轮次采取的行动 $a_i \in A\left(I_i\right)$ 反映了其在该轮次所采取的策略 $\sigma_i^t$ 。包含玩家 $i$ 在内的 所有玩家在第 $t$ 轮次采取的行动 $a \in A(I)$ 构成了一组策略组合 $\sigma^t$ 。
- 在信息集 $I$ 下采取行动 $a$ 所反映的策略记为 $\sigma_{I \rightarrow a}$ 。
- 在第 $t$ 轮次所有玩家采取的行动是一条序列, 记为 $h$ 。采取某个策略 $\sigma$ 计算行动序列 $h$ 出现的概率记为 $\pi^\sigma(h)$
- 每个信息集 $I$ 发生的概率 $\pi^\sigma(I)=\sum_{h \in I} \pi^\sigma(h)$, 表示所有能够到达该信息集的行动序 列的概率累加。
- 给定博弈的终结局势 $z \in Z ，$ 玩家 $i$ 在游戏结束后的收益记作 $u_i(z)$
- 在策略组合 $\sigma$ 下, 施加博恋行动序列 $h$ 后达到最终局势 $z$ 的概率为 $\pi^\sigma(h, z)$

当采取策略 $\sigma$ 时, 其所对应的行动序列 $h$ 的虚拟价值（Counterfactual Value）如下计算(注: 行动序列 $h$ 末能使博弈进入终结局势):

$$
v_i(\sigma, h)=\sum_{z \in Z} \pi_{-i}^\sigma(h) \pi^\sigma(h, z) u_i(z)
$$

玩家 $i$ 采取行动 $a$ 所得到的虚拟遗憾值:

$$
r(h, a)=v_i\left(\sigma_{I \rightarrow a}, h\right)-v_i(\sigma, h)
$$

行动序列 $h$ 所对应的信息集 $I$ 遗憾值为:

$$
r(I, a)=\sum_{h \in I} r(h, a)
$$

玩家 $i$ 在第 $T$ 轮次采取行动的遗憾值为:

$$
\operatorname{Regret}_i^T(I, a)=\sum_{t=1}^T r_i^t(I, a)
$$

同样, 对于遗憾值为负数的情况, 我们不予考虑, 记:

$$
\operatorname{Regret}_i^{T,+}(I, a)=\max \left(R_i^T(I, a), 0\right)
$$

在 $T+1$ 轮次, 玩家 $i$ 选择行动 $a$ 的概率计算如下:

$$
\sigma_i^{T+1}(I, a)=\left\{\begin{array}{cc}
\frac{\text { Regret }_i^{T,+}(I, a)}{\sum_{a \in A(I)} \operatorname{Regret}_i^{T,+}(I, a)} & \text { if } \sum_{a \in A(I)} \operatorname{Regret}_i^{T,+}(I, a)>0 \\
\frac{1}{|A(I)|} & \text { otherwise }
\end{array}\right.
$$

玩家 $i$ 根据遗憾值大小来选择下一时刻行为, 如果遗憾值为负数, 则随机挑选一种行为进行博弈

TODO: Kunh's pocker

[1]: https://rlcard.org/
[2]: https://zhuanlan.zhihu.com/p/25239682
[3]: https://arxiv.org/abs/1603.01121
[4]: https://rlcard.org/algorithms.html#nfsp
[5]: http://tigerneil.github.io/2016/05/30/nsfp/
[6]: https://www.youtube.com/watch?v=LcVamqY2GSQ
