

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-09-11 21:04:00
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-09-11 21:11:17
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请资助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 表示学习

为了提高机器学习系统的准确率，我们就需要将输入信息转换为有效的特征，或者更一般性地称为表示（Representation）．如果有一种算法可以自动地学习出有效的特征，并提高最终机器学习模型的性能，那么这种学习就可以叫作表示学习（Representation Learning）

语义鸿沟 表示学习的关键是解决**语义鸿沟（Semantic Gap）**问题．语义鸿沟问题是指输入数据的底层特征和高层语义信息之间的不一致性和差异性．比如给定一些关于“车”的图片，由于图片中每辆车的颜色和形状等属性都不尽相同，因此不同图片在像素级别上的表示（即底层特征）差异性也会非常大．但是我们理解这些图片是建立在比较抽象的高层语义概念上的．如果一个预测模型直接建立在底层特征之上，会导致对预测模型的能力要求过高．如果可以有一个好的表示在某种程度上能够反映出数据的高层语义特征，那么我们就能相对容易地构
建后续的机器学习模型．

在表示学习中，有两个核心问题：一是“什么是一个好的表示”；二是“如何学习到好的表示”

## 局部表示和分布式表示

“好的表示”是一个非常主观的概念，没有一个明确的标准．但一般而言，一个好的表示具有以下几个优点：

（1） 一个好的表示应该具有很强的表示能力，即同样大小的向量可以表示更多信息．
（2） 一个好的表示应该使后续的学习任务变得简单，即需要包含更高层的语义信息．
（3） 一个好的表示应该具有一般性，是任务或领域独立的．虽然目前的大部分表示学习方法还是基于某个任务来学习，但我们期望其学到的表示可以比较容易地迁移到其他任务上．

在机器学习中，我们经常使用两种方式来表示特征：局部表示（Local Rep-resentation）和分布式表示（Distributed  representation）．以颜色表示为例，我们可以用很多词来形容不同的颜色1，除了基本的“红”“蓝”“绿”“白”“黑”等之外，还有很多以地区或物品命名的，比如“中国红”“天蓝色”“咖啡色”“琥珀色”等．如果要在计算机中表示颜色，一般有两种表示方法．

一种表示颜色的方法是以不同名字来命名不同的颜色，这种表示方式叫作局部表示，也称为离散表示或符号表示．局部表示通常可以表示为one-hot向量的形式．

one-hot 向量参见

第A.1.4节．假设所有颜色的名字构成一个词表𝒱，词表大小为|𝒱|．我们可以用一个|𝒱|维的one-hot向量来表示每一种颜色．在第𝑖种颜色对应的one-hot向量中，第𝑖维的值为1，其他都为0．

局部表示有两个优点：

- 1）这种离散的表示方式具有很好的解释性，有利于人工归纳和总结特征，并通过特征组合进行高效的特征工程；
- 2）通过多种特征组合得到的表示向量通常是稀疏的二值向量，当用于线性模型时计算效率非常高．

但局部表示有两个不足之处：
1）one-hot向量的维数很高，且不能扩展．如果有一种新的颜色，我们就需要增加一维来表示；
2）不同颜色之间的相似度都为0，即我们无法知道“红色”和“中国红”的相似度要高于“红色”和“黑色”的相
似度．

另一种表示颜色的方法是用RGB值来表示颜色，不同颜色对应到R、G、B三维空间中一个点，这种表示方式叫作分布式表示． 将分布式表示叫作分散式表示可能更容易理解，即一种颜色的语义分散到语义空间中的不同基向量上．

分布式表示通常可以表示为低维的稠密向量．

和局部表示相比，分布式表示的表示能力要强很多，分布式表示的向量维度一般都比较低．我们只需要用一个三维的稠密向量就可以表示所有颜色．并且，分布式表示也很容易表示新的颜色名．此外，不同颜色之间的相似度也很容易计算．

表1.1列出了4种颜色的局部表示和分布式表示．

表1.1 局部表示和分布式表示示例

| 颜色 | 局部表示 | 分布式表示 |
| :--: | :----: | :----: |
| 琥珀色 | [1, 0, 0, 0]T | [1.00, 0.75, 0.00]T |
| 天蓝色 | [0, 1, 0, 0]T | [0.00, 0.5, 1.00]T  |
| 中国红 | [0, 0, 1, 0]T | [0.67, 0.22, 0.12]T |
| 咖啡色 | [0, 0, 0, 1]T | [0.44, 0.31 0.22]T  |

我们可以使用神经网络来将高维的局部表示空间ℝ|𝒱| 映射到一个非常低维的分布式表示空间ℝ𝐷,𝐷 ≪ |𝒱|．在这个低维空间中，每个特征不再是坐标轴上的点，而是分散在整个低维空间中．在机器学习中，这个过程也称为嵌入（Em-bedding）．嵌入通常指将一个度量空间中的一些对象映射到另一个低维的度量空间中，并尽可能保持不同对象之间的拓扑关系．比如自然语言中词的分布式表示，也经常叫作词嵌入．
图1.3展示了一个3维one-hot向量空间和一个2维嵌入空间的对比．在one-hot向量空间中，每个特征都位于坐标轴上，每个坐标轴上一个特征．而在低维的嵌入空间中，每个特征都不在坐标轴上，特征之间可以计算相似度．


## 表示学习

要学习到一种好的高层语义表示（一般为分布式表示），通常需要从底层特征开始，经过多步非线性转换才能得到。（连续多次的线性转换等价于一次线性转换。）

深层结构的优点是可以增加特征的重用性，从而指数级地增加表示能力．因此，表示学习的关键是构建具有一定深度的多层次特征表示[Bengio et al., 2013]．

在传统的机器学习中，也有很多有关特征学习的方法，比如主成分分析、线性判别分析、独立成分分析等．

但是，传统的特征学习一般是通过人为地设计一些准则，然后根据这些准则来选取有效的特征．

特征的学习是和最终预测模型的学习分开进行的，因此学习到的特征不一定可以提升最终模型的性能．

[1]:https://nndl.github.io/
