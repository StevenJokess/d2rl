

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-10-03 03:07:52
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-10-25 22:41:46
 * @Description:
 * @Help me: make friends by a867907127@gmail.com and help me get some “foreign” things or service I need in life; 如有帮助，请资助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# Richard S. Sutton

Richard S. Sutton(萨顿) 是加拿大计算机科学家。阿尔伯塔大学计算机科学教授和iCORE主席。

萨顿被认为是现代计算的奠基人之一的强化学习，贡献了包括时间差分学习，策略梯度法，Dyna架构等技术。

## 简历

- 他于1978年获得斯坦福大学心理学学士学位，硕士学位。
- 于1984年在马萨诸塞大学阿默斯特分校获得计算机科学博士学位，并在Andrew Barto的监督下。他的博士论文题为“强化学习中的时间信用分配”，他在其中介绍了演员-评论家的架构和“时间信用分配”等问题。
- 1984年，萨顿在阿默斯特的马萨诸塞大学担任博士后职务。
- 从1985年到1994年，他是GTE实验室计算机和智能系统实验室的技术人员的主要成员。
- 1995年，他回到马萨诸塞州阿默斯特大学担任高级研究科学家，
- 直到1998年，他加入AT＆T Shannon实验室，担任人工智能部门的首席技术人员。
- 1998年，Richard S.Sutton 与Andrew Barto合著出版了强化学习导论Reinforcement Learning：An Introduction，系统总结了1998年以前强化学习算法的各种进展，强化学习基本理论框架已经形成。[4]
  - Andrew Barto 是Massachusetts大学Amherst分校的教授, 已于2012年退休.退休前, 他是Massachusetts大学Amherst分校自治学习实验室主任.目前, 他是assachusetts大学神经科学和行为项目的准会员, Neural Computation 副主编, Machine Learning Research杂志顾问,  Adaptive Behavior的编辑[1]
- 自2003年以来，他一直担任阿尔伯塔大学计算科学系的教授和iCORE主席，领导强化学习和人工智能实验室（RLAI）。
- 2017年6月，Demis Hassabis宣布Sutton将共同领导一个新的艾伯塔省DeepMind(后文详解)办公室，同时保持他在阿尔伯塔大学的教授职位。
- Sutton自2001年以来一直是人工智能促进协会AAAI的成员。
- 2003年，他获得了国际神经网络学会颁发的总统奖，
- 于2013年获得了马萨诸塞大学阿默斯特分校颁发的杰出成就奖，著有强化学习界的圣经书籍《Reinforcement Learning: An Introduction》。[5]


## 文章：苦涩的教训

70 年的人工智能研究史告诉我们，利用计算能力的一般方法最终是最有效的方法。这个归摩尔定律解释，或者它对每单位计算成本持续指数级下降的概括。大部分 AI 研究都是在认为智能体可用的计算为恒定的情况下进行的（在这种情况下，利用人类知识是提高性能的唯一方法），但是，在比典型研究项目稍长的时间尺度内，我们不可避免地会需要大量的计算。

要在短期内有所提升，研究人员要利用专门领域的人类知识。但如果想要长期的获得提升，利用计算能力才是王道。这两者本无需对立，但实际上它们往往如此。花时间研究一个，就会忽略另一个。利用人类知识的方法容易复杂化，导致其不太适合利用计算的方法。很多例子表明 AI 研究人员对这些教训的认识太晚，因此我们有必要回顾一些突出的例子。

在计算机国际象棋中，1997 年击败世界冠军卡斯帕罗夫的方法基于大量深度搜索。当时，大多数 AI 计算机象棋研究人员沮丧地发现了这一点，他们的方法是利用人类对象棋特殊结构的理解。当这个利用硬件和软件的基于搜索的更简单方法被证明更有效时，这些基于人类知识的象棋研究人员却仍不肯认输。他们认为虽然这个「暴力」搜索方法此次赢了，但它并不是一个普遍的策略，无论如何它不是人类下国际象棋的方法。这些研究人员希望基于人类输入的方法获胜，但结果却令他们失望了。

计算机围棋中也有类似的研究进展模式，只是晚了 20 年。最初研究人员努力利用人类知识或游戏的特殊性来避免搜索，但所有的努力都被证明没什么用，因为搜索被大规模地有效应用。同样重要的是利用自我对弈（self play）来学习一种价值函数（就像在很多其他游戏甚至国际象棋中一样，虽然在 1997 年首次击败世界冠军的比赛中没起到什么作用）。通过自我对弈学习和一般学习有点像搜索，因为它能让大量的计算发挥作用。搜索和学习是人工智能研究中利用大量计算的两种最重要技术。在计算机围棋中，就像计算机国际象棋中一样，研究人员最初是想通过人类理解（这样无需太多搜索）来实现目的，只是在后来，通过搜索和学习才取得了巨大成功。

在语音识别领域，早在上世纪 70 年代就有一个由 DARPA 赞助的竞赛。参赛者利用了很多利用人类知识的特殊方法：单词、因素和人类声道等。另一方面，还有人利用了基于隐马尔可夫模型的新方法，这些方法在本质上更具统计性，计算量也更大。同样，统计方法战胜了基于人类知识的方法。这导致了自然语言处理领域的重大改变，过去几十年来，统计和计算在该领域逐渐占据主导地位。深度学习最近在语音识别中的兴起正是朝着这一方向迈出的最新一步。

深度学习方法更少依赖人类知识，使用更多的计算，并且伴有大量训练集的学习，从而生成更好的语音识别系统。就像在游戏中一样，研究人员总是试图令系统按照他们的思维方式进行运作——他们试图将知识放在系统中——但事实证明，最终结果往往事与愿违，并且极大浪费了研究人员的时间。但是通过摩尔定律，研究人员可以进行大量计算，并且找到一种有效利用的方法。

计算机视觉领域存在相似的模式。早期方法认为视觉是为了搜索边缘、广义圆柱体或者取决于 SIFT 特征。但是今天，所有这些方法都被抛弃了。现代深度学习神经网络仅使用卷积和某些不变性的概念即可以取得更好的效果。

这是一个非常大的教训。因为我们还在犯同一类错误，所以依然未能彻底了解人工智能领域。要看到这一点并且有效地避免重蹈覆辙，我们必须理解这些错误为何会让我们误入歧途。我们必须吸取惨痛的教训，即从长远看，固守我们的思维模式是行不通的。痛苦的教训基于以下历史观察结果：

AI 研究人员常常试图在自身智能体中构建知识，

从短期看，这通常是有帮助的，能够令研究人员满意，

但从长远看，这会令研究人员停滞不前，甚至抑制进一步发展，

突破性进展最终可能会通过一种相反的方法——基于以大规模计算为基础的搜索和学习。最后的成功往往带有一丝苦涩，并且无法完全消化，因为这种成功不是通过一种令人喜欢、以人为中心的方法获得的。

我们应该从痛苦的教训中学到的一点：通用方法非常强大，这类方法会随着算力的增加而继续扩展，即使可用计算变得非常大。搜索和学习似乎正是两种以这种方式随意扩展的方法。

我们从痛苦的教训中学到的第二个普遍观点是，意识的实际内容是极其复杂的；我们不应该试图通过简单方法来思考意识的内容，如思考空间、物体、多智能体或者对称性。所有这些都是任意的、本质上复杂的外部世界的一部分。它们不应该被固有化，其原因是复杂性是无穷无尽的；相反，我们只应该构建可以找到并捕获这种任意复杂性的元方法。这些方法的关键在于它们能够找到很好的近似值，但对它们的搜索应由我们的方法完成，而不是我们自己。我们希望 AI 智能体可以像我们一样发现新事物，而不是重新找到我们所发现的。在我们发现的基础上构建只能令人更加难以看清发现过程的完成情况。[2]



原文链接：[3]


[1]: https://cloud.tencent.com/developer/article/1616659
[2]: https://www.linkresearcher.com/information/01af2812-2cbc-4c3b-bec8-2e3701c6ac2f
[3]: http://www.incompleteideas.net/IncIdeas/BitterLesson.html
[4]: https://blog.csdn.net/lxs3213196/article/details/109846324
[5]: https://zhuanlan.zhihu.com/p/56522295
[6]: https://www.ualberta.ca/science/about-us/contact-us/faculty-directory/rich-sutton
