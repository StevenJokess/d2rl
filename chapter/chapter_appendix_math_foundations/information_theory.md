

<!--
 * @version:
 * @Author:  StevenJokessï¼ˆè”¡èˆ’èµ·ï¼‰ https://github.com/StevenJokess
 * @Date: 2023-03-13 23:23:58
 * @LastEditors:  StevenJokessï¼ˆè”¡èˆ’èµ·ï¼‰ https://github.com/StevenJokess
 * @LastEditTime: 2023-06-04 22:28:13
 * @Description:
 * @Help me: å¦‚æœ‰å¸®åŠ©ï¼Œè¯·èµåŠ©ï¼Œå¤±ä¸š3å¹´äº†ã€‚![æ”¯ä»˜å®æ”¶æ¬¾ç ](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# ä¿¡æ¯è®º

ä¿¡æ¯è®ºæ˜¯åº”ç”¨æ•°å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸»è¦ç ”ç©¶çš„æ˜¯å¯¹ä¸€ä¸ªä¿¡å·åŒ…å«ä¿¡æ¯çš„å¤šå°‘è¿›è¡Œé‡åŒ–ã€‚ å®ƒæœ€åˆè¢«å‘æ˜æ˜¯ç”¨æ¥ç ”ç©¶åœ¨ä¸€ä¸ªå«æœ‰å™ªå£°çš„ä¿¡é“ä¸Šç”¨ç¦»æ•£çš„å­—æ¯è¡¨æ¥å‘é€æ¶ˆæ¯ï¼Œä¾‹å¦‚é€šè¿‡æ— çº¿ç”µä¼ è¾“æ¥é€šä¿¡ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¿¡æ¯è®ºå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•å¯¹æ¶ˆæ¯è®¾è®¡æœ€ä¼˜ç¼–ç ä»¥åŠè®¡ç®—æ¶ˆæ¯çš„æœŸæœ›é•¿åº¦ï¼Œè¿™äº›æ¶ˆæ¯æ˜¯ä½¿ç”¨å¤šç§ä¸åŒç¼–ç æœºåˆ¶ã€ä»ç‰¹å®šçš„æ¦‚ç‡åˆ†å¸ƒä¸Šé‡‡æ ·å¾—åˆ°çš„ã€‚ åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŠŠä¿¡æ¯è®ºåº”ç”¨äºè¿ç»­å‹å˜é‡ï¼Œæ­¤æ—¶æŸäº›æ¶ˆæ¯é•¿åº¦çš„è§£é‡Šä¸å†é€‚ç”¨ã€‚ ä¿¡æ¯è®ºæ˜¯ç”µå­å·¥ç¨‹å’Œè®¡ç®—æœºç§‘å­¦ä¸­è®¸å¤šé¢†åŸŸçš„åŸºç¡€ã€‚[4]

å¼•å…¥ç›¸å…³åŒ…ï¼š

```py
import numpy as np
import scipy.stats
```

## ä¿¡æ¯

ä¿¡æ¯è®ºå¥ åŸºäººé¦™å†œ(Shannon)è®¤ä¸ºï¼Œä¿¡æ¯å°±æ˜¯ç”¨æ¥æ¶ˆé™¤ä¸ç¡®å®šæ€§çš„ä¸œè¥¿ã€‚

æ•…åº”æœ‰å¦‚ä¸‹ç‰¹æ€§ï¼š

- å¾ˆæœ‰å¯èƒ½å‘ç”Ÿçš„äº‹ä»¶å‡ ä¹æ²¡æœ‰ä¿¡æ¯ï¼›ç”šè‡³ä¸€å®šå‘ç”Ÿçš„äº‹ä»¶æ²¡æœ‰ä¿¡æ¯ã€‚ï¼ˆå³ï¼Œå½“æ¦‚ç‡xç­‰äº1æ—¶ï¼Œä¿¡æ¯é‡ä¸º0ï¼‰
- éšæœºäº‹ä»¶æ‹¥æœ‰æ›´å¤šçš„ä¿¡æ¯ï¼ˆç”±ä¸Šé¢å¾—ï¼Œf(x)åº”å½“æ˜¯é€’å‡å‡½æ•°ï¼‰ï¼›ç”šè‡³ä¸€å®šä¸å‘ç”Ÿçš„äº‹ä»¶å……æ»¡ä¿¡æ¯ã€‚ï¼ˆå³ï¼Œå½“äº‹ä»¶æ¦‚ç‡xç­‰äº0æ—¶ï¼Œä¿¡æ¯é‡ä¸ºæ­£æ— ç©·ï¼‰
- ç‹¬ç«‹äº‹ä»¶å¯ä»¥å¢åŠ ä¿¡æ¯â€”â€”æŠ›ä¸¤æ¬¡æ­£é¢çš„éª°å­çš„ä¿¡æ¯é‡å¤§äºæŠ›ä¸€æ¬¡æ­£é¢éª°å­çš„ä¿¡æ¯é‡ã€‚[3]
- ä¿¡æ¯é‡æ˜¯å¯åŠ çš„ï¼Œä¸¤ä¸ªç‹¬ç«‹äº‹ä»¶åŒæ—¶å‘ç”Ÿçš„ä¿¡æ¯é‡åº”å½“ç­‰äºå®ƒä»¬åˆ†åˆ«å‘ç”Ÿçš„ä¿¡æ¯é‡ä¹‹å’Œã€‚ å³$f(m*n)=f(m)+f(n)$

## è‡ªä¿¡æ¯/ä¿¡æ¯é‡

ä¸ºäº†æ»¡è¶³ä¸Šè¿°ä¸‰ä¸ªæ€§è´¨ï¼Œå®šä¹‰ä¸€ä¸ªäº‹ä»¶çš„è‡ªä¿¡æ¯ï¼ˆæˆ– ä¿¡æ¯é‡ï¼Œä¿¡æ¯çš„åº¦é‡ï¼‰ä¸ºï¼š

$I(x)=-\log _a(P(x))$

å…¶ä¸­ï¼Œa>1ï¼Œ0<x<=1

- å½“a=eæ—¶ï¼ŒI(x)å•ä½æ˜¯å¥ˆç‰¹(nat)ï¼›
- å½“a=2æ—¶ï¼ŒI(x)å•ä½æ˜¯æ¯”ç‰¹(bit)æˆ–è€…é¦™å†œï¼›
- å½“a=10æ—¶ï¼ŒI(x)å•ä½æ˜¯å“ˆç‰¹(hart)ã€‚

æ¢ç®—å…³ç³»ï¼š

- $1å¥ˆç‰¹=\log _2 eæ¯”ç‰¹â‰ˆ1.443æ¯”ç‰¹$
- $1å“ˆç‰¹=\log _2 10æ¯”ç‰¹â‰ˆ3.322æ¯”ç‰¹$

TODOï¼šÂ·```py

self_info = -np.log2(p)
print(self_info)
```


## ç†µ(Entropy)

### ç†µï¼š

ç†µç”¨äºè¡¡é‡ä¿¡æ¯çš„å¤šå°‘ï¼Œè¢«å®šä¹‰ä¸ºï¼š

$$
H(x)=\mathbb{E}_{x\sim P}(I(x))=- \mathbb{E}_{x\sim p}[\log P(x)]
$$

### ä¿¡æ¯ç†µï¼š

è€Œåœ¨ä¿¡æ¯è®ºä¸­ï¼Œä¿¡æ¯ä¸éšæœºæ€§æ˜¯æ­£ç›¸å…³çš„ã€‚é«˜ç†µç­‰äºé«˜éšæœºæ€§ï¼Œéœ€è¦æ›´å¤šçš„æ¯”ç‰¹æ¥ç¼–ç ã€‚

ä¸ºäº†è¡¨ç¤ºç³»ç»Ÿï¼ˆä¿¡æºï¼‰çš„ä¿¡æ¯æµ‹åº¦ï¼ˆå¹³å‡ä¿¡æ¯é‡ï¼Œå³ä¿¡æ¯ç†µçš„å®šä¹‰ï¼‰ï¼Œåˆ™æŠŠæ¯ç§ç»“æœçš„ä¿¡æ¯é‡æŒ‰å‘ç”Ÿçš„æ¦‚ç‡åŠ æƒå¹³å‡ã€‚

å…·ä½“å®šä¹‰ï¼šå‡å®šå½“å‰æ ·æœ¬é›†åˆXä¸­ç¬¬*i*ç±»æ ·æœ¬ $ğ‘¥_ğ‘–$ æ‰€å çš„æ¯”ä¾‹ä¸º$P(ğ‘¥_ğ‘–)(i=1,2,...,n)$ï¼Œåˆ™*X*çš„ä¿¡æ¯ç†µå®šä¹‰ä¸ºï¼Œ

$$
H(X) = -\sum_{i = 1}^n P(x_i)\log_2P(x_i)
$$

H(X)ä»£è¡¨æœ€ä¼˜ç¼–ç é•¿åº¦ï¼Œå…¶å€¼è¶Šå°ï¼Œåˆ™Xçš„çº¯åº¦è¶Šé«˜ï¼Œè•´å«çš„ä¸ç¡®å®šæ€§è¶Šå°‘ã€‚

ä¾‹å¦‚ï¼Œè®¡ç®—ä¸¢ä¸€æšç¡¬å¸çš„ç†µï¼š[3]

$$
H(x)=-p(æ­£é¢)\log_2 p(æ­£é¢)-p(åé¢)\log_2 p(åé¢)=-\log_2\frac{1}{2}=1 æ¯”ç‰¹
$$

## è”åˆç†µ

ä¸¤ä¸ªéšæœºå˜é‡Xå’ŒYçš„è”åˆåˆ†å¸ƒå¯ä»¥å½¢æˆè”åˆç†µï¼Œåº¦é‡äºŒç»´éšæœºå˜é‡XYï¼ˆå³éšæœºå˜é‡Xï¼ŒYåŒæ—¶å‘ç”Ÿï¼‰çš„ä¸ç¡®å®šæ€§ï¼š

$$
H(X, Y) = -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j)\log_2 P(x_i,y_j)
$$

## æ¡ä»¶ç†µ

å®šä¹‰ï¼šåœ¨ä¸€ä¸ªæ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡çš„ä¸ç¡®å®šæ€§ã€‚

åœ¨éšæœºå˜é‡Xå‘ç”Ÿçš„å‰æä¸‹ï¼Œéšæœºå˜é‡Yå‘ç”Ÿå¸¦æ¥çš„ç†µï¼Œå®šä¹‰ä¸ºYçš„æ¡ä»¶ç†µï¼Œç”¨H(Y|X)è¡¨ç¤ºï¼Œå®šä¹‰ä¸ºï¼š

$$
CH(Y|X) = \sum_{i = 1}^n P(x_i)H(Y|X = x_i)
 = -\sum_{i = 1}^n P(x_i) \sum_{j = 1}^n P(y_j|x_i)\log_2
P(y_j|x_i)
= -\sum_{i = 1}^n \sum_{j = 1}^n P(x_i,y_j) \log_2
P(y_j|x_i)
$$

æ¡ä»¶ç†µç”¨æ¥è¡¡é‡åœ¨å·²çŸ¥éšæœºå˜é‡Xçš„æ¡ä»¶ä¸‹ï¼Œéšæœºå˜é‡Yçš„ä¸ç¡®å®šæ€§ã€‚

ç†µã€è”åˆç†µå’Œæ¡ä»¶ç†µä¹‹é—´çš„å…³ç³»ï¼š$H(Y|X) = H(X,Y)-H(X)$.

## äº¤å‰ç†µï¼ˆCross Entropyï¼‰

äº¤å‰ç†µï¼ˆCross Entropyï¼‰æ˜¯æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒqçš„æœ€ä¼˜ç¼–ç å¯¹çœŸå®åˆ†å¸ƒä¸ºpçš„ä¿¡æ¯è¿›è¡Œç¼–ç çš„é•¿åº¦ï¼Œå®šä¹‰ä¸ºï¼š

$$
\begin{aligned} H(P, Q) &=\mathbb{E}{P}[-\log _2 Q(x)] \ &=-\sum{x} P(x) \log _2 Q(x) \end{aligned}
$$

åœ¨ç»™å®špçš„æƒ…å†µä¸‹ï¼Œå¦‚æœqå’Œpè¶Šæ¥è¿‘ï¼Œäº¤å‰ç†µè¶Šå°ï¼›å¦‚æœqå’Œpè¶Šè¿œï¼Œäº¤å‰ç†µå°±è¶Šå¤§ã€‚

æ‰€ä»¥ä¸€èˆ¬ç”¨æ¥æ±‚ç›®æ ‡ä¸é¢„æµ‹å€¼ä¹‹é—´çš„å·®è·ï¼Œæ·±åº¦å­¦ä¹ ä¸­ç»å¸¸ç”¨åˆ°çš„ä¸€ç±»æŸå¤±å‡½æ•°åº¦é‡ï¼Œæ¯”å¦‚åœ¨å¯¹æŠ—ç”Ÿæˆç½‘ç»œ(GAN)ä¸­

$$
D(p||q) = \sum P(x)\log \frac{p(x)}{q(x)}
= \sum p(x)\log p(x) -  \sum p(x)\log _2 q(x)
=-H(p(x)) -\sum p(x)\log _2 q(x)
$$

```py

def cross_entropy(p, q):
    p = np.float_(p)
    q = np.float_(q)
    return -sum([p[i]*np.log2(q[i]) for i in range(len(p))])

p = np.asarray([0.65, 0.25, 0.07, 0.03])
q = np.array([0.6, 0.25, 0.1, 0.05])
print(cross_entropy(p, q))  # 1.3412204456967705
print(cross_entropy(q, p))  # 1.5094878372721525
```

## äº’ä¿¡æ¯(mutual information)


$$
I(X;Y) = H(X)+H(Y)-H(X,Y)
$$



<img src="../../img/mutual_information.png" alt="2.5" style="zoom:50%;" />

---

## ç›¸å¯¹ç†µ(Relative Entropy) æˆ– KLæ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰

â€‹ç›¸å¯¹ç†µ(Relative Entropy)ï¼Œä¹Ÿå« KL æ•£åº¦ï¼ˆKullback-Leibler Divergenceï¼‰æˆ–KL è·ç¦»ï¼Œæ˜¯ç”¨æ¦‚ç‡åˆ†å¸ƒQæ¥è¿‘ä¼¼Pæ—¶æ‰€é€ æˆçš„ä¿¡æ¯æŸå¤±é‡ï¼Œæ‰€ä»¥å¯ä»¥ç”¨æ¥æè¿°ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒPå’ŒQå·®å¼‚ã€‚è®°åš $D_{\text{KL}}(P||Q)$ æˆ– $\mathrm{KL}(p, q)$ ã€$\mathrm{KL}(p||q)$ã€‚

åœ¨ä¿¡æ¯è®ºä¸­ï¼Œ$D_{\text{KL}}(P||Q)$è¡¨ç¤ºç”¨æ¦‚ç‡åˆ†å¸ƒQæ¥æ‹ŸåˆçœŸå®åˆ†å¸ƒPæ—¶ï¼Œäº§ç”Ÿçš„ä¿¡æ¯è¡¨è¾¾çš„æŸè€—ï¼Œå…¶ä¸­Pè¡¨ç¤ºä¿¡æºçš„çœŸå®åˆ†å¸ƒï¼ŒQè¡¨ç¤ºPçš„è¿‘ä¼¼åˆ†å¸ƒã€‚

KLæ•£åº¦æ˜¯æŒ‰ç…§æ¦‚ç‡åˆ†å¸ƒQçš„æœ€ä¼˜ç¼–ç å¯¹çœŸå®åˆ†å¸ƒä¸ºPçš„ä¿¡æ¯è¿›è¡Œç¼–ç ï¼Œå…¶å¹³å‡ç¼–ç é•¿åº¦ï¼ˆå³äº¤å‰ç†µï¼‰H(p, q)å’Œpçš„æœ€ä¼˜å¹³å‡ç¼–ç é•¿åº¦ï¼ˆå³ç†µï¼‰H(p)ä¹‹é—´çš„å·®å¼‚ã€‚

1. å¯¹äºç¦»æ•£æ¦‚ç‡åˆ†å¸ƒp å’Œqï¼Œä»qåˆ°pçš„KLæ•£åº¦å®šä¹‰ä¸ºï¼š

$$
\mathrm{KL}(p, q)=H(p, q)-H(p) \quad=\sum_x p(x) \log \frac{p(x)}{q(x)}\\ =\sum_i P(i) \log P(i)-\sum_i P(i) \log Q(i) =H(p,q)-H(p)  [ç›¸å¯¹ç†µ=äº¤å‰ç†µ-ç†µ]
$$

2. å¯¹äºè¿ç»­æ¦‚ç‡åˆ†å¸ƒp å’Œqï¼Œä»qåˆ°pçš„KLæ•£åº¦å®šä¹‰ä¸ºï¼š

$$
\mathrm{KL}(p, q)=H(p, q)-H(p) \quad=\int P(x)\log \frac{p(x)}{q(x)}dx
$$

å…¶ä¸­ï¼Œä¸ºäº†ä¿è¯è¿ç»­æ€§ï¼Œå®šä¹‰ï¼š

$$
0 \log \frac{0}{0}=0,0 \log \frac{0}{q}=0
$$

KL æ•£åº¦çš„ä¸€ä¸ªé‡è¦æ€§è´¨æ˜¯éè´Ÿæ€§ã€‚å½“ä¸¤ä¸ªåˆ†å¸ƒå®Œå…¨ç›¸åŒï¼Œå¯¹äºä»»æ„xï¼Œæœ‰p(x)ï¼q(x)ï¼Œæ­¤æ—¶log(p(x)/q(x))ä¸º0ï¼ŒKLæ•£åº¦ä¸º0ã€‚å½“ä¸¤ä¸ªåˆ†å¸ƒä¸å®Œå…¨ç›¸åŒï¼Œæ ¹æ®å‰å¸ƒæ–¯ä¸ç­‰å¼ï¼ˆGibbs' Inequalityï¼‰å¯è¯æ˜KLæ•£åº¦ä¸ºæ­£æ•°ã€‚[6]

$KL(p,q)â‰¥0$ ï¼Œå¯ä»¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚

> ä»è´å¶æ–¯æ¨è®ºï¼ˆBayesian inferenceï¼‰è§’åº¦ï¼ŒKLæ•£åº¦è¡¡é‡äº†å½“å°†æ¡ä»¶ä»å…ˆéªŒæ¦‚ç‡åˆ†å¸ƒQä¸Šåˆ‡æ¢åˆ°åéªŒæ¦‚ç‡åˆ†å¸ƒPä¸Šæ‰€è·å¾—ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯å½“ç”¨Qæ¥è¿‘ä¼¼Pæ—¶çš„ä¿¡æ¯æŸå¤±ã€‚åœ¨å®è·µä¸­ï¼ŒPä¸€èˆ¬ä»£è¡¨äº†æ•°æ®çš„çœŸå®åˆ†å¸ƒï¼ŒQä»£è¡¨äº†æ¨¡å‹ï¼Œç”¨äºè¿‘ä¼¼Pã€‚ä¸ºäº†è·å¾—æœ€æ¥è¿‘Pçš„Qï¼Œå¯ä»¥æœ€å°åŒ–KLæ•£åº¦ã€‚[5]

- KLæ•£åº¦åªæœ‰å½“p = qæ—¶ï¼Œ$KL(p,q)=0$ã€‚
- å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šæ¥è¿‘ï¼ŒKLæ•£åº¦è¶Šå°ï¼›
- å¦‚æœä¸¤ä¸ªåˆ†å¸ƒè¶Šè¿œï¼ŒKLæ•£åº¦å°±è¶Šå¤§ã€‚

ä½†KLæ•£åº¦å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸæ­£çš„åº¦é‡æˆ–è·ç¦»ï¼ŒåŸå› æ˜¯ï¼š

- KLæ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„å¯¹ç§°æ€§ï¼Œå³$D_{K L}(P \| Q) \neq D_{K L}(Q \| P)$
- KLæ•£åº¦ä¸æ»¡è¶³è·ç¦»çš„ä¸‰è§’ä¸ç­‰å¼æ€§è´¨

### æ­£å‘KLæ•£åº¦$KL(p||q)$

$$ \hat{q}=\operatorname{argmin}_{q} \int{x} p(x) \log \frac{p(x)}{q(x)} d x $$

ä»”ç»†è§‚å¯Ÿ(3)å¼ï¼Œ$p(x)$ æ˜¯å·²çŸ¥çš„çœŸå®åˆ†å¸ƒï¼Œè¦æ±‚ä½¿ä¸Šå¼æœ€å°çš„ $q(x)$ã€‚

è€ƒè™‘å½“ $p(x)=0$ æ—¶ï¼Œè¿™æ—¶q(x)å–ä»»ä½•å€¼éƒ½å¯ä»¥ï¼Œå› ä¸º $logp(x)q(x)$ è¿™ä¸€é¡¹å¯¹æ•´ä½“çš„KLæ•£åº¦æ²¡æœ‰å½±å“ã€‚å½“  $p(x)>0$ æ—¶ï¼Œè¿™ä¸€é¡¹å¯¹ $logp(x)q(x)$ æ•´ä½“çš„KLæ•£åº¦å°±ä¼šäº§ç”Ÿå½±å“ï¼Œä¸ºäº†ä½¿(3)å¼æœ€å°ï¼Œ$q(x)$ åˆå¤„äº $logp(x)q(x)$ ä¸­åˆ†æ¯çš„ä½ç½®ï¼Œæ‰€ä»¥ $q(x)$ å°½é‡å¤§ä¸€äº›æ‰å¥½ã€‚

æ€»ä½“è€Œè¨€ï¼Œå¯¹äºæ­£å‘ KL æ•£åº¦ï¼Œåœ¨ $p(x)$ å¤§çš„åœ°æ–¹ï¼Œæƒ³è®© KL æ•£åº¦å°ï¼Œå°±éœ€è¦ $q(x)$ çš„å€¼ä¹Ÿå°½é‡å¤§ï¼›åœ¨p(x)å°çš„åœ°æ–¹ï¼Œq(x)å¯¹æ•´ä½“ KL å½±å“å¹¶ä¸å¤§ï¼ˆå› ä¸º log é¡¹æœ¬èº«åˆ†å­å¾ˆå°ï¼Œåˆä¹˜äº†ä¸€ä¸ªéå¸¸å°çš„p(x)ã€‚æ¢ä¸€ç§è¯´æ³•ï¼Œè¦æƒ³ä½¿æ­£å‘ KL æ•£åº¦æœ€å°ï¼Œåˆ™è¦æ±‚åœ¨ p ä¸ä¸º 0 çš„åœ°æ–¹ï¼Œq ä¹Ÿå°½é‡ä¸ä¸º 0ï¼Œæ‰€ä»¥æ­£å‘ KL æ•£åº¦è¢«ç§°ä¸ºæ˜¯ zero avoidingã€‚æ­¤æ—¶å¾—åˆ°çš„åˆ†å¸ƒ q æ˜¯ä¸€ä¸ªæ¯”è¾ƒ â€œå®½â€ çš„åˆ†å¸ƒã€‚

### åå‘KLæ•£åº¦ $KL(q||p)$

$$ \hat{q}=\operatorname{argmin}_{q} \int{x} q(x) \log \frac{q(x)}{p(x)} d x $$

ä»”ç»†è§‚å¯Ÿ(4)å¼ï¼Œ$p(x)$ æ˜¯å·²çŸ¥çš„çœŸå®åˆ†å¸ƒï¼Œè¦æ±‚ä½¿ä¸Šå¼æœ€å°çš„q(x).è€ƒè™‘å½“ p(x)=0 æ—¶ï¼Œè¿™æ—¶ä¸ºäº†ä½¿(4)å¼å˜å°ï¼Œq(x)å–0å€¼æ‰å¯ä»¥ï¼Œå¦åˆ™(4)å¼å°±ä¼šå˜æˆæ— ç©·å¤§ã€‚å½“p(x)>0æ—¶ï¼Œä¸ºäº†ä½¿(4)å¼å˜å°ï¼Œå¿…é¡»åœ¨p(x)å°çš„åœ°æ–¹ï¼Œq(x)ä¹Ÿå°ã€‚åœ¨p(x)å¤§çš„åœ°æ–¹å¯ä»¥é€‚å½“å¿½ç•¥ã€‚æ¢ä¸€ç§è¯´æ³•ï¼Œè¦æƒ³ä½¿åå‘ KL æ•£åº¦æœ€å°ï¼Œåˆ™è¦æ±‚åœ¨ p ä¸º 0 çš„åœ°æ–¹ï¼Œq ä¹Ÿå°½é‡ä¸º 0ï¼Œæ‰€ä»¥åå‘ KL æ•£åº¦è¢«ç§°ä¸ºæ˜¯ zero forcingã€‚æ­¤æ—¶å¾—åˆ°åˆ†å¸ƒ q æ˜¯ä¸€ä¸ªæ¯”è¾ƒ â€œçª„â€ çš„åˆ†å¸ƒã€‚

ä¸€ä¸ªä¾‹å­ï¼šå‡å¦‚p(x)æ˜¯ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒçš„æ··åˆï¼Œq(x)æ˜¯å•ä¸ªé«˜æ–¯ï¼Œç”¨q(x)å»è¿‘ä¼¼p(x)ï¼Œä¸¤ç§KLæ•£åº¦è¯¥å¦‚ä½•é€‰æ‹©ï¼Ÿ

å¯¹äºæ­£å‘KLæ•£åº¦æ¥è¯´ï¼Œq(x) çš„åˆ†å¸ƒå›¾åƒæ›´ç¬¦åˆç¬¬äºŒè¡Œï¼Œæ­£å‘KLæ•£åº¦æ›´åœ¨æ„ä¸­p(x) çš„å¸¸è§äº‹ä»¶ï¼Œä¹Ÿå°±æ˜¯é¦–å…ˆè¦ä¿è¯p(x)å³°å€¼é™„è¿‘çš„xï¼Œåœ¨q(x)ä¸­çš„æ¦‚ç‡å¯†åº¦å€¼ä¸èƒ½ä¸º0ã€‚å½“ p å…·æœ‰å¤šä¸ªå³°æ—¶ï¼Œq é€‰æ‹©å°†è¿™äº›å³°æ¨¡ç³Šåˆ°ä¸€èµ·ï¼Œä»¥ä¾¿å°†é«˜æ¦‚ç‡è´¨é‡æ”¾åˆ°æ‰€æœ‰å³°ä¸Šã€‚

å¯¹äºåå‘KLæ•£åº¦æ¥è¯´ï¼Œq(x)çš„åˆ†å¸ƒå›¾åƒæ›´ç¬¦åˆç¬¬ä¸€è¡Œã€‚åå‘KLæ•£åº¦æ›´åœ¨æ„ä¸­p(x)çš„ç½•è§äº‹ä»¶ï¼Œä¹Ÿå°±æ˜¯é¦–å…ˆè¦ä¿è¯p(x)ä½è°·é™„ä»¶çš„xï¼Œåœ¨q(x)ä¸­çš„æ¦‚ç‡å¯†åº¦å€¼ä¹Ÿè¾ƒå°ã€‚å½“ $p$ å…·æœ‰å¤šä¸ªå³°å¹¶ä¸”è¿™äº›å³°é—´éš”å¾ˆå®½æ—¶ï¼Œå¦‚è¯¥å›¾æ‰€ç¤ºï¼Œæœ€å°åŒ– KL æ•£åº¦ä¼šé€‰æ‹©å•ä¸ªå³°ï¼Œä»¥é¿å…å°†æ¦‚ç‡å¯†åº¦æ”¾ç½®åœ¨ $p$ çš„å¤šä¸ªå³°ä¹‹é—´çš„ä½æ¦‚ç‡åŒºåŸŸä¸­ã€‚

```py

def KL_divergence(p,q):
    return scipy.stats.entropy(p, q)

p=np.asarray([0.65,0.25,0.07,0.03])
q=np.array([0.6,0.25,0.1,0.05])
print(KL_divergence(p, q)) # 0.011735745199107783
print(KL_divergence(q, p)) # 0.013183150978050884
print(KL_divergence(p, p)) # 0
```

## JSæ•£åº¦

JS æ•£åº¦ï¼ˆJensen-Shannon Divergenceï¼‰æ˜¯ä¸€ç§å¯¹ç§°çš„è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒç›¸ä¼¼åº¦çš„åº¦é‡æ–¹å¼ï¼Œå®šä¹‰ä¸ºï¼š

$$
\mathrm{JS}(p, q)=\frac{1}{2} \mathrm{KL}(p, m)+\frac{1}{2} \mathrm{KL}(q, m)
$$

å…¶ä¸­ $m=\frac{1}{2}(p+q)$ ã€‚

JS æ•£åº¦æ˜¯ KL æ•£åº¦ä¸€ç§æ”¹è¿›ã€‚ä½†ä¸¤ç§æ•£åº¦éƒ½å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œå³å¦‚æœä¸¤ä¸ªåˆ†å¸ƒp, qæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘æ—¶ï¼ŒKLæ•£åº¦å’ŒJSæ•£åº¦éƒ½å¾ˆéš¾è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„è·ç¦»ã€‚

```py
import numpy as np
import scipy.stats
def JS_divergence(p,q):
    M=(p+q)/2
    return 0.5*scipy.stats.entropy(p,M)+0.5*scipy.stats.entropy(q, M)

p=np.asarray([0.65,0.25,0.07,0.03])
q=np.array([0.6,0.25,0.1,0.05])
q2=np.array([0.1,0.2,0.3,0.4])

print(JS_divergence(p, q))  # 0.003093977084273652
print(JS_divergence(p, q2)) # 0.24719159952098618
print(JS_divergence(p, p)) # 0.0
```

## Wassersteinè·ç¦»

Wassersteinè·ç¦»ï¼ˆWasserstein Distanceï¼‰ä¹Ÿç”¨äºè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ã€‚

å¯¹äºä¸¤ä¸ªåˆ†å¸ƒ $q_1, q_2$ï¼Œ$p_{th}âˆ’Wasserstein$ è·ç¦»å®šä¹‰ä¸ºï¼š

$$
W_{p}\left(q_{1}, q_{2}\right)=\left(\inf {\gamma(x, y) \in \Gamma\left(q{1}, q_{2}\right)} \mathbb{E}{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]\right)^{\frac{1}{p}}
$$

å…¶ä¸­  $\Gamma\left(q{1}, q_{2}\right)$ æ˜¯è¾¹é™…åˆ†å¸ƒä¸º $q_1$, $q_2$ çš„æ‰€æœ‰å¯èƒ½çš„è”åˆåˆ†å¸ƒé›†åˆ, ğ‘‘(ğ‘¥, ğ‘¦)ä¸ºxå’Œyçš„è·ç¦»ï¼Œæ¯”å¦‚ $â„“_p$ è·ç¦»ç­‰ç­‰

å¦‚æœå°†ä¸¤ä¸ªåˆ†å¸ƒçœ‹ä½œæ˜¯ä¸¤ä¸ªåœŸå †ï¼Œè”åˆåˆ†å¸ƒ Î³(x,y)çœ‹ä½œæ˜¯ä»åœŸå † $q_1$ çš„ä½ç½® x åˆ°åœŸå † $q_2$ çš„ä½ç½® y çš„æ¬è¿åœŸçš„æ•°é‡ï¼Œå¹¶æœ‰

$\sum_x \gamma(x, y)=q_2(y) \sum_y \gamma(x, y)=q_1(x)$

$\mathbb{E}{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]$

å¯ä»¥ç†è§£ä¸ºåœ¨è”åˆåˆ†å¸ƒ $\gamma(x, y)$ ä¸‹æŠŠå½¢çŠ¶ä¸º $q_1$ çš„åœŸå †æ¬è¿åˆ°å½¢çŠ¶ä¸º $q_2$ çš„åœŸå †æ‰€éœ€çš„å·¥ä½œé‡ï¼š

$$
\mathbb{E}{(x, y) \sim \gamma(x, y)}\left[d(x, y)^{p}\right]=\sum_{(x, y)} \gamma(x, y) d(x, y)^{p}
$$

å…¶ä¸­ä»åœŸå †$q_1$ä¸­çš„ç‚¹ x åˆ°åœŸå † $q_2$ ä¸­çš„ç‚¹ y çš„ç§»åŠ¨åœŸçš„æ•°é‡å’Œè·ç¦»åˆ†åˆ«ä¸º $\gamma(ğ‘¥, ğ‘¦)$ å’Œ $d(x, y)^{p}$ ã€‚

å› æ­¤ï¼ŒWasserstein è·ç¦»å¯ä»¥ç†è§£ä¸ºæ¬è¿åœŸå †çš„æœ€å°å·¥ä½œé‡ï¼Œä¹Ÿç§°ä¸ºæ¨åœŸæœºè·ç¦»ï¼ˆEarth-Moverâ€™s Distanceï¼ŒEMDï¼‰ã€‚

Wasserstein è·ç¦»ç›¸æ¯” KL æ•£åº¦å’Œ JS æ•£åº¦çš„ä¼˜åŠ¿åœ¨äºï¼šå³ä½¿ä¸¤ä¸ªåˆ†å¸ƒæ²¡æœ‰é‡å æˆ–è€…é‡å éå¸¸å°‘ï¼ŒWassersteinè·ç¦»ä»ç„¶èƒ½åæ˜ ä¸¤ä¸ªåˆ†å¸ƒçš„è¿œè¿‘ã€‚

å¯¹äº $mathbb{R}^N$ ç©ºé—´ä¸­çš„ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒ $p = mathbb{N}(Î¼_1,Î£_1)$ å’Œ $ q = mathbb{N}(Î¼_2,Î£_2)$, å®ƒä»¬çš„2nd Wasserstein è·ç¦»ä¸º

$$
W_{2}(p, q)=\left|\mu_{1}-\mu_{2}\right|{2}^{2}+\operatorname{tr}\left(\Sigma{1}+\Sigma_{2}-2\left(\Sigma_{2}^{\frac{1}{2}} \Sigma_{1} \Sigma_{2}^{\frac{1}{2}}\right)^{\frac{1}{2}}\right)
$$

å½“ä¸¤ä¸ªåˆ†å¸ƒçš„æ–¹å·®ä¸º 0 æ—¶ï¼Œ$2^{nd}-Wasserstein$ è·ç¦»ç­‰ä»·äºæ¬§æ°è·ç¦»ã€‚

[1]: https://datawhalechina.github.io/unusual-deep-learning/#/%E4%BA%A4%E5%8F%89%E7%86%B5%E5%92%8C%E6%95%A3%E5%BA%A6?id=%e4%ba%a4%e5%8f%89%e7%86%b5%e5%92%8c%e6%95%a3%e5%ba%a6
[2]: https://github.com/datawhalechina/unusual-deep-learning/edit/main/docs/02.%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80.md
[3]: https://zhuanlan.zhihu.com/p/165139520
[4]: https://exacity.github.io/deeplearningbook-chinese/Chapter3_probability_and_information_theory/
[5]: https://mp.weixin.qq.com/s/fg5GxW83Ui_joJKrsNQdBg
[6]: https://finance.sina.com.cn/stock/stockzmt/2020-05-09/doc-iirczymk0646869.shtml
[7]: https://zhuanlan.zhihu.com/p/143105854#5.1%20%E7%A6%BB%E6%95%A3%E5%88%86%E5%B8%83%E7%9A%84JS%E6%95%A3%E5%BA%A6python%E5%AE%9E%E7%8E%B0
[8]: https://chenjunren.gitbook.io/read-the-beauty-of-mathematics/di-6-zhang-xin-xi-de-du-liang-he-zuo-yong

TODO:https://d2l.ai/chapter_appendix-mathematics-for-deep-learning/information-theory.html#information
