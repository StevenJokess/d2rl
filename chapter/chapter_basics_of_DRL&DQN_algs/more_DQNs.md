

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-02-24 00:03:50
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-10-24 01:36:54
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# DQN改进算法

## 简介

DQN 算法敲开了深度强化学习的大门，但是作为先驱性的工作，其本身存在着一些问题以及一些可以改进的地方。于是，在 DQN 之后，学术界涌现出了非常多的改进算法。[1]

深度Q网络(DQN)有3个经典的变种：双深度Q网络(Double DQN)、竞争深度Q网络(Dueling DQN)、优先级双深度Q网络(Prioritized Replay Buffer)。

1）双深度Q网络：将动作选择和价值估计分开，避免Q值被过高估计。
2）竞争深度Q网络：将Q值分解为状态价值和**优势函数**，得到更多有用信息。
3）优先级双深度Q网络：将经验池中的**经验按照优先级进行采样**。[2]

本章将介绍其中两个非常著名的算法：Double DQN 和 Dueling DQN，这两个算法的实现非常简单，只需要在 DQN 的基础上稍加修改，它们能在一定程度上改善 DQN 的效果。如果读者想要了解更多、更详细的 DQN 改进方法，可以阅读 Rainbow 模型的论文及其引用文献。

## 双深度Q网络

双深度Q网络（double DQN，DDQN）。为什么要有DDQN呢？因为在实现上，由于总是选择当前最优的动作价值函数来更新当前的动作价值函数，导致Q 值往往是被高估的。

选择当前最优的动作价值函数来更新当前的动作价值函数的过程：如图 7.1 所示，这里有 4 个不同的小游戏，横轴代表迭代轮次，红色锯齿状的一直在变的线表示Q函数对不同的状态估计的平均 Q 值，有很多不同的状态，每个状态我们都进行采样，算出它们的 Q 值，然后进行平均。这条红色锯齿状的线在训练的过程中会改变，但它是不断上升的。因为Q函数是取决于策略的，在学习的过程中策略越来越强，我们得到的 Q 值会越来越大。在同一个状态， 我们得到奖励的期望会越来越大，所以一般而言，Q值都是上升的，但这是深度Q网络预估出来的值。

**为何高估？**：现在有4个动作，本来它们得到的Q值都是差不多的，它们得到的奖励也都是差不多的，但是**在估算的时候是有误差的**。如果第1个动作被高估了，那目标就会执行该动作，然后就会选这个高估的动作的Q值加上 $r_t$ 当作目标值。如果第4个动作被高估了，那目标就会选第4个动作的Q值加上 $r_t$ 当作目标值。所以目标总是会选那个**Q值被高估的动作**，我们也*总是会选那个奖励被高估的动作的Q值当作Q值的最大值的结果*去加上 $r_t$ 当作新目标值，因此目标值总是太大。

接下来我们就用策略去玩游戏，玩很多次，比如100万次，然后计算在某一个状态下，我们得到的 Q 值是多少。我们会得到在某一个状态采取某一个动作的累积奖励是多少。预估出来的值远比真实值大，且大很多，在每一个游戏中都是这样。所以DDQN的方法可以让预估值与真实值比较接近。

其原理是：**解耦**“选择当前最优的动作价值函数来更新当前的动作价值函数”这两个过程，双深度Q网络使用两个价值网络，一个网络用来执行动作选择，然后**用另一个网络的价值函数对应的动作值更新当前网络**。

即存在两个Q网络，一个是目标的Q网络，一个是真正需要更新的Q网络。具体实现方法是使用需要更新的Q网络选动作，然后使用目标的Q网络计算价值。双深度Q网络相较于深度Q网络的更改是最少的，它几乎没有增加任何的运算量，甚至连新的网络都不需要。唯一要改变的就是在找最佳动作 $a$ 的时候，本来使用 $Q^′$ 来计算，即用目标的Q网络来计算，现在改成用**需要更新的Q网络**来计算。

选择用DQN：

$$
a^{\star}=\underset{a}{\operatorname{argmax}} Q\left(s_{t+1}, a ; \boldsymbol{\theta}_t\right) .
$$

计算估计用目标网络：

$$
y_t=r_t+\gamma \cdot Q\left(s_{t+1}, a^{\star} ; \boldsymbol{\theta_t^{-}}\right) .
$$

Double DQN 中的 TD 目标：

$$
Y_t^{\text {DoubleDQN }} \equiv R_{t+1}+\gamma Q\left(S_{t+1},  a^* ; \boldsymbol{\theta_t^{-}}\right)
$$



> 对比：Double Q-learning 中的 TD 目标，
> $$
> Y_t^{\text {DoubleQ }} \equiv R_{t+1}+\gamma Q\left(S_{t+1}, \underset{a}{\operatorname{argmax}} Q\left(S_{t+1}, a; \boldsymbol{\theta}_t\right) ; \boldsymbol{\theta_t^{\prime}}\right)
> $$

它减少了高估问题：

$$
Q\left(s_{t+1}, a^{\star} ; \mathbf{\theta}^{-}\right) \leq \max _a Q\left(s_{t+1}, a ; \mathbf{\theta}^{-}\right) .
$$

但没完全解决高估问题。[5]

## 竞争深度Q网络（Dueling DQN）

Dueling DQN 是 DQN 另一种的改进算法，出自ICML 2016 Best Paper：DUELING NETWORK ARCHITECTURES FOR DEEP REINFORCEMENT LEARNING ，它在传统 DQN 的基础上只进行了微小的改动，但却能大幅提升 DQN 的表现。

在强化学习简介一节中，我们将状态动作价值函数 $Q$ 减去状态价值函数 $V$ 的结果定义为优势函数 $A$ ，即 $A(s, a)=Q(s, a)-V(s)$ 。

故，逆向思维，可将动作价值函数Q，分解为状态价值函数和优势函数二者相加，即 $\boldsymbol{Q}(s, a)=$ $V(s)+\boldsymbol{A}(s, a)$ 。将状态价值函数和优势函数分别建模，可以得到更多信息。

![dueling(Q=V+A)](../../img/dueling(Q=V+A).png)

上面图片中，第一个网络代表着原来的natural Q-net，下面的就是我们将其改变后的样子。

所以本质上，我们最终的矩阵 $\boldsymbol{Q}(s, a)$ 是将每一个 $V(s)$ 加到矩阵 $\boldsymbol{A}(s, a)$ 中得到的。在这种情况下，原来进行一次梯度回传只能够改变一个值Q，但是现在可以同时改变优势函数和状态价值函数。但是有时我们更新时不一定会将 $V(s)$ 和 $\boldsymbol{Q}(s, a)$ 都更新。

利用这种分解我们能得到什么好处？

1. ：某些情境下智能体只会关注状态的价值，而并不关心不同动作导致的差异；即使智能体更好地处理与动作关联较小的状态。[1]例如，智能体开车时，在前面没有车时，车辆自身动作并没有太大差异，此时智能体更关注状态价值；而当智能体前面有车时（智能体需要超车），智能体开始关注不同动作优势值的差异。
2. 反向传播时，能够更加频繁地学习状态价值函数，从而使状态价值函数更准确：每一次更新时，函数 $V$ 都会被更新。
3. 正向传播时，能通过优势函数，再利用学习到的状态价值函数，间接地学习该状态其他的所有动作价值函数：在假设优势函数不变的情况下，当状态价值函数的值被更新得到改变，就会影响到在该状态下所有动作价值函数去进行更新，这意味着我们即使不能够采样到所有的动作-价值对，也可以高效的估计Q值。[7]（对比，传统的 DQN 只会更新某个动作的 $Q$ 值，其他动作的 $Q$ 值就不会更新。这相比，也就是 Dueling DQN 会比 DQN 好的原因）

#### 可能的问题一：两个未知值相加得一已知值，导致V值和A值建模可能不唯一

对于 Dueling DQN 中的公式 $Q_{\eta, \alpha, \beta}(s, a)=V_{\eta, \alpha}(s)+A_{\eta, \beta}(s, a)$ ，它存在对于 **$V$ 值和 $A$ 值建模不唯一性**的问题。例如，对于同样的 $Q$ 值，如果将 $V$ 值 加上任意大小的常数 $C$, 再将所有 $A$ 值减去 $C$, 则得到的 $Q$ 值依然不变, 这就导致了训练的不稳定性。

**对应的解决方案**：

为了解决这一问题, Dueling DQN 强制最优动作的优势函数的实际输出为 0 ，即:

$$
Q_{\eta, \alpha, \beta}(s, a)=V_{\eta, \alpha}(s)+A_{\eta, \beta}(s, a)-\max _{a^{\prime}} A_{\eta, \beta}\left(s, a^{\prime}\right)
$$

此时 $V(s)=\max _a Q(s, a)$ ，可以确保 $V$ 值建模的唯一性。

#### 可能的问题二：无法分解的情况，当$V==0, Q=A$

可能出现一种极端的情况，就是 $V==0, Q=A$，导致无法分解。

**对应的解决方案**：

为了避免该情况发生，我们要对 $A$ 添加约束。最直观的就是同一状态下，所有动作的优势加起来为 0。

用平均值$\operatorname{mean} A(s, a)=\frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A_{\eta, \beta}\left(s, a^{\prime}\right)$就可实现这种约束效果，即:

$$
Q_{\eta, \alpha, \beta}(s, a)=V_{\eta, \alpha}(s)+A_{\eta, \beta}(s, a)-\frac{1}{|\mathcal{A}|} \sum_{a^{\prime}} A_{\eta, \beta}\left(s, a^{\prime}\right)
$$

此时 $V(s)=\frac{1}{|A|} \sum_{a^{\prime}} Q\left(s, a^{\prime}\right)$ 。在下面的代码实现中，我们将采取该种方式，虽然它不再满足贝尔曼最优方程，但实际应用时更加稳定。


有的读者可能会问: “为什么 Dueling DQN 会比 DQN 好? "部分原因在于 Dueling DQN 能更高效学习状态价值函数。每一次更新时，函数 $V$ 都会被更新，这也会影响到其他动作的 $Q$ 值。而传统的 DQN 只会更新某个动作的 $Q$ 值，其他动作的 $Q$ 值就不会更新。因此，Dueling DQN 能够更加频繁、 准确地学习状态价值函数。

### 代码

根据代码运行结果我们可以发现，相比于传统的 DQN，Dueling DQN 在多个动作选择下的学习更加稳定，得到的回报最大值也更大。由 Dueling DQN 的原理可知，随着动作空间的增大，Dueling DQN 相比于 DQN 的优势更为明显。之前我们在环境中设置的离散动作数为 11，我们可以增加离散动作数（例如 15、25 等），继续进行对比实验。

## 优先级经验回放（PER）

第三个技巧称为**优先级经验回放（prioritized experience replay，PER）**。如图 7.6 所示，我们原来在采样数据训练 Q 网络的时候，会均匀地从回放缓冲区里面采样数据。这样不一定是最好的， 因为也许有一些数据比较重要。

假设有一些数据，我们之前采样过，发现这些数据的时序差分误差特别大（时序差分误差就是网络的输出与目标之间的差距），这代表我们在训练网络的时候，这些数据是比较不好训练的。既然比较不好训练，就应该给它们比较大的概率被采样到，即给它**优先权（priority）**，这将大大缩短训练的时间，具体作法是赋予更大的采样权重：

> 选择权重的一个方法是根据 TD 偏差 (TD error) 的大小，TD 偏差越大，说明 该状态处的值函数与 TD 目标的差距越大，agent 的更新幅度变大，学习效率就 更高。[4]
> 对于经验池中第 $\mathrm{i}$ 条样本，设置它的权重为
> $$
> \mathrm{P}(\mathrm{i})=\frac{P_i^a}{\sum_k p_k^a}
> $$
>
> 其中 $P_i^a$ 由 TD 偏差决定，TD 偏差越大， $P_i^a$ 也就越大，对应的权重就越高。 因为采样分布与动作值函数的分布是两个完全不同的分布，为了矫正这个偏差， 更新参数时，需要在 TD 偏差前乘一个重要性采样系数
> $$
> \mathcal{W}(\mathrm{i})=\left(\frac{1}{N} * \frac{1}{P(i)}\right)^\beta
> $$

这样在训练的时候才会*多考虑那些不好训练的数据*。实际上在做 PER 的时候，我们不仅会更改采样的过程，还会因为更改了采样的过程，而更改更新参数的方法。所以PER不仅改变了采样数据的分布，还改变了训练过程。



## 噪声网络（noisy net）

这里要注意，在每个回合开始的时候，与环境交互之前，我们就采样噪声。接下来我们用固定的噪声网络玩游戏，直到游戏结束，才重新采样新的噪声，噪声在一个回合中是不能被改变的。OpenAI 与 DeepMind 在同时间提出了几乎一模一样的噪声网络方法，并且对应的两篇论文都发表在 ICLR 2018 会议中。不一样的地方是，他们用不同的方法加噪声。OpenAI 的方法比较简单，直接加一个高斯噪声，也就是把每一个参数、每一个权重（weight）都加一个高斯噪声。DeepMind 的方法比较复杂，该方法中的噪声是由一组参数控制的，网络可以自己决定噪声要加多大。但是两种方法的概念都是一样的，总之，我们就是对Q函数里面的网络加上一些噪声，把它变得有点儿不一样，即与原来的Q函数不一样，然后与环境交互。两篇论文里面都强调，参数虽然会被加上噪声，但在同一个回合里面参数是固定的。我们在换回合、玩另一场新的游戏的时候，才会重新采样噪声。在同一场游戏里面就是同一个噪声Q网络在玩该场游戏，这非常重要。因为这导致了噪声网络与原来的ε
ε-贪心或其他在动作上做采样的方法的本质上的差异。

有什么本质上的差异呢？在原来采样的方法中，比如 ε-贪心中，就算给定同样的状态，智能体采取的动作也不一定是一样的。因为智能体通过采样来决定动作，给定同一个状态，智能体根据Q函数的网络来输出一个动作，或者采样到随机来输出一个动作。所以给定相同的状态，如果是用 ε-贪心的方法，智能体可能会执行不同的动作。但实际上策略并不是这样的，一个真实世界的策略，给定同样的状态，它应该有同样的回应。而不是给定同样的状态，它有时候执行Q函数，有时候又是随机的，这是一个不正常的动作，是在真实的情况下不会出现的动作。但是如果我们是在Q函数的网络的参数上加噪声， 就不会出现这种情况。因为如果在Q函数的网络的参数上加噪声，在整个交互的过程中，在同一个回合里面，它的网络的参数总是固定的，所以看到相同或类似的状态，就会采取相同的动作，这是比较正常的。这被称为依赖状态的探索（state-dependent exploration），我们虽然会做探索这件事，但是探索是与状态有关系的，看到同样的状态，就会采取同样的探索的方式，而噪声（noisy）的动作只是随机乱试。但如果我们是在参数下加噪声，在同一个回合里面，参数是固定的，我们就是系统地尝试。比如，我们每次在某一个状态，都向左试试看。在下一次在玩同样游戏的时候，看到同样的状态，我再向右试试看，是系统地在探索环境。

## 分布式Q函数


图 7.12 分布式Q函数

除了选平均值最大的动作以外，我们还可以对分布建模。例如，我们可以考虑动作的分布，如果分布方差很大，这代表采取这个动作虽然平均而言很不错，但也许风险很高，我们可以训练一个网络来规避风险。在两个动作平均值都差不多的情况下，也许可以选一个风险比较小的动作来执行，这就是分布式Q函数的好处。

在训练过程中，DQN使用单个机器进行训练，这导致在实际中训练时间较长。为了充分利用计算资源，Nair A等人[17]提出一种分布式架构来加快算法的训练速度。它主要包括4个部分：①并行的行动者，算法采用N个不同的行动者，每个行动者复制一份 Q 网络，并在同一个环境中执行不同的动作，从而得到不同的经验；②经验回放存储机制，它将N个行动者与环境交互的经验存储到经验池中；③并行的学习者，算法采用N个学习者使用经验池存储的经验数据来计算损失函数的梯度，并发送到参数服务器中，从而对 Q 网络的参数进行更新；④参数服务器，用来接收学习者发送的梯度，并通过梯度下降的方式对Q网络参数进行更新。在49个Atari 2600游戏中，有41个游戏的基于分布式DQN算法的性能超过了DQN，并且在多种Atari 2600游戏中，分布式DQN算法的训练时间大大减少。[6]

## 彩虹（rainbow）

最后一个技巧称为彩虹（rainbow），如图 7.10 所示，它将**7个**没有冲突的技巧/算法综合起来的方法，7个技巧分别是————分别是深度Q网络、双深度Q网络、优先级经验回放的双深度Q网络、竞争深度Q网络、异步优势演员-评论员算法（A3C）、分布式Q函数、噪声网络，进而考察每一个技巧的贡献度或者与环境的交互是否是正反馈的。假设每个方法有一种自己的颜色（如果每一个单一颜色的线代表只用某一个方法），有 7 种不同颜色，把所有的颜色组合起来，就变成“彩虹”。[3]

横轴代表训练过程的帧数，纵轴代表玩十几个雅达利小游戏的平均分数的和，但它取的是分数的中位数。为什么是取中位数而不是直接取平均呢？因为不同小游戏的分数差距很大，如果取平均，某几个游戏可能会控制结果，因此我们取中位数。

去考察每一个技巧的贡献度或者与环境的交互是否是正反馈的：

1. 如果我们使用的是一般的深度Q网络（灰色的线），深度Q网络的性能不是很好。
1. 噪声深度Q网络（noisy DQN）比DQN的性能好很多。紫色的线代表 DDQN，DDQN 还挺有效的。
1. 优先级经验回放的双深度Q网络（prioritized DDQN）、竞争双深度Q网络（dueling DDQN）和分布式深度Q网络（distributional DQN）性能也挺高的。
1. **异步优势演员-评论员（asynchronous advantage actor-critic，A3C）**是演员-评论员的方法，A3C算法又被译作异步优势动作评价算法，我们会在第九章详细介绍异步优势演员-评论员算法。单纯的异步优势演员-评论员算法看起来是比深度Q网络强的。图 7.10 中没有多步方法，这是因为异步优势演员-评论员算法本身内部就有多步方法，所以实现异步优势演员-评论员算法就等同于实现多步方法，我们可以把 异步优势演员-评论员算法的结果看成多步方法的结果。

![图 7.10 彩虹方法](../img/rainbow.png)

我们把所有的方法加在一起，模型的表现会提高很多，但会不会有些方法其实是没有用的呢？我们可以去掉其中一种方法来判断这个方法是否有用。如图 7.11 所示，虚线就是彩虹方法去掉某一种方法以后的结果，黄色的虚线去掉多步方法后“掉”很多。彩虹是彩色的实线，去掉多步方法会“掉下来”，去掉优先级经验回放后会“掉下来”，去掉分布也会“掉下来”。 这边有一个有趣的地方，在开始的时候，分布训练的方法与其他方法速度差不多。但是我们去掉分布训练方法的时候，训练不会变慢，但是性能（performance）最后会收敛在比较差的地方。我们去掉噪声网络后性能也差一点儿，去掉竞争深度 Q 网络后性能也差一点儿，去掉双深度 Q 网络却没什么差别。所以我们把全部方法组合在一起的时候，去掉双深度 Q 网络是影响比较小的。当我们使用分布式深度Q网络的时候，本质上就不会高估奖励。我们是为了避免高估奖励才加了DDQN。如果我们使用了分布式深度Q网络，就可能不会有高估的结果，多数的情况是低估奖励的，所以变成DDQN没有用。

为什么分布式深度Q网络不会高估奖励奖励，反而会低估奖励呢？因为分布式深度Q网络输出的是一个分布的范围，输出的范围不可能是无限的，我们一定会设一个限制， 比如最大输出范围就是从 −
−10 ~ 10。假设得到的奖励超过 10，比如 100 怎么办？我们就当作没看到这件事，所以奖励很极端的值、很大的值是会被丢弃的，用分布式深度Q网络的时候，我们不会高估奖励，反而会低估奖励。

![图 7.11 彩虹：去掉其中一种方法](../../img/rainbow_2.png)

## 总结

在传统的 DQN 基础上，有两种非常容易实现的变式——Double DQN 和 Dueling DQN，Double DQN 解决了 DQN 中对值的过高估计，而 Dueling DQN 能够很好地学习到不同动作的差异性，在动作空间较大的环境下非常有效。从 Double DQN 和 Dueling DQN 的方法原理中，我们也能感受到深度强化学习的研究是在关注深度学习和强化学习有效结合：一是在深度学习的模块的基础上，强化学习方法如何更加有效地工作，并避免深度模型学习行为带来的一些问题，例如使用 Double DQN 解决值过高估计的问题；二是在强化学习的场景下，深度学习模型如何有效学习到有用的模式，例如设计 Dueling DQN 网络架构来高效地学习状态价值函数以及动作优势函数。

## 习题测试

1. 深度Q网络都有哪些变种？尽可能简单描述下其变化的内容。
1. Double DQN 解决DQN存在的什么问题？
1. 为什么 Dueling DQN 会比 DQN 好？
1.

## 扩展阅读：对 Q 值过高估计的定量分析

我们可以对 $Q$ 值的过高估计做简化的定量分析。假设在状态 $s$ 下所有动作的期望回报均无差异，即 $Q^*(s, a)=V^*(s)$ (此设置是为了定量分析所简化的情形，实际上不同动作的期望回报通常会存在差异 $)$; 假设神经网络估算误差 $Q_{\omega^{-}}(s, a)-V^*$ 服从 $[-1,1]$ 之间的均匀独立同分布; 假设动作空间大小为 $m$ 。那么，对于任意状态 $s$ ，有:

$$
\mathbb{E}\left[\max _a Q_\omega-(s, a)-\max _{a^{\prime}} Q_*\left(s, a^{\prime}\right)\right]=\frac{m-1}{m+1}
$$

即动作空间 $m$ 越大时， $Q$ 值过高，估计越严重。

**证明**: 将估算送差记为 $\epsilon_a=Q_{\omega^{-}}(s, a)-\max _{a^{\prime}} Q^*\left(s, a^{\prime}\right)$ ，由于估算误差对于不同的动作是独立的，因此有:

$$
P\left(\max _a \epsilon_a \leq x\right)=\prod_{a=1}^m P\left(\epsilon_a \leq x\right)
$$

$P\left(\epsilon_a \leq x\right)$ 是 $\epsilon_a$ 的累积分布函数（cumulative distribution function，即 CDF)，它可以具体被写为:

$$
P\left(\epsilon_a \leq x\right)= \begin{cases}0 & \text { if } x \leq-1 \\ \frac{1+x}{2} & \text { if } x \in(-1,1) \\ 1 & \text { if } x \geq 1\end{cases}
$$

因此，我们得到关于 $\max _{i 2} \epsilon_a$ 的累积分布函数:

$$
\begin{aligned}
P\left(\max _a \epsilon_a \leq x\right) & =\prod_{a=1}^m P\left(\epsilon_a \leq x\right) \\
& = \begin{cases}0 & \text { if } x \leq-1 \\
\left(\frac{1+x}{2}\right)^m & \text { if } x \in(-1,1) \\
1 & \text { if } x \geq 1\end{cases}
\end{aligned}
$$

最后我们可以得到:

$$
\begin{aligned}
\mathbb{E}\left[\max _a \epsilon_a\right] & =\int_{-1}^1 x \frac{\mathrm{d}}{\mathrm{d} x} P\left(\max _a \epsilon_a \leq x\right) \mathrm{d} x \\
& =\left[\left(\frac{x+1}{2}\right)^m \frac{m x-1}{m+1}\right]_{-1}^1 \\
& =\frac{m-1}{m+1}
\end{aligned}
$$

虽然这一分析简化了实际环境，但它仍然正确刻画了 $Q$ 值过高估计的一些性质，比如 $Q$ 值的过高估计随动作空间大小 $m$ 的增加而增加，换言之，在动作选择数更多的环境中， $Q$ 值的过高估计会更严重。


[1]: https://hrl.boyuai.com/chapter/2/dqn%E6%94%B9%E8%BF%9B%E7%AE%97%E6%B3%95
[2]: https://www.cnblogs.com/kailugaji/p/16140474.html
[3]: https://datawhalechina.github.io/easy-rl/#/chapter7/chapter7_questions&keywords
[4]: http://www.icdai.org/ibbb/2019/ID-0004.pdf
[5]: https://www.youtube.com/watch?v=X2-56QN79zc
[6]: https://zhuanlan.zhihu.com/p/433114679#3.5%20%E8%BF%91%E7%AB%AF%E7%AD%96%E7%95%A5%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95
[7]: https://zhuanlan.zhihu.com/p/548659845#4.dueling-network
