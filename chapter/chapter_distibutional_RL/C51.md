# C51

本篇讲大名鼎鼎的C51算法。

论文链接：A Distributional Perspective on Reinforcement Learning

经典强化学习算法和值分布强化学习的区别

上节我们说到，由于状态转移的随机性、状态表示的混叠效应，以及函数逼近的引入，智能体与环境之间存在着随机性，这就导致了折扣累计回报 Z 是一个随机变量，给定策略 \pi 后，随机变量 Z 服从一个分布，这个分布我们称之为值分布。经典强化学习算法是优化值分布的均值，而忽略了整个分布所提供的信息。

用一个图来表示是这样的：


图1 值分布示意图
如图1所示，经典强化学习算法只用到了值分布的均值，而还有更多宝贵的信息没有利用上，假如说将值分布当成是一个宝矿，那么经典强化学习算法只在一点进行挖掘，而值分布强化学习的动机是挖整个宝矿，即利用整个值分布来进行学习。

那么如何利用呢？

C51算法是其中一种利用方式。

在讲C51算法之前，我们先介绍贝尔曼操作符这个强化学习中最重要的概念。

经典强化学习算法中贝尔曼操作符

说得通俗一点，操作符其实就是一个运算，贝尔曼操作符就是一个迭代更新运算。对于策略 \pi

，贝尔曼操作符的定义为：

\mathcal{T}^{\pi}Q\left( x,a \right) :=\mathbb{E}R\left( x,a \right) +\gamma \underset{P,\pi}{\mathbb{E}}Q\left( x’,a' \right)

贝尔曼最优操作符为：

\mathcal{T}Q\left( x,a \right) :=\mathbb{E}R\left( x,a \right) +\gamma \mathbb{E}_P\max_{a'\in \mathcal{A}}Q\left( x’,a' \right)

关于贝尔曼操作符的理解：

（1）贝尔曼操作符的几何解释


图2 贝尔曼操作符的几何解释


如图2为贝尔曼操作符的几何解释。对于一个有限的状态空间 \mathcal{X} ，动作空间 \mathcal{A}

，行为值函数可表述为 Q\left( X,A \right) ，此处的 Q\left( X,A \right) 是一个 \left| \mathcal{X} \right|\times \left| \mathcal{A} \right| 的向量，该向量可以表示为超曲面上的一个点。

如果用几何的观点来解释贝尔曼操作符，那么贝尔曼操作符就是将值函数超曲面上的点 Q\left( X,A \right) 映射到超曲面上与它相邻的点 TQ\left( X,A \right)

命题1：固定策略 \pi 对应的操作符 \mathcal{T}^{\pi} 和最优操作符 \mathcal{T} 都是收缩映射。

所谓收缩映射是指，不管初始的行为值函数在哪，经过无穷次的操作符运算，行为值函数都会收敛到一个固定点。

对于固定策略 \pi 贝尔曼操作符，最终行为值函数会收敛到

Q^{\pi}\left( X,A \right)

对于最优操作贝尔曼操作符，最终行为值函数会收敛到 Q^*\left( X,A \right)



从命题1中我们看到，经典强化学习的贝尔曼操作符具有很好的性质：收缩性。这就保证了算法的收敛性。

有了这些知识，我们再看一看值分布贝尔曼操作符。

值分布贝尔曼操作符的定义

值分布贝尔曼操作符将一个随机变量映射成另外一个随机变量，即将一个分布映射成另外一个分布。

用数学式子来表示为：

\mathcal{T}^{\pi}Z\left( x,a \right) \overset{D}{:=}R\left( x,a \right) +\gamma P^{\pi}Z\left( x,a \right)

那么怎么去理解上面的式子呢？我们还是用图（论文里的图）来说明。

（1）首先定义转移操作符P^{\pi}:\mathcal{Z}\rightarrow \mathcal{Z}

P^{\pi}Z\left( x,a \right) \overset{D}{:=}Z\left( X',A' \right)

X \tilde{} P\left( \cdot |x,a \right) ,\ A'\tilde{}\pi \left( \cdot |X' \right)



即为下一个状态-动作对处的概率分布。我们假设为图3蓝色所示分布。


图3 下一个状态-动作对处的值分布


将随机变量乘以收缩因子 \gamma

后，图3的概率分布变为图4紫色所示分布。


图4 乘以收缩因子后概率分布的变化
假设回报也是一个常数随机变量，则 R\left( x,a \right) +\gamma P^{\pi}Z\left( x,a \right) 的概率分布由图4进行平移常数 R\left( x,a \right) 得到，如图5绿色所示分布。


图5 平移后的概率分布
下面我们不加证明地给出以下引理：

引理1：值分布贝尔曼操作符 \mathcal{T}^{\pi}:\mathcal{Z}\rightarrow \mathcal{Z} 在Wasserstein度量下是 \gamma -\text{收缩}

的。

尽管值分布贝尔曼操作符是关于Wasserstein度量收缩的，但分布式贝尔曼操作符并不能保证收敛到固定点。

C51算法

有了上面的铺垫，我们就可以介绍C51算法了。理解C51算法关键是要理解两个过程，即参数化分布和投影贝尔曼更新，下面我们一一介绍。

第一个知识点：参数化分布

正如图1所示，在任何状态-动作对(x,a)处，累计折扣回报都是一个分布。那么如何表示这个分布呢？c51算法的方法是对随机变量空间进行离散化。假设随机变量（这里指折扣累计回报

Z ）的最大值为 V_{MAX} ，最小值为 V_{MIN} ，并将最小值与最大值之间的区间均匀离散化为N个点，则每个等分支集为 \left\{ z_i=V_{\min}+i\Delta z:0\le i<N,\Delta z:=\frac{V_{MAX}-V_{MIN}}{N-1} \right\}

，所谓支集是指那些使得概率密度不为0的点。因此我们可以对概率分布进行参数化建模 \theta :\mathcal{X}\times \mathcal{A}\rightarrow \mathbb{R}^N



用数学式子表示为：

Z_{\theta}\left( x,a \right) =z_i\ \ w.p.\ p_i\left( x,a \right) :=\frac{e^{\theta _i\left( x,a \right)}}{\sum_j{e^{\theta _j\left( x,a \right)}}}

这样表示似乎还是比较抽象，我们还是用图来说明c51是如何参数化的：


图6 c51参数化的过程
如图6所示为C51参数化的过程，即参数的输出对应着相应的随机变量的概率。

第二个知识点：投影贝尔曼更新

正如前面所说的，值分布的贝尔曼更新公式为：

\mathcal{T}^{\pi}Z\left( x,a \right) \overset{D}{:=}R\left( x,a \right) +\gamma P^{\pi}Z\left( x,a \right)

图5给出了值分布贝尔曼操作符的作用过程。从图中我们看到，经过值分布贝尔曼操作符的作用后，新的随机变量的取值范围可能会超出第一个知识点中离散化的支集的范围，因此我们必须将贝尔曼操作符更新后的随机变量投影到离散化的支集上，这就是投影贝尔曼更新。

具体来说应该这样算：

比如给定一个样本单元 \left( x,a,r,x' \right) ，我们对每个支点（共N个）计算贝尔曼更新： \widehat{\mathcal{T}}z_j:=r+\gamma z_j,\ j=0,\cdots ,N-1 ， 然后将 \left( x',\pi \left( x' \right) \right) 处的每个支点

z_j 的值分布概率 p_j\left( x',\pi \left( x' \right) \right) 分配给 \widehat{\mathcal{T}}z_j 相邻的支点，相邻的支点所分得的概率为 \left[ 1-\frac{\left| \left[ \widehat{\mathcal{T}}z_j \right] _{V_{MIN}}^{V_{MAX}}-z_i \right|}{\Delta z} \right] _{0}^{1}p_j\left( x',\pi \left( x' \right) \right)

。

因此经过投影贝尔曼更新后，每个支点处的所得概率为：

\left( \Phi \hat{\mathcal{T}}Z_{\theta}\left( x,a \right) \right) _i=\sum_{j=0}^{N-1}{\left[ 1-\frac{\left| \left[ \widehat{\mathcal{T}}z_j \right] _{V_{MIN}}^{V_{MAX}}-z_i \right|}{\Delta z} \right] _{0}^{1}p_j\left( x',\pi \left( x' \right) \right)}



我们称之为投影贝尔曼更新。

为了进一步说明，我们再画一个简单的图来表示


图7 投影过程说明
如图7所示，将状态行为对 \left( x',a' \right) 的支点 z_0 对应的概率 p_0\left( x',a' \right)

投影到经过贝尔曼操作 \hat{\mathcal{T}}z_0 相邻的支点 z_0

和 z_1 ，则 z_0 分得的概率大小为：



\frac{z_1-\hat{\mathcal{T}}z_0}{\Delta z}p_0\left( x',a' \right) =\frac{z_0+\Delta z-\hat{\mathcal{T}}z_0}{\Delta z}p_0\left( x',a'\right) =1-\frac{\hat{\mathcal{T}}z_0-z_0}{\Delta z}p_0\left( x',a' \right)

经过上面的投影贝尔曼操作符之后，我们就会在每个状态-动作对处得到一个新的分布： \Phi \hat{\mathcal{T}}Z_{\theta}\left( x,a \right) 。

利用新的分布与旧的分布构建KL散度作为损失函数来训练参数化的值分布，即损失函数为：



D_{KL}\left( \Phi \hat{\mathcal{T}}Z_{\theta}\left( x,a \right) ||Z_{\theta}\left( x,a \right) \right)

至此c51算法的元素已经讲完了，最后，我们给出c51算法的流程。




C51算法与DQN相同的地方

（1）C51算法的框架依然是DQN算法

（2）采样过程依然使用 \epsilon -greedy 策略，这里贪婪是取期望贪婪

（3）采用单独的目标网络

C51算法与DQN不同的地方

（2）C51算法的卷积神经网络的输出不再是行为值函数，而是支点处的概率。

（3）C51算法的损失函数不再是均方差和而是如上所述的KL散度

最后一个问题，该算法为什么叫C51呢？

这是因为在论文中，作者将随机变量的取值分成了51个支点类。



本节完

下一篇我们会就C51算法进行一些讨论和分析，参考论文为：

An Analysis of Categorical Distributional Reinforcement Learning

论文链接为如下。小伙伴们可以提前下载阅读，或者坐等“包子哥”进一步更文

https://arxiv.org/pdf/1802.0816




[1]: https://www.bilibili.com/video/BV1qx411j7Tq/?spm_id_from=333.999.0.0
