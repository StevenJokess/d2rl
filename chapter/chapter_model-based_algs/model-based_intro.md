

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-02-23 20:09:19
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-04-09 23:51:46
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 基于模型的方法（Model-based）

## 与免模型的方法（Model-free）的起源对比

基于模型的方法（Model-based）主要发展自最优控制领域。通常先通过高斯过程（GP）或贝叶斯网络（BN）等工具针对具体问题建立模型，然后再通过机器学习的方法或最优控制的方法，如模型预测控制（MPC）、线性二次调节器（LQR）、线性二次高斯（LQG）、迭代学习控制（ICL）等进行求解[3]

免模型的方法（Model-free）更多地发展自机器学习领域，属于数据驱动的方法。算法通过大量采样，估计代理的状态、动作的值函数或回报函数，从而优化动作策略

## 免模型的方法（Model-free）的缺陷与为啥常用

### 缺陷

- 免模型方法无法从不带反馈信号的样本中学习，而反馈本身就是稀疏的，因此免模型方向样本利用率很低，而数据驱动的方法则需要大量采样
  - 比如在Atari平台上的《Space Invader》和《Seaquest》游戏中，智能体所获得的分数会随训练数据增加而增加。利用免模型DRL方法可能需要 2 亿帧画面才能学到比较好的效果。AlphaGo 最早在 Nature 公布的版本也需要 3000 万个盘面进行训练。而但凡与机械控制相关的问题，训练数据远不如视频图像这样的数据容易获取，因此只能在模拟器中进行训练。而模拟器与现实世界间的Reality Gap，直接限制了训练自其中算法的泛化性能。另外，数据的稀缺性也影响了其与DL技术的结合
- 免模型方法不对具体问题进行建模，而是尝试用一个通用的算法解决所有问题。而基于模型的方法则通过针对特定问题建立模型，充分利用了问题固有的信息。免模型方法在追求通用性的同时放弃这些富有价值的信息
- 基于模型的方法针对问题建立动力学模型，这个模型具有解释性。而免模型方法因为没有模型，解释性不强，调试困难
- 相比基于模型的方法，尤其是基于简单线性模型的方法，免模型方法不够稳定，在训练中极易发散

### 为啥常用

- 免模型的方法相对简单直观，开源实现丰富，比较容易上手
- 当前RL的发展还处于初级阶段，学界的研究重点还是集中在环境是确定的、静态的，状态主要是离散的、静态的、完全可观察的，反馈也是确定的问题（如Atari游戏）上。针对这种相对“简单”、基础、通用的问题，免模型方法本身很合适
- 绝大多数DRL方法是对DQN的扩展，属于免模型方法


## 分类

不同于免模型学习，有模型学习方法不是很好分类：很多方法之间都会有交叉。我们会给出一些例子，当然肯定不够详尽，覆盖不到全部。在这些例子里面， 模型 要么已知，要么是可学习的。

**背景：纯规划 ：**这种最基础的方法，从来不显示的表示策略，而是纯使用规划技术来选择行动，例如 [模型预测控制](/MPC.md) (model-predictive control, MPC)。在模型预测控制中，智能体每次观察环境的时候，都会计算得到一个对于当前模型最优的规划，这里的规划指的是未来一个固定时间段内，智能体会采取的所有行动（通过学习值函数，规划算法可能会考虑到超出范围的未来奖励）。智能体先执行规划的第一个行动，然后立即舍弃规划的剩余部分。每次准备和环境进行互动时，它会计算出一个新的规划，从而避免执行小于规划范围的规划给出的行动。

- [MBMF](https://sites.google.com/view/mbmf) 在一些深度强化学习的标准基准任务上，基于学习到的环境模型进行模型预测控制。

**专家迭代 ：**纯规划的后来之作，使用、学习策略的显示表示形式： $\pi_{\theta}(a|s)$ 。智能体在模型中应用了一种规划算法，类似蒙特卡洛树搜索(Monte Carlo Tree Search)，通过对当前策略进行采样生成规划的候选行为。这种算法得到的行动比策略本身生成的要好，所以相对于策略来说，它是“专家”。随后更新策略，以产生更类似于规划算法输出的行动。

- [ExIt](https://arxiv.org/abs/1705.08439) 算法用这种算法训练深层神经网络来玩 Hex
- [AlphaZero](https://arxiv.org/abs/1712.01815) 这种方法的另一个例子

**免模型方法的数据增强** ：使用免模型算法来训练策略或者 Q 函数，要么 1）更新智能体的时候，用构造出的假数据来增加真实经验 2）更新的时候 仅 使用构造的假数据

- [MBVE](https://arxiv.org/abs/1803.00101) 用假数据增加真实经验
- [World Models](https://worldmodels.github.io/) 全部用假数据来训练智能体，所以被称为：“在梦里训练”

Embedding Planning Loops into Policies. ：另一种方法直接把规划程序作为策略的子程序，这样在基于任何免模型算法训练策略输出的时候，整个规划就变成了策略的附属信息。这个框架最核心的概念就是，策略可以学习到如何以及何时使用规划。这使得模型偏差不再是问题，因为如果模型在某些状态下不利于规划，那么策略可以简单地学会忽略它。

参见 [I2A](https://arxiv.org/abs/1707.06203)智能体被这种想象力赋予了这种风格。

## Rollout

这个词经常会出现在 model-based 算法中，我一般常译作'展开'，或'模型展开'，用于描述如何使用 learned model 加速training过程。

实际意义: 在 current state 上，从每一个可能的action出发，根据给定的 policy 进行路径采样，最后根据多次采样的奖励和来对 current state 的每一个action的Q值进行估计。形象地描述就是，站在路口(current state)先按照大脑中的map(learned model)想象一下接下来每条路(action)的后果(future reward)。

- MC 中，采样是为了逐步使信息更准确，进而更准确地改善策略。
- Rollout 中，采样是采出每一步之后的一定信息，利用信息更新后，然后做出选择让这一步进入下一个状态（思想依然是主要关注当前状态）。[4]

[1]: https://spinningup.readthedocs.io/zh_CN/latest/spinningup/rl_intro2.html
[2]: https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html
[3]: https://yuancl.github.io/2019/02/22/rl/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%8E%B0%E7%8A%B6%E4%B8%8E%E6%9C%AA%E6%9D%A5/
[4]: https://zhuanlan.zhihu.com/p/115629505
