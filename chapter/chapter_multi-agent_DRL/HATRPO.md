研究的动机是什么？
在单智体强化学习（single-RL）中，置信域方法（trust-region method）有两个比较典型的算法，分别是置信域策略优化算法Trust Region Policy Optimization (TRPO)以及近端策略优化算法Proximal Policy Optimization (PPO)，他们在离散和连续RL问题上都表现出十分优越的性能。置信域方法的有效性，很大程度上源于其具有理论保证的策略迭代过程，即通过在当前策略的可信任邻域内做优化策略，使得置信域学习在每次迭代中都享有单调性能改进的保证(monotomic improvement gurantee)，从而避免朝着有风险的方向进行过于激进的策略更新。但是在多智能体强化学习（MARL）中，单调改进的特性并不能简单适用，因为即使在合作游戏中，智能体策略更新方向可能会相互冲突，导致算法陷入次优解。因此，如何在每个智能体单独行动的同时，保证联合策略上实现有理论保证的策略改进仍然是一个挑战。

主要解决了什么问题？
本文提出了第一个有理论保证的多智能体置信域方法策略优化方法。 该方法的核心理论是多智能体优势函数分解引理和序列策略更新方案，基于这两项技术突破，该工作将置信域策略学习理论扩展到 MARL领域，提出了异构智能体置信域策略优化（HATPRO）和异构智能体近端策略优化（HAPPO）算法。

与现有的许多主流 MARL 算法不同，HATRPO/HAPPO 不需要智能体之间共享参数，也不需要对联合价值函数的可分解性做任何限制性假设（比如QMIX中的单调性和加性）。更重要的是，本文在理论上证明了HATRPO/HAPPO 的单调改进特性(monotonic improvement guarantee)，并且在 Multi-Agent MuJoCo和StarCraftII的一系列任务上进行评估，结果表明，HATRPO 和 HAPPO 在几乎所有测试任务上都显著优于 IPPO、MAPPO 和 MADDPG 等经典算法。

所提方法是什么？
这个算法的核心是优势函数分解定理。本文提出并证明，在任何马尔科夫协作博弈中，智能体的联合优势函数可进行如下分解:

Ai1:mπ(s,ai1:m)=∑mi=1Aijπ(s,ai1:j−1,aij)
A
π
i
1:m
​

​
 (s,a
i
1
​
 :m
 )=∑
i=1
m
​
 A
π
i
j
​

​
 (s,a
i
1:j−1
​

 ,a
i
j
​

 )

这条优势函数的分解引理指明了一条可以进行多智能体策略优化的方法。



最后值得注意的是，这个基于优势函数的分解天然成立。不需要任何假设，例如智能体共享参数，或者联合Q函数可以被分解成单调函数累加。

为了阐述如何对联合策略进行优化，本文提出并证明， 为当前联合策略，对于任意联合策略 ，其期望奖励函数满足如下不等式



并提出了单调改进的多智能体策略迭代算法范式。



注意，此算法并不是简单的将TRPO应用于联合策略下的多智能体中，首先，本算法不是直接更新联合策略，而是序列化的更新每个智能体单独的策略，其次，在序列化更新的过程中，每个智能体有唯一的优化目标，这个优化目标结合了在这个智能体之前的所有智能体的策略更新，这一点也是保证单调改进性的关键。

以下详细证明请看原文



有了算法1之后我们可以写出TRPO型的目标方程：



最后，优势函数可以通过以下一个巧妙的方法进行估计：



可以看到，和传统的CTDE方法不同，每个智能体无需自己维持一个联合的值函数，而只需要维持一个联合的优势函数（注意这并不需要假设共享参数！），并通过引入一个额外的更新过的智能体的策略与其更新前的策略的比值作为调整因子，调整每个动作状态对的优势函数对策略更新的影响大小。

总结来说，HATRPO的伪代码如下：



一个简单暴力的PPO版本的HAPPO伪代码如下:



关键结果和结论是什么？
mappo采用参数共享的方式，在SMAC的多部分地图上都达到了100%的胜率，本文在SMAC的困难和极难的地图上，将MAPPO与HAPPO，HATRPO进行对比，结果表明即使不进行参数共享，HAPPO与HATRPO仍然能达到与MAPPO相近的性能。



为了更好的测试本文提出算法与其他算法的差异，本文在Multi-agent Mujoco场景下对比了更多的基准算法，实验场景涵盖了Mujoco环境下的大部分任务，并且从实验结果也能看出不同算法之间有很大的性能差异，HATRPO与HAPPO相比于其他算法也展现了更加优异和稳定的性能。对于HATRPO性能优于HAPPO，本文认为硬性KL约束相比于clip操作，更接近于维持算法的单调改进性。



创新点在哪里？
论文以提出的联合优势函数分解以及联合策略更新方式为核心，提出了序列化更新异构多智能体的算法，并通过理论证明，提出了能保证单调改进性的优化目标，丰富了多智能体协作式博弈的算法以及理论框架，提出的算法具有泛用性，并且具有优异的性能。彻底弥补了置信域方法在多智能体强化学习中缺失的问题。

有值得阅读的相关文献吗？
基础算法TRPO
Schulman J, Levine S, Abbeel P, et al. Trust region policy optimization[C]//International conference on machine learning. PMLR, 2015: 1889-1897.

基础算法PPO
Schulman J, Wolski F, Dhariwal P, et al. Proximal policy optimization algorithms[J]. arXiv preprint arXiv:1707.06347, 2017.

基础算法MAPPO
Yu C, Velu A, Vinitsky E, et al. The surprising effectiveness of mappo in cooperative, multi-agent games[J]. arXiv preprint arXiv:2103.01955, 2021.

综合评价如何？
该论文创新地提出了具有性能单调改进性保证的多智能体置信域算法，并给出了完备的理论证明，弥补了在多智能体领域单调改进性策略更新理论的空缺。并且提出了两种序列化更新的异构多智能体算法，算法可适应连续与离散场景，泛用性高，并且基于文中的实验可看出性能优异，是当前多智能体学习最强基线算法。同时，我们将该方法进一步拓展到带（安全）约束的多智能体策略优化问题中，并且提出了multi-agent constrianted policy optimization. MACPO同时享有符合约束的保证以及性能单调性提升的保证。感兴趣的同学可以关注 https://arxiv.org/abs/2110.02793.

[1]: http://rlchina.org/topic/154
