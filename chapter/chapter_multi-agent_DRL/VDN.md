# 值分解网络（value decomposition networks, VDN）

## value-based 的方法

在前面提到的 policy-based 方法中，中心化的值函数是直接使用全局信息进行建模，没有考虑个体的特点。在多智能体系统是由大规模的多个个体构成时，这样的值函数是难以学习或者是训练到收敛的，很难推导出理想的策略。并且仅依靠局部观测值，无法判断当前奖励是由于自身的行为还是环境中其他队友的行为而获得的。

## 值分解网络（value decomposition networks, VDN）简介

值分解网络（value decomposition networks, VDN）由 DeepMind 团队在 2018 年提出，该方法的核心是将全局的 Q(s,a)值分解为各个局部 Qi(si,ai)的加权和，每个智能体拥有各自的局部值函数。

这样的分解方式，在联合动作 Q 值的结构组成方面考虑了个体行为的特性，使得该 Q 值更易于学习。另一方面，它也能够适配集中式的训练方式，在一定程度上能够克服多智能体系统中环境不稳定的问题。在训练过程中，通过联合动作 Q 值来指导策略的优化，同时个体从全局 Q 值中提取局部的 Qi 值来完成各自的决策（如贪心策略 ai=argmax Qi），实现多智能体系统的分布式控制。


图 9：左图是完全分布式的局部 Q 值网络结构，右图是 VDN 的联合动作 Q 值网络结构。考虑两个智能体，它们的联合动作 Q 值由个体的 Q1 和 Q2 求和得到，在学习时针对这个联合 Q 值进行迭代更新，而在执行时个体根据各自的 Qi 值得到自身的动作 ai。图源：[11]
VDN 对于智能体之间的关系有较强的假设，但是，这样的假设并不一定适合所有合作式多智能体问题。在 2018 年的 ICML 会议上，有研究者提出了改进的方法 QMIX。


[1]: https://www.thepaper.cn/newsDetail_forward_9829763
