# Dec-POMDP

去中心化的部分可观测马尔科夫模型（Decentralized partially observable Markov decision progress，DEC-POMDP），是研究不确定性情况下多主体协同决策的重要模型。

由于其求解难度是 NEXP-complete，迄今为止尚没有有效的算法能求出其最优解，但是可以用强化学习来近似求解。

在多智能体强化学习中一种比较典型的学习模式为中心式训练，分布式执行，即在训练时利用所共享的信息来帮助更有效的分布式执行。然而，围绕如何最好地利用集中培训仍然存在着许多挑战。

其中一个挑战是如何表示和使用大多数强化学习方法学习的动作值函数。一方面，正确地捕捉主体行为的影响，需要一个集中的行动价值函数，它决定了全球状态和联合行动的条件。

另一方面，当存在多个 agent 时，这样的函数很难学习，即使可以学习，也无法提供明显的方法来提取分散的策略，允许每个智能体根据单个观察结果选择单个操作。[1]

[1]: https://blog.csdn.net/wzduang/article/details/115874734?spm=1001.2014.3001.5502
