# 多智能体系统

之前章节的设定都是单个智能体与环境的交互，即单智能体系统(Single-Agent System，缩写SAS)。

当同时存在多个智能体与环境交互时，整个系统就变成一个多智能体系统（multi-agent system）。每个智能体仍然是遵循着强化学习的目标，也就是是最大化能够获得的累积回报，而此时环境全局状态的改变就和所有智能体的联合动作（joint action）相关了。因此在智能体策略学习的过程中，需要考虑联合动作的影响。[2]

本章和后面三章介绍多智能体系统(Multi-Agent System，缩写MAS) 和多智能体强化学习(Multi-Agent Reinforcement Learning，缩写MARL)。本章讲解多智能体系统的基本概念，帮助大家理解MAS 与SAS 的区别。第14.1 节讲解MAS 的四种常见设定。第14.2 节定义MAS 的专业术语，将之前所学的观测、动作、奖励、策略、价值等概念推广到MAS。第14.3 节介绍几种常用的实验环境，用于对比MARL 方法的优劣。

## 哪些实体必须视为智能体？

能体A（例如出租车司机）是否必须将对象B（另一辆车）视为智能体，还是可以仅将其视为根据物理定律运行的对象，类似于海滩上的波浪或随风飘动的树叶？关键的区别在于B 的行为是否被最佳地描述为一个性能度量的最大化，而这一性能度量的值取决于智能体A的行为。[4]

## 研究历史

多智能体、多机器人、群智能算法、群体智能的比较

1. 生物的群体智能：人们首先注意到生物群体的活动，尤其是生物群体产生了超越个体的能力，这引起了人们的研究兴趣。
2. 物理的群体动力学：物理学家首先做了一些研究，关于群体动力学，如观念传播模型。
3. 计算机的群智能算法：计算机领域的研究者随后做了一些研究，被后世归结为群智能算法(蚁群、鱼群、粒子群等等)·
4. 机器人的多机器人：机器人领域的研究者偏向于制造出模仿生物集群的机器人群体，开启了多机器人领域。
5. 控制的多智能体：本世纪初，哈佛大学和加州理工学院的控制研究者注意到了这个新概念，迅速把它转化成一个控制问题。[5]

## 多智能体系统的设定

多智能体系统与单智能体系统的区别： 多智能体系统 (Multi-Agent System，缩写 MAS) 中包含m 个智能体，智能体共享环境，智能体之间会相互影响。智能体之间是如何相互影响的呢？一个智能体的动作会改变环境状态，从而影响其余所有智能体。举个例子，股市中的每个自动交易程序就可以看做一个智能体。尽管智能体（自动交易程序）之间不会交流，它们依然会相互影响：一个交易程序的决策会影响股价，从而对其它自动交易程序有利或有害。

注意，MAS 与上一章的并行强化学习是不同的概念。上一章用m 个节点并行计算，每个节点有独立的环境，每个环境中有一个智能体。虽然m 个节点上一共有m 个智能体，但是智能体之间完全独立，不会相互影响。而本章MAS 只有一个环境，环境中有m个相互影响的智能体。并行强化学习的设定是 m 个单智能体系统 (Single-Agent System，缩写 SAS) 的并集，可以视作 MAS 的一种特例。举个例子，环境中有 m 个机器人，这属于MAS 的设定。假如把每个机器人隔绝在一个密闭的房间中，机器人之间不会通信，那么MAS 就变成了多个SAS 的并集。

多智能体强化学习(Multi-Agent Reinforcement Learning，缩写MARL) 是指让多个智能体处于相同的环境中，每个智能体独立与环境交互，利用环境反馈的奖励改进自己的策略，以获得更高的回报（即累计奖励）。在多智能体系统中，一个智能体的策略不能简单依赖于自身的观测、动作，还需要考虑到其他智能体的观测、动作。因此，MARL 比 单智能体强化学习(Single-Agent Reinforcement Learning，缩写SARL) 更困难。

注意，一组旨在在真正的多智能体环境中协同工作的智能体程序必然表现出模块化，即这些程序不共享内部状态，只通过环境相互通信。在多智能体系统领域，通常将单智能体的智能体编程为一个模块化的自主子智能体的集合。在某些情况下，人们甚至可以证明，由此产生的系统提供了与整体设计相同的最佳解决方案。[3]

## 常见设定

多智能体系统有四种常见设定：合作关系(Fully Cooperative)、竞争关系(Fully Competitive)、合作竞争的混合(Mixed Cooperative & Competitive)、利己主义(Self-Interested)。
图14.1 举例说明了四种常见设定。接下来具体讲解这些设定

![图14.1: 多智能体强化学习的四种常见设定。](../img/MAS.png)

第一种设定是完全合作关系：智能体的利益一致, 获得的奖励相同, 有共同的目标（maximize the common benefits）。 比如图 14.1 中, 多个工业机器人协同装配汽车。他们的目标是相同的, 都希望把汽车装好。假设 共有 $m$ 个智能体, 它们在 $t$ 时刻获得的奖励分别是 $R_t^1, R_t^2, \cdots, R_t^m$ 。（用上标表示智能体，用下标表示时刻。在完全合作关系中, 它们的奖励是相同的：

$$
R_t^1=R_t^2=\cdots=R_t^m, \quad \forall t
$$

第二种设定是完全竞争关系：一方的收益是另一方的损失。比如两个机器人的格斗比赛、网球比赛[6], 它们的利益是冲突的, 一方的胜利就是另一方的失败，所以己方智能体希望通过最小化对手来最大化其利益。（maximize its benefits by minimizing the opponent ones）。在完全竞争的设定下, 双方的奖励是负相关的，对于所有的 $t$，有$R_t^1 \propto-R_t^2$ 。如果是零和博弈, 双方的获得的奖励总和等于 $0: R_t^1=-R_t^2$ 。

第三种设定是合作竞争的混合（Mixed of both adversarial and cooperative）。智能体分成多个群组；组内的智能体是合作关系，它们的奖励相同；组间是竞争关系，两组的奖励是负相关的。比如图14.1 中的足球机器人：两组是竞争关系，一方的进球是另一方的损失；而组内是合作关系，队友的利益是一致的。在出租车驾驶环境中，避免碰撞使所有智能体的性能度量最大化，因此它是一个部分合作的（cooperative）多智能体环境。它还具有部分竞争性，例如，一个停车位只能停一辆车。[4]

第四种设定是利己主义。系统内有多个智能体；一个智能体的动作会改变环境状态，从而让别的智能体受益或者受损。利己主义的意思是**智能体只想最大化自身的累计奖励，而不在乎他人收益或者受损。**比如图14.1 中的股票自动交易程序可以看做是一个智能体；环境（股市）中有多个智能体。这些智能体的目标都是最大化自身的收益，因此可以看做利己主义。智能体之间会相互影响：一个智能体的决策会影响股价，从而影响其他自动交易程序的收益。智能体之间有潜在而又未知的竞争与合作关系：一个智能体的决策可能会帮助其他智能体获利，也可能导致其他智能体受损。设计自动交易程序的时候，不应当把它看做孤立的系统，而应当考虑到其他自动交易程序的行为。

不同设定下学出的策略会有所不同。在合作的设定下，每个智能体的决策要考虑到队友的策略，要与队友做到尽量好的配合，而不是个人英雄主义；这个道理在足球、电子竞技中是显然的。在竞争的设定下，智能体要考虑到对手的策略，相应调整自身策略；比如在象棋游戏中，如果你很熟悉对手的套路，并相应调整自己的策略，那么你的胜算会更大。在利己主义的设定下，一个智能体的决策无需考虑其他智能体的利益，尽管一个智能体的动作可能会在客观上帮助或者妨害其他智能体。

## RL应用MAS的问题与挑战

强化学习应用在多智能体系统中会遇到哪些问题和挑战？[2]

- 环境的不稳定性：智能体在做决策的同时，其他智能体也在采取动作；环境状态的变化与所有智能体的联合动作相关；
- 智能体获取信息的局限性：不一定能够获得全局的信息，智能体仅能获取局部的观测信息，但无法得知其他智能体的观测信息、动作和奖励等信息；
- 个体的目标一致性：各智能体的目标可能是最优的全局回报；也可能是各自局部回报的最优；
- 可拓展性：在大规模的多智能体系统中，就会涉及到高维度的状态空间和动作空间，对于模型表达能力和真实场景中的硬件算力有一定的要求。

## 多智能体VS单智能体的智能体设计

多智能体环境中的智能体设计问题通常与单智能体环境下有较大差异。例如，在多智能体环境中，通信通常作为一种理性行为出现；在某些竞争环境中，随机行为是理性的，因为它避免了一些可预测性的陷阱。

[1]: https://www.math.pku.edu.cn/teachers/zhzhang/drl_v1.pdf
[2]: https://www.thepaper.cn/newsDetail_forward_9829763
[3]: https://ticket-assets.baai.ac.cn/uploads/%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%9A%E7%8E%B0%E4%BB%A3%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC4%E7%89%88%EF%BC%89%E3%80%8B%E6%A0%B7%E7%AB%A0.pdf
[4]: https://ticket-assets.baai.ac.cn/uploads/%E3%80%8A%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%EF%BC%9A%E7%8E%B0%E4%BB%A3%E6%96%B9%E6%B3%95%EF%BC%88%E7%AC%AC4%E7%89%88%EF%BC%89%E3%80%8B%E6%A0%B7%E7%AB%A0.pdf
[5]: https://zhuanlan.zhihu.com/p/343976644
[6]: https://huggingface.co/learn/deep-rl-course/unit7/introduction-to-marl?fw=pt
[7]:
