
3. 解决方案： 函数塑形（reward shaping）

3.1 直觉解决方案： 额外奖励法

一个直觉的方法解决奖励稀疏性问题是当agent向目标迈进一步时，给于agent 回报函数（reward）之外的奖励。 R'(s,a,s') = R(s,a,s')+F(s'). 其中R'(s,a,s') 是改变后的新回报函数。 这个过程称之为函数塑形（reward shaping）。


3.2 改变Reward可能改变问题的最优解。

比如上图MDP的最优解方案是在 s_{A} 和 s_{B} 中间来回走动，不停的得到+1的奖励。 俗称“刷分”。 很明显， 刷分得到的奖励比直接走向终点 s_{T} 得到的累计回报更多。 改变奖励函数导致问题的最优解发生变化， 这违背了我们加速学习不改变任务的解的初衷。

3.3 势能函数，解决“刷分”

势能函数，记为 \Phi(s) 定义了一个状态的势能. 这个概念借鉴了物理学的势能概念。 当agent从一个高势能状态转移到第低势能转移时，它将获得额外地奖励（类似物理中的动力）。 反过来，如果agent从低势能状态到高势能状态，它将失去奖励（得到一个和上面相同但负的奖励）。 这个机制和物理中的能量守恒类似。

使用两个状态的势能差值作为额外奖励可以保证不改变MDP的最优解。 详细证明参考[1].

R'(s,a,s') = R(s,a,s')+F(s')

F(s')=\Phi(s')-\gamma\Phi(s) 其中 \gamma 是折扣因子

类似Reward function势能函数F也可以定义为F(s,a)和F(s,a,s')。

[1] 证明势能函数的本质，就是Q函数的初始化状态。 一般我们使用全0 Q-value作为初始Q-value，通过更新Q-value最终收敛到最优值. 势能函数改变Q-value的初始状态。好的势能函数一定程度上接近最优Q-value， 可以减少一些早期学习从而加速学习过程。 试想一下，如果势能函数完全等于最优Q-value， 当前状态仅仅学习一步满足Bellman方程。

Q(s,a)=R(s)+F(s)+Q^{Init}(s',a') 其中 Q^{Init}(s',a') 为全0 Q-value

4. 应用举例

函数塑形可以作为一个插入人类知识的入口。 在许多任务中， 人类总结了大量的次优（大概对， 不是100%对， 基本上符合经验)启发式的规则。 记为 a_{h} = \Phi(s) ,其中 a_{h} 是根据启发规则在状态s下得到的启发行为。 可以用agent所选择action是否和启发规则一致作为势能函数。 如果agent的行动<s,a>和启发规则一致，则认为该<s,a>具体高势能。 参考文献[3] 使用和人类示例数据的高斯距离作为势能函数，加快RL学习同事保险证收敛到最优解。



5. 结论

为了解决基于值函数的RL在稀疏奖励空间学习慢的问题， 当agent靠近最终目标或者完成一个子任务时，将被给予额外的奖励。 然而改变奖励函数可能改变MDP的最优解。 使用势能函数差作为塑形函数可以保证不改变MDP的最优解。 定义势能函数的时，插入人对任务的领域知识，增加状态-行为空间奖励信号，有助于加快RL agent学习。
