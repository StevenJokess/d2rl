

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-02-23 19:56:31
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-02-23 20:04:20
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 总结与展望

## 总结

亲爱的读者，你已经完成了对本书内容的学习，包括：

- 强化学习基础中关于强化学习的基本概念和基础的表格型强化学习算法；
- 强化学习进阶中关于深度强化学习的思维方式、**深度价值函数和深度策略学习方法**；
- 强化学习前沿中关于模仿学习、模型预测控制、基于模型的策略优化、离线强化学习、目标导向的强化学习和多智能体强化学习。

至此，你已经掌握了强化学习的基本知识，更拥有了第一手的强化学习代码实践经验。 但我们要知道，对于强化学习的学习是无止境的，本书只是探索强化学习浩瀚世界的开始。近年来强化学习的科研进展极快，主流的机器学习和人工智能顶级学术会议超过五分之一的论文都是关于强化学习的，计算机视觉、智能语音、自然语言处理、数据挖掘、信息检索、计算机图形学、计算机网络等方向越来越多的学术会议和期刊的研究工作在使用强化学习来解决其领域中的关键决策优化问题。越来越多的企业开始在实际业务中使用强化学习技术，让它们的决策系统变得越来越智能，而一些以强化学习为核心技术的国内外初创公司则开始在业界崭露头角。几乎每天，我们都可以从各种渠道了解到强化学习技术最新的科研进展和产业落地情况，其中的很多成果都会让人眼前一亮。

## 展望：克服强化学习的落地挑战

再厉害的技术都需要通过落地服务人民来创造真正的价值。强化学习技术发展的总目标就是有效落地，从而服务于广泛的决策任务。本书作者以浅薄的学识，对强化学习的技术发展做出一些展望，希望能为读者在未来对于强化学习的学习、科研和落地应用提供一些帮助。 首先我们给出在强化学习算法研究方面的展望。

（1）**提升样本效率是强化学习一直以来的目标。** 由于强化学习的交互式学习本质，策略或者价值函数是否能从交互得到的数据中获得有效的提升并没有保证，以至于强化学习算法总是存在样本效率低的问题（尤其是深度强化学习）。在本书的第三部分中，我们已经从多方面讨论了当前主流的提升强化学习样本效率的方法，包括模仿学习、基于模型的策略优化、目标导向的强化学习等。这些方法目前都是强化学习的前沿研究方向，但各自都具有较强的局限性。我们有理由相信，在未来的研究中，强化学习算法的样本效率会持续提升，最终在算力需求和数据采样需求方面都能降低到可观的水平。

（2）**在奖励函数并不明确的场景下学习有效的策略。** 在标准的强化学习任务中，奖励函数总是确定的。在不少现实场景中，甚至人类也无法确定什么样的奖励函数是好的，但可以给出一个不错的行为控制。对于这个问题，模仿学习目前是一类主流的解决方案，主要方法包括行为克隆、逆向强化学习和占用度量匹配。尽管逆向强化学习和占用度量匹配在模仿学习的研究中占主体，但其训练过程复杂、训练不稳定等问题限制了其在实际场景中的广泛应用。近年来自模仿学习（self-imitation learning）的一些研究开始进入人们的视野，其基本框架就是最简单的行为克隆，但需要对学习的目标行为做一些筛选或者权重分配，进而在训练十分简单的前提下使学习策略的性能获得可观的提升。类似这样的方法有望在各种强化学习的实际场景中落地。

（3）  **以离线的方式学习到一个较好的策略。** 我们在本书中讨论到，离线强化学习使得智能体能从一个离线的经验数据中直接学习到一个较好的策略，在此过程中智能体不和环境交互。从理论上讲，这样的离线强化学习任务极大地拓展了强化学习适用的场景，但现在主流的离线强化学习研究仍然假设离线数据较为丰富，并且探索性较强，对学习到的策略总能完成在线的测试。这样的研究设定其实并不现实，根据离线强化学习评测平台NeoRL给出的评测结果，在离线数据较少、行为策略较为保守并且缺乏在线测试条件的情况下，大多数离线强化学习无法奏效。以学习的方式构建一个高仿真度的模拟器，进而对策略进行评测和训练，可能是一条有效的路线。

（4）  **真实世界中的分布式决策智能快速发展。** 场景中经常出现不止一个智能体，例如多人游戏、无人驾驶、物品排名等场景。在多智能体场景下，有效训练智能体和其他智能体之间的协作和对抗策略具有很高的挑战性，而直接评估一个具体的策略则没有太大的意义，因为给定当前策略，对手总是能训练出专门克制该策略的策略。目前刚刚兴起的一种有效的解决方案是开放博弈中的种群训练，博弈双方或多方通过构建自己的策略池以及训练采样单个策略的元策略，在开放博弈中寻找元策略的均衡点，从而得到总体不败的元策略和对训练算法的总体评估。然而，此类方法的计算复杂度过高，对算法和算力都提出了更高的要求。近期出现的流水线PSRO算法以及MALib、OpenSpiel等计算框架有望让开放博弈下的多智能体强化学习取得突破，服务于真实世界中的分布式决策任务。

此外，我们也从工业落地的具体角度，浅谈强化学习落地的实际挑战。一方面，强化学习的技术门槛较高，具备成功落地强化学习完成智能决策任务能力的工程师较少；另一方面，对具体的场景任务的领域知识的了解程度对于有效落地强化学习算法十分重要。因此，我们认为克服强化学习落地的实际挑战有两种路线。

（1）**自动化的强化学习**。 在强化学习任务中需要进行选择，包括场景的设定、算法的选择、模型架构的设计、学习算法的超参数规划等。如果能设计一套自动搜索最佳选择的解决方案，则有望大幅度降低强化学习的落地使用门槛。在深度学习任务中，自动化的网络架构搜索、超参数调优等工作已经被证明能有效地自动学习超越人类设计的模型，这类方法被称为自动机器学习（automated machine learning，AutoML）。这样的思路对于强化学习的落地自然是很有希望的，但是自动化的强化学习很可能会耗费极大的算力，因为相比于深度有监督学习，深度强化学习中单个策略的训练已经*需要高于一个数量级以上的算力*了，那么自动化的强化学习则需要比AutoML耗费更多的算力。

（2）**培养深入各个场景一线的强化学习工程师**。 另外一个可以使强化学习平民化的路线其实更加直接，即积极培养针对不同实际场景的强化学习工程师。长期深入具体场景一线的工程师能够精准地把握强化学习问题的具体设定，例如奖励函数的设计、策略的限制、数据量是否足够、场景的探索是否充分等要素，从而通过人类智慧高效地完成强化学习的训练任务。本书则希望为强化学习工程师的培养略尽绵薄之力。[1]

[1]: https://hrl.boyuai.com/chapter/ending/
