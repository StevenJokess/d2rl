

<!--
 * @version:
 * @Author:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @Date: 2023-03-01 00:20:18
 * @LastEditors:  StevenJokess（蔡舒起） https://github.com/StevenJokess
 * @LastEditTime: 2023-03-01 01:10:43
 * @Description:
 * @Help me: 如有帮助，请赞助，失业3年了。![支付宝收款码](https://github.com/StevenJokess/d2rl/blob/master/img/%E6%94%B6.jpg)
 * @TODO::
 * @Reference:
-->
# 资源

如果你是一位深度强化学习的研究者，你现在可能已经对深度强化学习有了很多的了解。你知道它 “很难_ 而且 `不总是有效`_ 。即便是严格按照步骤来， `可重现性`_ 依然是一大挑战。如果你准备从头开始， `学习的曲线非常陡峭`_ 。虽然已经有很多很棒的学习资源（比如 ），但是因为很多资料都很新，以至于还没有一条清晰明确的捷径。这个项目的目的就是帮助你克服这些一开始的障碍，并且让你清楚的知道，如何成为一名深度强化学习研究院。在这个项目里，我们会介绍一些有用的课程，作为基础知识，同时把一些可能适合研究的方向结合进来。

正确的背景
建立良好的数学背景 从概率和统计学的角度，要对于随机变量、贝叶斯定理、链式法则、期望、标准差和重要性抽样等要有很好的理解。从多重积分的角度，要了解梯度和泰勒展开（可选，但是会很有用）。

对于深度学习要有基础的了解 你不用知道每一个技巧和结构，但是了解基础的知识很有帮助。要了解多层感知机额、LSTM、GRU、卷积、层、resnets、注意力机制、mechanisms，常见的正则手段(weight decay, dropout)，归一化方式(batch norm, layer norm, weight norm)和优化方式(SGD, momentum SGD, Adam, 以及其它)。要了解什么是 reparameterization trick 。

至少熟悉一种深度学习框架 Tensorflow or PyTorch 非常适合练手。你不用知道所有东西，但是你要能非常自信的实现一种监督学习算法。

对于强化学习中的主要概念和术语很了解 知道什么是状态、行动、轨迹、策略、奖励、值函数和行动值函数。如果你对这些不了解，去读一读项目里面介绍部分的材料。OpenAI Hackthon 的 [强化学习介绍](https://spinningup.readthedocs.io/zh_CN/latest/spinningup/spinningup.html#id79) 也很值得看，或者是 Lilian Weng 的 [综述](https://arxiv.org/abs/1810.06339)。如果你对于数学理论很感兴趣，可以学习 [monotonic improvement theory](http://joschu.net/docs/thesis.pdf) （策略梯度算法的的基础）或者 [classical RL algorithms](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf) （尽管被深度强化学习所替代，但还是有很多能推动新的研究的 insights）。

## 在动手中学习

**自己实现算法** 你应该尽可能地从头开始编写尽可能多的深度学习的核心算法，同时要保证自己的实现尽量简单、正确。这是了解这些算法如何工作、培养性能特征直觉的最佳方法。

**简单是最重要的 **你要对自己的工作有合理的规划，从最简单的算法开始，然后慢慢引入复杂性。如果你一开始就构建很多复杂的部分，有可能会耗费你接下来几周的时间来尝试调试。对于刚刚接触强化学习的人来说，这是很常见的问题。如果你发现自己被困在其中，不要气馁，尝试回到最开始然后换一种更简单的算法。

**哪些算法？** 你可以按照 vanilla policy gradient(也被称为 REINFORCE )、DQN, A2C ( A3C 的同步版本), PPO (具有 clipped objective 特性的变体), DDPG 的顺序来学习。 这些算法的最简版本可以用几百行代码编写（大约250-300行），有些更少，比如 a no-frills version of VPG 只需要 80 行的代码。再写并行版本代码之前，先尝试写单线程版本的。（至少实现一种并行的算法）

**注重理解** 编写有效的强化学习代码需要对于算法有明确的理解，同时注重细节。因为错误的代码总是悄无声息：看起来运行的很正常，但实际上智能体什么也没有学到。这种情况通常是因为有些公式写错了，或者分布不对，又或者数据传输到了错误的地方。有时候找到这些错误的唯一办法，就是批判性地阅读代码，明确知道它应该做什么，找到它偏离正确行为的地方。这就需要你一方面了解学术文献，另一方面参考已有的实现，所以你要花很多时间在这些上面。

**看论文的时候要注意什么** 当基于一篇论文实现算法的时候，要彻读论文

**但是不要过拟合论文的细节。**有时，本文规定了比严格必要的技巧更多的技巧，因此请对此有所警惕，并在可能的情况下尝试简化。例如，原始DDPG论文提出了一个复杂的神经网络体系结构和初始化方案，以及批处理规范化。这些并不是严格必要的，并且某些最不明显的DDPG结果使用了更简单的网络。作为另一个示例，原始的A3C论文使用各种演员学习者的异步更新，但事实证明，同步更新也可以使用。

**也不要过拟合现有实施。**研究现有的实现灵感，但请注意不要过分地适应这些实施的工程细节。RL库经常为抽象做出选择，这对于算法之间的代码重复使用非常有用，但是如果您仅编写单个算法或支持单个用例，这是不必要的。

**在简单的环境中快速迭代。**要调试您的实现，请尝试使用简单的环境尝试，其中应快速进行学习，例如Cartpole-V0，InvertedPendulum-V0，Frozenlake-V0和HalfCheetah-V2（短时间范围 - 仅100或250步，而不是完整的1000步）来自Openai Gym。如果您尚未首先确认它适用于最简单的任务，请不要尝试在Atari或复杂的Humanoid环境中运行算法。在调试阶段，您理想的实验周转时间<5分钟（在本地机器上）或稍长少，但不多。这些小型实验不需要任何特殊的硬件，并且可以在CPU上没有太多麻烦。

**如果不起作用，请假设有一个BUG。**在求助于调整超参数之前，请花费大量精力搜索BUG：通常是一个错误。不良的超参数可能会大大降低RL性能，但是如果您使用的超参数类似于论文和标准实施中的超参数，那么这些可能不是问题。还要记住：有时即使您有一个破坏性的错误，有时事情也会在一个环境中起作用，因此，一旦结果看起来很有希望，请确保在一个以上的环境中进行测试。

**测量一切。**做很多测量，看看到底发生了什么。关于您在每次迭代中读取的学习过程的统计数据越多，调试就越容易 - 毕竟，如果看不到它正在破裂，就无法确定它会破裂。我个人喜欢查看累计奖励，情节长度和价值函数估计的平均/std/min/max，以及目标的损失以及任何探索参数的详细信息（例如，随机策略优化的平均熵，或当前用于epsilon-greedy的Epsilon，如DQN中）。另外，时不时观看代理商表现的视频；这将为您提供一些您不会获得的见解。

当事情起作用时进行比例实验（Scale experiments）。在实现RL算法的实现之后，该算法似乎在最简单的环境中正常工作，请在更艰难的环境上进行测试。在此阶段的实验将花费更长的时间 - 在几个小时到几天之间的某个地方，具体取决于不同。在这一点上，专门的硬件（例如强大beefy的GPU或32核机）可能会很有用，您应该考虑研究AWS或GCE等云计算资源。

**保持这些习惯！**这些习惯值得超越您仅仅了解Deep RL的阶段，它们将加速您的研究！

## 开展一个研究项目

一旦您对Deep RL中的基础知识感到合理地感到满意，就应该开始推动界限并进行研究。要到达那里，您需要一个项目的想法。

**首先探索文献以意识到该领域的主题**。您可能会发现广泛的主题：样本效率，探索，转移学习，层次结构，内存，基于模型的RL，元学习和多代理，仅举几例。如果您正在寻找灵感，或者只是想对那里的内容有粗略的了解，请查看Spining UP的关键论文列表。在其中一位主题中找到您喜欢的论文 - 激发您的灵感 - 并彻底阅读。使用相关的工作部分和引用来查找密切相关的论文，并深入研究文献。您将开始弄清楚未解决的问题在哪里以及在哪里产生影响。

**创意生成的方法**：有许多不同的方法开始思考项目的想法，而您选择的框架会影响项目的发展方式以及面临什么风险。这里有一些例子：

**框架1：改进现有方法。**这是增量角度，您尝试通过调整现有算法来在已建立的问题设置中获得性能提高。在这里重新实现先前的工作非常有帮助，因为它使您接触到现有算法变脆并可以改善的方式。新手（novice）将发现这是最容易有所收获的框架，但对于任何经验的研究人员来说，这也是值得的。尽管一些研究人员发现渐进主义不那么令人兴奋，但机器学习中一些最令人印象深刻的成就来自这种性质的工作。

因为这样的项目与现有方法息息相关，所以它们本质上是狭窄的范围，并且可以迅速结束（几个月），这可能是可取的（尤其是在作为研究人员刚开始时）。但这也设定了风险：您想到的算法的调整可能无法改善它，在这种情况下，除非您提出更多的调整，否则该项目已经结束，并且您没有明确的信号接下来做什么。

**框架2：专注于未解决的基准测试。**您没有考虑如何改善现有方法，而是打算成功完成以前没有人解决的任务。例如：从训练水平到声音域或健身房复古的测试水平实现完美的概括。当您对未解决的任务进行锤击时，您可能会尝试多种方法，包括先前的方法和您为项目发明的新方法。新手有可能批准这种问题，但是学习曲线会更陡峭。

此框架中的项目具有广泛的范围，可以持续一段时间（几个月至一年以上）。主要风险是，基准无法解决，没有实质性的突破，这意味着在不取得任何进展的情况下，很容易花费大量时间。但是，即使这样的项目失败了，它也经常导致研究人员获得许多新的见解，这些见解成为下一个项目的肥沃土壤。

**框架3：创建一个新的问题设置。**与其考虑现有方法或当前的宏伟挑战，不如想一想尚未研究的完全不同的概念问题。然后，弄清楚如何取得进步。对于这些行的项目，标准的基准可能还不存在，您将不得不设计一个基准。这可能是一个巨大的挑战，但值得一提的是 - 优质的基准测试将整个领域向前迈进。

当他们出现这个框架中会出现问题 - 很难去寻找它们。

**避免重新发明轮子。**当您想出一个好主意时，您想开始测试，那就太好了！但是，当您仍处于早期阶段时，请进行最彻底的检查，以确保尚未完成。进入项目中途可能会令人沮丧，然后才发现已经有关于您的想法的论文。当该工作也发生了时，这尤其令人沮丧，这是不时发生的！但是，不要让这阻止您 - 绝对不要让它激励您通过不太精通的研究并过分地确定部分工作的优点。进行良好的研究并通过完整而彻底的调查来完成您的项目，因为这是重要的，从长远来看，这是最重要的。


## 做严谨的强化学习研究

现在，您已经提出了一个想法，并且可以肯定的是尚未完成。您使用开发的技能来实施它，并开始对标准域进行测试。看起来它有效！但是，这意味着什么，以及必须如何工作才能重要？这是Deep RL研究中最难的部分之一。为了验证您的建议是有意义的贡献，您必须严格证明它实际上比最强的基线算法获得了性能收益 - 无论目前在测试域中都能实现SOTA（最新技术）。如果您发明了一个新的测试域，因此没有以前的SOTA，您仍然需要尝试文献中最可靠的算法的任何内容，即在新的测试域中可以合理地做得很好，然后您必须击败它。

**设置公平的比较。**如果您从头开始实施基线（而不是直接与其他论文的数字进行比较），那么花费尽可能多的时间来调整基线，就像花费自己的算法一样。这将确保比较公平。另外，即使您的算法和基线之间存在实质性差异，也要尽最大努力保持“所有其他平等”。例如，如果您正在研究体系结构变体，请在模型和基线之间保持模型参数的数量大致相等。在任何情况下，基准！事实证明，RL中的基线非常强大，并且获得巨大，一致的胜利可能很棘手，或者需要在算法设计中进行一些良好的见解。

**消除作为混杂因素的随机性。**提防随机种子，使事情看起来比实际情况更强大或更弱，因此请设置许多随机种子去运行所有东西（至少3个，但是如果您想彻底，请执行10个或更多）。这确实很重要，值得重点：在许多常见用例中，关于随机种子的深度RL似乎*相当脆弱*。有足够的差异，两个不同的随机种子可以产生具有显着差异的学习曲线，以至于它们看起来根本不是来自相同的分布（请参见图10）。

**进行高融合实验。**不要只是从论文中使用最好或最有趣的跑步中的结果。取而代之的是，启动新的，最终的实验 - 对于您打算比较的所有方法（如果您要与自己的基线实现进行比较），并且可以预先报告任何内容。这是为了强制执行预注册的薄弱形式：您使用调整阶段提出假设（hypotheses），并使用最终运行来得出结论。

**单独检查每个主张（claim）。**进行研究的另一个关键方面是进行消融分析（ablation analysis）。您提出的任何方法都可能有几个关键的设计决策，例如体系结构选择（architecture choices）或正则化技术，例如，这些决策可能会分别影响性能。您将在工作中提出的主张是这些设计决策共同提供了帮助，但这确实是一个伪装的几个主张：一个用于每个这样的设计元素。通过系统地评估什么会发生，如果你之前将它们换成替代的设计选择，或者完全删除它们，您可以弄清楚如何正确地将功劳归因于method所赋予的好处。这使您可以通过信心衡量每个主张，并增加你作品的整体优势。

## 别想太多

Deep RL是一个令人兴奋的，快速发展的领域，我们需要尽可能多的人来解决开放问题并在其中取得进展。希望您能在阅读本文后感到有些准备好成为其中的一部分！每当您准备就绪时，[请告诉我们](https://openai.com/careers/ai-product-counsel)。

## 后记

考虑阅读有关该领域研究人员或工程师成长的其他信息文章：

[Advice for Short-term Machine Learning Research Projects](https://rockt.github.io/2018/08/29/msc-advice), by Tim Rocktäschel, Jakob Foerster and Greg Farquhar.

[ML Engineering for AI Safety & Robustness: a Google Brain Engineer’s Guide to Entering the Field](https://80000hours.org/articles/ml-engineering-career-transition-guide/), by Catherine Olsson and 80,000 Hours.

## 核心论文

https://spinningup.readthedocs.io/zh_CN/latest/spinningup/keypapers.html

[1]: https://spinningup.readthedocs.io/zh_CN/latest/spinningup/spinningup.html#id2
