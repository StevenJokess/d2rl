{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MtXQpdrmnn3M"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义状态转移概率矩阵P\n",
        "P = [\n",
        "    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],\n",
        "    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],\n",
        "    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "]\n",
        "\n",
        "P = np.array(P)\n",
        "\n",
        "rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数\n",
        "gamma = 0.5  # 定义折扣因子"
      ],
      "metadata": {
        "id": "EojDnGf9oCnA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报\n",
        "\n",
        "from copy import deepcopy\n",
        "def compute_return(start_index, chain, gamma):\n",
        "    G = 0\n",
        "    ret = []\n",
        "    for i in reversed(range(start_index, len(chain))): ## 运算的方向要倒着来\n",
        "        g_ = deepcopy(G)\n",
        "        G = gamma * G + rewards[chain[i] - 1]       ## 从后往前依次运算求出结果\n",
        "        ret.append([G, gamma, g_, rewards[chain[i] - 1], chain[i]]) ## 保存\n",
        "    '''\n",
        "    逆向，先算后面的，\n",
        "     0.0 = 0.5 *  0     +   0         R_6\n",
        "    -2.0 = 0.5 *  0     +  -2         R_6->R_3\n",
        "    -3.0 = 0.5 * -2.0   +  -2         R_3->R_2\n",
        "    -2.5 = 0.5 * -3.0   +  -1         R_2->R_1\n",
        "   ret=    [[ 0.0, 0.5,  0,    0, 6],\n",
        "            [-2.0, 0.5,  0.0, -2, 3],\n",
        "            [-3.0, 0.5, -2.0, -2, 2],\n",
        "            [-2.5, 0.5, -3.0, -1, 1]]\n",
        "    '''\n",
        "    return G"
      ],
      "metadata": {
        "id": "cHVFjXGfoD6s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 一个状态序列,s1-s2-s3-s6\n",
        "chain = [1, 2, 3, 6]\n",
        "start_index = 0\n",
        "G = compute_return(start_index, chain, gamma)\n",
        "print(\"根据本序列计算得到回报为：%s。\" % G)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_iDmvyKoPtw",
        "outputId": "02853dde-db3a-4702-fd7b-64eba1ff097f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "根据本序列计算得到回报为：-2.5。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute(P, rewards, gamma, states_num):\n",
        "    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''\n",
        "    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式\n",
        "    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),\n",
        "                   rewards) # 对应求解析解的公式，V=(1-γP)^(-1)r\n",
        "    return value\n",
        "\n",
        "\n",
        "V = compute(P, rewards, gamma, 6)\n",
        "print(\"MRP中每个状态价值分别为\\n\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABRCf_Hto2WT",
        "outputId": "393c55c3-71a1-4fe5-d0e6-f8b9ee159a9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MRP中每个状态价值分别为\n",
            " [[-2.01950168]\n",
            " [-2.21451846]\n",
            " [ 1.16142785]\n",
            " [10.53809283]\n",
            " [ 3.58728554]\n",
            " [ 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "S = [\"s1\", \"s2\", \"s3\", \"s4\", \"s5\"]  # 状态集合\n",
        "A = [\"保持s1\", \"前往s1\", \"前往s2\", \"前往s3\", \"前往s4\", \"前往s5\", \"概率前往\"]  # 动作集合\n",
        "# 状态转移函数\n",
        "P = {\n",
        "    \"s1-保持s1-s1\": 1.0,\n",
        "    \"s1-前往s2-s2\": 1.0,\n",
        "    \"s2-前往s1-s1\": 1.0,\n",
        "    \"s2-前往s3-s3\": 1.0,\n",
        "    \"s3-前往s4-s4\": 1.0,\n",
        "    \"s3-前往s5-s5\": 1.0,\n",
        "    \"s4-前往s5-s5\": 1.0,\n",
        "    \"s4-概率前往-s2\": 0.2,\n",
        "    \"s4-概率前往-s3\": 0.4,\n",
        "    \"s4-概率前往-s4\": 0.4,\n",
        "}\n",
        "# 奖励函数\n",
        "R = {\n",
        "    \"s1-保持s1\": -1,\n",
        "    \"s1-前往s2\": 0,\n",
        "    \"s2-前往s1\": -1,\n",
        "    \"s2-前往s3\": -2,\n",
        "    \"s3-前往s4\": -2,\n",
        "    \"s3-前往s5\": 0,\n",
        "    \"s4-前往s5\": 10,\n",
        "    \"s4-概率前往\": 1,\n",
        "}\n",
        "gamma = 0.5  # 折扣因子\n",
        "MDP = (S, A, P, R, gamma)\n",
        "\n",
        "# 策略1,随机策略\n",
        "Pi_1 = {\n",
        "    \"s1-保持s1\": 0.5,\n",
        "    \"s1-前往s2\": 0.5,\n",
        "    \"s2-前往s1\": 0.5,\n",
        "    \"s2-前往s3\": 0.5,\n",
        "    \"s3-前往s4\": 0.5,\n",
        "    \"s3-前往s5\": 0.5,\n",
        "    \"s4-前往s5\": 0.5,\n",
        "    \"s4-概率前往\": 0.5,\n",
        "}\n",
        "# 策略2\n",
        "Pi_2 = {\n",
        "    \"s1-保持s1\": 0.6,\n",
        "    \"s1-前往s2\": 0.4,\n",
        "    \"s2-前往s1\": 0.3,\n",
        "    \"s2-前往s3\": 0.7,\n",
        "    \"s3-前往s4\": 0.5,\n",
        "    \"s3-前往s5\": 0.5,\n",
        "    \"s4-前往s5\": 0.1,\n",
        "    \"s4-概率前往\": 0.9,\n",
        "}\n",
        "\n",
        "\n",
        "# 把输入的两个字符串通过“-”连接,便于使用上述定义的P、R变量\n",
        "def join(str1, str2):\n",
        "    return str1 + '-' + str2"
      ],
      "metadata": {
        "id": "eFK1Xf_yo4Sf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.5\n",
        "# 转化后的MRP的状态转移矩阵\n",
        "P_from_mdp_to_mrp = [\n",
        "    [0.5, 0.5, 0.0, 0.0, 0.0],\n",
        "    [0.5, 0.0, 0.5, 0.0, 0.0],\n",
        "    [0.0, 0.0, 0.0, 0.5, 0.5],\n",
        "    [0.0, 0.1, 0.2, 0.2, 0.5],\n",
        "    [0.0, 0.0, 0.0, 0.0, 1.0],\n",
        "]\n",
        "\n",
        "'''\n",
        "P(1|1) = Pi_1[\"s1-保持s1\"] * P[\"s1-保持s1-s1\"] = 1 * 0.5 = 0.5\n",
        "P(2|1) = Pi_1[\"s1-前往s2\"] * P[\"s1-前往s2-s2\"] = 1 * 0.5 = 0.5\n",
        "P(1|2) = Pi_1[\"s2-前往s1\"] * P[\"s2-前往s1-s1\"] = 1 * 0.5 = 0.5\n",
        "P(3|2) = Pi_1[\"s2-前往s3\"] * P[\"s2-前往s3-s3\"] = 1 * 0.5 = 0.5\n",
        "P(4|3) = Pi_1[\"s3-前往s4\"] * P[\"s3-前往s4-s4\"] = 1 * 0.5 = 0.5\n",
        "P(5|3) = Pi_1[\"s3-前往s5\"] * P[\"s3-前往s5-s5\"] = 1 * 0.5 = 0.5\n",
        "P(2|4) = Pi_1[\"s4-概率前往\"] * P[\"s4-概率前往-s2\"] = 0.5 * 0.2 = 0.1\n",
        "P(3|4) = Pi_1[\"s4-概率前往\"] * P[\"s4-概率前往-s3\"] = 0.5 * 0.4 = 0.2\n",
        "P(4|4) = Pi_1[\"s4-概率前往\"] * P[\"s4-概率前往-s4\"] = 0.5 * 0.4 = 0.2\n",
        "P(5|4) = Pi_1[\"s4-前往s5\"] * P[\"s4-前往s5-s5\"] = 0.5 * 1 = 0.5\n",
        "P(5|5) = 1\n",
        "'''\n",
        "\n",
        "P_from_mdp_to_mrp = np.array(P_from_mdp_to_mrp)\n",
        "\n",
        "\n",
        "R_from_mdp_to_mrp = [-0.5, -1.5, -1.0, 5.5, 0]\n",
        "'''\n",
        "r(s1) = Pi_1[\"s1-保持s1\"] * R[\"s1-保持s1\"] + Pi_1[\"s1-前往s2\"] * R[\"s1-前往s2\"] = -1 * 0.5 + 0 * 0.5 = -0.5\n",
        "r(s2) = Pi_1[\"s2-前往s1\"] * R[\"s2-前往s1\"] + Pi_1[\"s2-前往s3\"] * R[\"s2-前往s3\"] = -1 * 0.5 + -2 * 0.5 = -1.5\n",
        "r(s3) = Pi_1[\"s3-前往s4\"] * R[\"s3-前往s4\"] + Pi_1[\"s3-前往s5\"] * R[\"s3-前往s5\"] = -2 * 0.5 + 0 * 0.5 = -1\n",
        "r(s4) = Pi_1[\"s4-前往s5\"] * R[\"s4-前往s5\"] + Pi_1[\"s4-概率前往\"] * R[\"s4-概率前往\"] = 10 * 0.5 + 1 * 0.5 = 5.5\n",
        "r(s5) = 0\n",
        "也就求出了R_from_mdp_to_mrp 奖励函数的\n",
        "'''\n",
        "\n",
        "V = compute(P_from_mdp_to_mrp, R_from_mdp_to_mrp, gamma, 5)\n",
        "print(\"MDP中每个状态价值分别为\\n\", V)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2dJrstDqaDS",
        "outputId": "a61d6a60-7dd7-49d8-94dc-3bd3a8cb6b95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MDP中每个状态价值分别为\n",
            " [[-1.22555411]\n",
            " [-1.67666232]\n",
            " [ 0.51890482]\n",
            " [ 6.0756193 ]\n",
            " [ 0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(MDP, Pi, timestep_max, number):\n",
        "    ''' 采样函数,策略Pi,限制最长时间步timestep_max,总共采样序列数number '''\n",
        "    S, A, P, R, gamma = MDP\n",
        "    episodes = []\n",
        "    for _ in range(number):\n",
        "        episode = []\n",
        "        timestep = 0\n",
        "        s = S[np.random.randint(4)]  # 随机选择一个除s5以外的状态s作为起点\n",
        "        # 当前状态为终止状态或者时间步太长时,一次采样结束\n",
        "        while s != \"s5\" and timestep <= timestep_max:\n",
        "            timestep += 1\n",
        "            rand, temp = np.random.rand(), 0   ## 初始化概率，以及动作概率是0\n",
        "            # 在状态s下根据策略选择动作\n",
        "            for a_opt in A:       ## 遍历每个动作\n",
        "                temp += Pi.get(join(s, a_opt), 0)    ## 拿到状态+动作的名称，然后从策略Pi_1内拿到对应的动作概率\n",
        "                if temp > rand:   ## 该状态和动作对应的概率，满足概率条件，则执行动作action\n",
        "                    a = a_opt        ## 执行动作的action，a是动作名称\n",
        "                    r = R.get(join(s, a), 0)    ## 拿到状态+动作的名称，然后从状态+动作的奖励 R 内拿到对应的动作奖励\n",
        "                    break                       ## 已经执行了动作，退出动作的循环\n",
        "            rand, temp = np.random.rand(), 0    ## 初始化概率，以及状态的概率\n",
        "            # 根据状态转移概率得到下一个状态s_next\n",
        "            for s_opt in S:   ## 遍历状态列表\n",
        "                temp += P.get(join(join(s, a), s_opt), 0)  ## 组合了状态+动作+转移的状态名称，然后从状态+动作+转移状态的概率 P 内拿到对应的转移概率\n",
        "                if temp > rand:      ##    满足概率条件，则执行状态转移\n",
        "                    s_next = s_opt   ##    转移到的状态是 s_next\n",
        "                    break            ##    已经转移了状态，退出状态的循环\n",
        "            ## s是当前的状态，a是执行的动作，r是执行动作的奖励，s_next是执行动作以后转移到的状态\n",
        "            episode.append((s, a, r, s_next))  # 把（s,a,r,s_next）元组放入序列中\n",
        "            s = s_next  # s_next变成当前状态,开始接下来的循环\n",
        "        episodes.append(episode)\n",
        "    return episodes"
      ],
      "metadata": {
        "id": "Qi4H4Mcmr2GB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def occupancy(episodes, s, a, timestep_max, gamma):\n",
        "    ''' 计算状态动作对（s,a）出现的频率,以此来估算策略的占用度量 '''\n",
        "    rho = 0\n",
        "    total_times = np.zeros(timestep_max)  # 记录每个时间步t各被经历过几次\n",
        "    occur_times = np.zeros(timestep_max)  # 记录(s_t,a_t)=(s,a)的次数\n",
        "    for episode in episodes:              ## 遍历每个采样的内容\n",
        "        for i in range(len(episode)):     ## 遍历每个采样内部的状态、动作的内容\n",
        "            (s_opt, a_opt, r, s_next) = episode[i]   ## 拿到每个采样内的 状态、动作、动作的奖励、下一个状态\n",
        "            total_times[i] += 1                      ## 统计序列所在index的次数\n",
        "            if s == s_opt and a == a_opt:            ## 状态、动作和给定的相同\n",
        "                occur_times[i] += 1                  ## 也就是（状态，动作）对的次数+1\n",
        "    for i in reversed(range(timestep_max)):       ## 逆序算占用度量\n",
        "        if total_times[i]:                        ## 序列所在的index有值\n",
        "            rho += gamma**i * occur_times[i] / total_times[i]        ## 按照公式运算\n",
        "    return (1 - gamma) * rho"
      ],
      "metadata": {
        "id": "cTFqerTbsHfL"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.5\n",
        "timestep_max = 1000\n",
        "\n",
        "episodes_1 = sample(MDP, Pi_1, timestep_max, 1000)\n",
        "episodes_2 = sample(MDP, Pi_2, timestep_max, 1000)\n",
        "rho_1 = occupancy(episodes_1, \"s4\", \"概率前往\", timestep_max, gamma)\n",
        "rho_2 = occupancy(episodes_2, \"s4\", \"概率前往\", timestep_max, gamma)\n",
        "print(rho_1, rho_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmXK-tfsr3-q",
        "outputId": "92d88fff-1b3c-4686-dc91-b2b2462c0d47"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.10933789486751247 0.22662902209217917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1]: https://zhuanlan.zhihu.com/p/655615836"
      ],
      "metadata": {
        "id": "s-hd-81Psnx5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1]: https://zhuanlan.zhihu.com/p/655615836"
      ],
      "metadata": {
        "id": "k8GfBySkqmgx"
      }
    }
  ]
}