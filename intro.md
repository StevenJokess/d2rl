

<!--
 * @version:
 * @Author:  StevenJokes https://github.com/StevenJokes
 * @Date: 2021-02-04 20:30:32
 * @LastEditors:  StevenJokes https://github.com/StevenJokes
 * @LastEditTime: 2023-02-22 15:00:37
 * @Description:
 * @TODO::
 * @Reference:
-->
# 强化学习

2018年的围棋AlphaGO战胜李世石使RL（强化学习）大为闻名，证明了这种强化模型能有超人类的表现，故我们才关注它。对于下围棋这一任务，即使是专家也很难给出“正确”的动作，二是获取大量数据的成本往往比较高。对于下棋强化学习我们很难知道每一步的“正确”动作，但是其最后的结果（即赢输）却很容易判断。因此，如果可以通过大量的模拟数据，通过最后的结果（奖励）来倒推每一步棋的好坏，从而学习出“最佳”的下棋策略，这就是强化学习。

这种在复杂、不确定的环境中交互时不断做出选择（sequential decision making）边学习的行为，我们其实早就在进行了。当一个婴儿玩耍，挥动手臂或环顾四周时，他没有明确的老师，但他确实通过直接的感觉与环境联系。他可以通过这种联系获得大量关于因果关系、动作的结果以及如何实现目标的信息。 在我们的生活中，这种交互无疑是环境和自身知识的主要来源。无论我们是学习驾驶汽车还是进行交谈，我们都敏锐地意识到我们的环境如何响应我们的行为，并且我们试图通过我们的行为来影响所发生的事情。

决策 与 预测 的区别：

- 决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。
- 预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变。

## RL概念

强化学习（Reinforcement Learning，简称RL），也叫增强学习，是指一类智能体从与环境交互中不断学习的问题以及解决这类问题的方法。强化学习问题可以描述为一个智能体（Agent）从与环境（Environment）的交互中不断学习以取得最大回报（Reward）。强化是增加行为的意思，即当某个行为在从环境中获得正回报后就会倾向去增加这种行为。[26] 历史更多见[25]

![强化学习示意](/img/rl.png)

## 监督学习、非监督学习、强化学习之间的区别[5]

|      | 监督学习         | 非监督学习     | 强化学习|
| ---- | ---------------- | ------------- | ------ |
| 标签 | supervisor提供正确且严格的标签 | 没有标签       | 没有标签、supervisor，时间序列数据（sequential data）是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。
| 输入 | 独立同分布(i.i.d.), 为了消除数据之间的相关性。    | 独立同分布(i.i.d.)     | 归一化的占用度量（occupancy measure）用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。|
| 动作 | exploration     | exploration | Trial-and-error，即存在exploration和exploitation的平衡 (不一定按照已知的最优做法去做)|
| 反馈 | 有反馈      | 无反馈     | 无即时反馈，即reward常有延迟, 用结果的总reward用来判断这个行为是好是坏，不会告诉什么是正确的action|
| 驱动 | 任务驱动      | 数据驱动     | 有目标,从错误中学习[24]|
| 模型 | 建立新输入对应原标签的预测模型     | 自学习映射关系，以此为模型     | 学习到从环境状态到行为的映射即决策（决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。），使得智能体选择的该行为能够获得环境最大的回报reward|
| 公式 | $\text {最优预测模型} =\arg \min _{\text {模型}} \mathbb{E}_{(\text {特征, 标签}) \sim \text {数据分布}}[\text {损失函数 (标签, 模型（特征）)]}$     | TODO:| $\text {最优策略} =\arg \max _{\text {策略}} \mathbb{E}_{(\text {状态, 动作}) \sim \text {策略的占用度量}}[\text {奖励函数 (状态, 动作)]}$|
| 任务 | 预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变。预测任务总是单轮的独立任务。| TODO: |决策往往会带来“后果”，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。决策任务往往涉及多轮交互，即序贯决策[31] |
| 上限（upper bound） | 传统的机器学习算法依赖人工标注好的数据，从中训练好的模型的性能上限是产生数据的模型（人类）的上限      | 可超人类     | 不受人类先验知识所限，表现可超人类 |
| 适用情况 | 任务(分类/回归);若强化信号r与Agent产生的动作A有明确的函数形式描述，可得到梯度信息r/A则可直接可以使用监督学习算法。 | 数据驱动聚类[23]   | 因为强化信号r与Agent产生的动作A没有明确的函数形式描述，所以Agent在可能动作空间中进行搜索并发现正确的动作。[22] |

## 基本概念

### 环境 (Environment)

- 环境 (Environment):强化学习系统中除智能体以外的所有事物，它是智能体交互的对象。环境可以是已知的，也可以是未知的，因此可以对环境建模，也可以不对环境建模。[^3]

#### 环境分类

##### 按智能体和环境的交互方式

- 离散时间环境（discrete time environment）：如果智能体和环境的交互是分步进行的，那么就是离散时间环境。
- 连续时间环境（continuous time environment）：如果智能体和环境的交互是在连续的时间中进行的，那么就是连续时间环境。[19]

##### 按照环境是否具有随机性

- 确定性环境任务（deterministic environment）：环境不具有随机性，如对于机器人走固定的某个迷宫的问题，只要机器人确定了移动方案，那么结果就总是一成不变的。这样的环境就是确定性的。
- 非确定性环境（stochastic environment）：环境具有随机性例，如果迷宫会时刻随机变化，那么机器人面对的环境就是非确定性的。

#### 第一步:感知——观察 (Observation)

状态 (State): 一个关于这个世界状态的完整描述。这个世界除了状态以外没有别的信息。状态更新$S_{t}=f\left(H_{t}\right)$。完整的环境信息，包括所有可见和不可见的变量和参数。以汽车举例，状态是:引擎是否开启，车辆是否正在行驶等。状态默认指的就是环境状态。
  - 环境状态 $S_{t}^{e}$ (Environment State)：指智能体外部的状态，包括智能体所处的环境的所有特征，如周围的物体、声音、光线、温度、风向等等。环境的状态是智能体的感知输入，即智能体通过感知获取到的外部信息。环境状态的更新$S_{t}^{e}=f^{e}\left(H_{t}\right)$
  - 智能体的状态 $S_{t}^{a}$ (Agent State)： 指智能体当前的内部状态，包括其知识、信念、意图、规划等等，以及可能的外部状态，比如其位置、速度、姿态等等。智能体状态是智能体自身的一个描述，它反映了智能体的**内部和外部**环境的特征。智能体状态的更新$S_{t}^{a}=f^{a}\left(H_{t}\right)$

> 特例——马尔科夫状态(Markov State）： 当且仅当：$P\left[S_{t+1} \mid S_{t}\right]=P\left[S_{t+1} \mid S_{1}, \ldots, S_{t}\right]，$$S_{t}$ 是马尔科夫状态。到这一步可以把历史丟掉了, 只要每一步的状态即可。历史 (History):是在截止某刻之前的所有时刻的一序列的观察、行动、奖励。  $H_{t}=A_{1}, O_{1}, R_{1}, \ldots A_{t}, O_{t}, R_{t_{0}}$。其长度取决于任务环境和任务要求。

观察 (Observation):$O_{t}, t$ 时刻对环境的观察。是对于一个状态的部分描述，只包括智能体可以观测到的环境信息，可能漏掉一些信息。以汽车举例，观察则是看到汽车在行驶，听到汽车的发动机声音等。
      - 当能观测当前所有环境，即观察即是状态，叫做全观测环境 (Full observability) :$ O_{t}=S_{t}^{a}=S_{t}^{e}$.
      - 部分观测环境 (Partial observability): $S_{t}^{e}$。

> 有时候用符号 s 代表状态，有些地方也会写作观测符号 o。 尤其是，当智能体在决定采取什么动作的时候，符号上的表示按理动作是基于状态的， 但实际上，动作是基于观测的，因为智能体并不能知道状态（只能通过观测了解状态）。[18]

### 智能体 (Agent)

智能体 (Agent):强化学习系统中的行动者和学习者，它可以做出决策和接受奖励信号，我们并不需要对智能体本身进行建模，只需了解它在不同环境下可以做出的动作，并接受奖励信号。

#### 智能体的组成

通常将智能体分为两个部分：学习者和决策者。by ChatGPT

1. 学习者是指智能体的学习部分，其任务是根据环境的反馈信息和自身的经验，*更新*自己的价值函数或策略，以最大化累积奖励。学习者可以采用不同的算法来实现策略或价值函数的更新，例如Q-learning、SARSA、Actor-Critic等。
1. 决策者是指智能体的决策部分，其任务是根据当前状态和学习者更新的价值函数或策略，选择一个最优的动作(decision)，以对环境产生影响并获取奖励。决策者可以采用不同的方法来选择动作，例如ε-greedy、Softmax等。

学习者和决策者在智能体的训练和使用过程中起着不同的作用。学习者通过不断地学习和更新，提高智能体的性能和适应性。而决策者则负责在实际运行时根据当前状态和策略选择最优的动作，对环境进行影响和控制。

需要注意的是，在某些情况下，学习者和决策者可能合并成一个整体，称为直接策略搜索（Direct Policy Search）或基于模型的强化学习（Model-Based Reinforcement Learning）。在这种情况下，智能体通过直接搜索或建模策略或环境，实现学习和决策的统一。

#### 第二步:决策——行动(Action):

行动(Action):$A_{t}, t$ 时刻采取的行动。
  - 动作空间(Action Spaces)：动作空间是所有给定环境中智能体可以执行的所有可能的有效动作的集合。
    - 离散动作空间（discrete action space）: 有些环境，比如说 Atari 游戏和围棋，属于 离散动作空间，这种情况下智能体只能采取有限的动作。
    - 连续动作空间（continuous action space）: 其他的一些环境，比如智能体在物理世界中控制机器人，属于 连续动作空间。在连续动作空间中，动作是实数向量。

这种区别对于深度强化学习来说，影响深远。有些种类的算法只能一种情况下直接使用，而在另一种情况下则必须进行大量修改。

在训练智能体的过程中，很多时候我们也是通过正在学习的智能体与环境交互来得到数据的。所以如果在训练过程中，智能体不能保持稳定，就会使我们采集到的数据非常糟糕。我们通过数据来训练智能体，如果数据有问题，整个训练过程就会失败。所以在强化学习里面一个非常重要的问题就是，怎么让智能体的动作一直稳定地提升。见后

#### 第三步:反馈——回报(Reward)

回报 (Reward):$r_{t}$是 $t$ 时刻的回报。它由当前状态、已经执行的动作和下一步的状态这个三个参数的回报函数$R$决定。 $r_t = R(s_t, a_t, s_{t+1})$ 有时候这个公式会被改成只依赖当前的状态 $r_t = R(s_t)$，或者状态动作对 $r_t = R(s_t,a_t)$。奖励与回报的区别：回报通常包括有益的和有害的回报。在常用语境, 有益智能体的是奖励, 有害的是惩罚。

#### 第四步:时间步+1，环境和智能体的状态更新，循环前3步，不断地试错探索去获取较大的回报

##### 相关术语:

- 环境的转态转换指的是环境从某一时间 $t$ 的状态 $s_t$ 到 另一时间 $t+1$ 的状态 $s_{t+1}$ 。它是由环境的自然法则确定的，并且只依赖于最近的动作 $a_t$ 。它们可以是确定性的：$s_{t+1} = f(s_t, a_t)$ 也可以是随机的：$s_{t+1} \sim P(\cdot|s_t, a_t)$
- 智能体的学习者的参数更新方式，包括回合更新和单步更新。
  - 回合更新: 在一个回合(episode)后才进行参数的更新。如:原始版Policy Gradients ， Monte-Carlo Learning
  - 单步更新: 不需要等回合结束，可以综合利用现有的信息和现有的估计进行更新学习。如:Temporal-Difference, Q-learning ，Sarsa ， 进阶版Policy Gradients

- 行动轨迹 $\tau$(Trajectory) 指的是$t$时刻前一系列观测和动作的序列，$\tau = (s_0, a_0, s_1, a_1, ..., s_t, a_t).$ 第一个状态 $s_0$ 是从 **开始状态分布** 中随机采样的，有时候表示为 $\rho_0$:$s_0 \sim \rho_0(\cdot).$
- 历史 $H$ (History)是$t$时刻前一系列观测、动作、奖励的序列：$H_t = (o_0, a_0, r_0, o_1, a_1, r_1, ..., o_t, a_t, r_t).$

##### 不断地试错探索

不断地以试错探索（trial-and-error exploration）的方式选择动作去获取回报的过程。

- 探索（exploration）：指尝试一些新的动作，这些新的动作有可能会使我们得到更多的奖励，也有可能使我们“一无所有”
- 利用（exploitation）：利用指采取已知的可以获得最多奖励的动作，重复执行这个动作，因为我们知道这样做可以获得一定的奖励。

事实上，“探索”和“利用”两者是矛盾的，因为尝试次数有限，加强了一方则会自然削弱另一方，这就是强化学习所面临的探索-利用窘境（Exploration-Exploitation dilemma）[29]

所以我们需要在探索和利用之间进行权衡（balance）：如果智能体过于偏向于利用已知的最优策略，那么它可能会错过更好的策略，从而无法获得更高的累计奖励。这种现象被称为“局部最优”（local optimum）。这种现象被称为“局部最优”（local optimum）。另一方面，如果智能体过于偏向于探索新的动作，那么它可能会花费大量的时间和资源在次优的策略上，从而导致学习过程的效率低下。

##### 实际回报公式

实际回报公式(formulations of return): 奖励函数 $R$ 非常重要。它由当前状态、已经执行的动作和下一步的状态共同决定。

- 即时回报，即当前回报$r_0$。
- 远期回报，又叫延迟回报，是指在一次行动后在一定时间后（或者是一系列动作后）才获得回报。这种回报可能是一次, 也可能是多次。单个时间步的奖励$r_t = R(s_t, a_t, s_{t+1})$。

在实际问题中，智能体可能会面临如下的困境：选择一些立即的回报可能会影响未来获得的回报，而放弃即时回报可能会带来更多的长期回报。所以通常存在着即时回报和远期回报之间的权衡。使用折扣因子能去平衡即时回报和远期回报，具体见下:

不同的回报公式可以用来计算在不同任务环境中的回报值:

- 累积回报是指在一次行动轨迹中所有回报值的总和。公式是$G(\tau) = \sum_{t=0}^T r_t$
- 折扣累积回报是指在一次行动轨迹中所有回报值按时间折扣的总和。
   - 折扣因子（discount factor）是一个用来平衡未来回报的价值衰减因子，表示在未来的每个时刻，奖励会以一定的比例进行衰减。使用时间折扣是为了使强化学习智能体更好地处理长期决策问题，同时能够适应不同的环境和任务。
   - 折扣因子通常表示为 $\gamma$，其中 $0 \leq \gamma \leq 1$。由于 $\gamma \leq 1$，因未来的奖励价值会以指数级别的速度进行衰减，这表达出了我们更加关注立即可获得的奖励，而不怎么关注远期可能获得的奖励。$\gamma$ 越接近1，越接近原累计回报公式，越远视;越接近0,越短视.
   - 从当前时间步$t$开始到未来有限视野T的所有时间步的累积奖励可以表示为：$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{T} \gamma^k R_{t+k+1}$$
   - 而无限视野(infinite-horizon) 是$$G_t = r_{t+1} + \gamma r_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$
   - 其中，$r_t$ 表示在时间步 $t$ 时获得的奖励，$\gamma$ 是时间折扣因子，$G_t$ 表示从时间步 $t$ 开始的累积奖励。[20]

#### 优化问题(Optimization  problem)

最大化回报是强化学习系统的学习目标。强化学习的优化问题是指为了最大化智能体的累积回报而进行的。

##### 最大化回报

我们用期望回报来表示预测的未来的回报，那么就变成最大化期望回报问题。

强化学习中的核心优化问题可以表示为最大化未来累计（折扣）期望回报。

#### 如何让智能体的动作得到的回报一直稳定地提升

在强化学习中，让智能体的动作得到的回报一直稳定地提升是一个重要的目标。下面介绍几种常用的方法：(ChatGPT)

1. 随机初始化：在训练智能体之前，我们需要对其进行随机初始化。这样可以使智能体开始时对环境一无所知，从而避免其陷入局部最优解。
1. 逐步提高动作选择的随机性：在智能体学习过程中，逐步增加动作选择的随机性可以帮助智能体探索更多的状态和动作，从而提高其决策能力。例如，在训练过程中可以使用逐渐减小的 $\epsilon$ 值，以便智能体更多地进行探索。
1. 调整学习率：学习率是指模型在更新参数时所使用的步长。如果学习率设置过大，可能会导致模型在更新过程中出现不稳定的情况。因此，调整学习率可以帮助模型稳定地更新参数。
1. 使用基于价值函数的方法：基于价值函数的方法可以帮助智能体更好地理解当前状态和动作的价值，从而提高其决策能力。例如，使用 Q-learning 算法可以帮助智能体学习到每个状态下采取不同动作的最大价值，从而稳定地提升其动作选择能力。
1. 使用经验回放：经验回放是一种常用的训练技术，它可以帮助智能体学习到更加广泛的状态和动作。具体来说，经验回放是指将智能体在环境中的经验存储在一个回放缓冲区中，然后在训练过程中从中随机抽取一部分经验，用于更新智能体的模型参数。

综上所述，让智能体的动作一直稳定地提升需要结合多种方法，包括随机初始化、逐步提高动作选择的随机性、使用经验回放、调整学习率和使用基于价值函数的方法等。

### 任务分类

#### 按智能体数量

- 单智能体任务（single agent task）：单智能体任务中只有一个决策者，它能得到所有可以观察到的观测，并能感知全局的奖励值；
- 多智能体任务（multi-agent task）：多智能体任务中有多个决策者，它们*只能知道自己的观测*，感受到环境给它的奖励。当然，在有需要的情况下，多个智能体间可以交换信息。在多智能体任务中，不同智能体奖励函数的不同会导致它们有不同的学习目标（甚至是互相对抗的）。在下面没有特别说明的情况下，一般都是指单智能体任务。王者荣耀里5v5时，能互相知道彼此的观测。

#### 任务有无开始结束条件

- 回合制任务（episodic task）或阶段性任务：对于回合制任务，有明确的开始状态和结束状态。很多游戏都是回合制的，如围棋。
- 连续性任务（continuing task）：对于连续性任务，没有明确的开始结束条件。

### 预演（rollout）和试验（trial）

预演（rollout）是一种模拟智能体在当前策略下进行一系列动作的过程。预演的目的是为了评估当前策略的效果，以便对其进行改进。在预演中，智能体会在当前策略下选择一个动作并执行，然后根据环境的反馈信息（例如奖励信号）更新自己的状态和价值估计。然后，智能体会基于更新后的状态选择下一个动作并执行，重复这个过程直到达到预定的终止条件。预演通常被用于评估当前策略的表现，例如计算状态值或动作值函数。它也可以被用于生成训练数据，例如在蒙特卡罗树搜索中，预演可以用于生成候选动作序列，以便选择最优的动作。预演还可以被用于生成演示数据，例如在逆强化学习中，预演可以用于生成人类专家的行为轨迹，以便训练一个智能体来模仿人类行为。

试验（trial）是指对一个智能体在一个环境中执行一次完整的交互过程的过程。这个过程通常包括以下几个步骤：

1. 环境初始化：初始化智能体的状态和环境状态。
1. 智能体与环境交互：智能体根据当前状态采取一个动作，环境根据这个动作返回一个奖励和下一个状态。
1. 状态更新：智能体更新自己的状态估计，例如状态值或动作值函数。
1. 判断是否结束：如果满足某个停止条件，例如达到预定的步数或目标状态，则结束试验，否则继续执行步骤2。

试验(trial)是强化学习中的一个重要概念，因为它是学习的基本单位。通过执行多个试验，智能体可以逐步改善自己的策略和价值估计，以实现更好的性能。试验的次数越多，智能体的性能就越好，但同时也需要注意防止过拟合。

#### 智能体 (Agent)根据优化方法分类：

- 不基于模型的智能体，即免模型学习(Model-Free)放弃了模型学习，在效率上不如前者，但是这种方式更加容易实现，也容易在真实场景下调整到很好的状态。所以免模型学习方法更受欢迎，得到更加广泛的开发和测试。
  1. 基于价值函数的智能体(value-based agent)：该类智能体通过学习值函数(value function)（如状态值函数或动作值函数）来做出决策，即选择具有最大值的动作。值函数可以描述在某个状态或状态-动作对下，智能体能够获得多少期望奖励。如:使用表格学习的 Q-learning和Sarsa算法。此时我们训练的是一个主要完成任务的Actor。
  2. 基于策略的智能体(policy-based agent)：该类智能体尝试学习环境的动态模型，即预测从给定状态和动作转移到下一个状态的概率。然后，智能体可以使用学习到的模型来规划决策。我们训练的是不完成任务的一个Critic。Policy Gradient。
     1. 在线控制 或 同策学习（on-policy）是指直接对策略进行建模和优化的方法，其目标是找到一个能够最大化期望累积奖励的最优策略。边决策边学习，学习者同时也是决策者。包括Sarsa，Sarsa（λ）[14]。on-policy方法更加稳定但收敛速度较慢。一般只有一个策略(最常见的是ϵ−贪婪法)[^15]
     2. 离线控制 或 异策学习（off-policy）则是指在训练过程中使用一个不同于当前策略的策略进行采样和更新，也就是说，智能体在执行动作时可以采用任意策略生成的动作进行训练。常见的off-policy方法包括Q-learning，Deep-Q-Network，Deep Deterministic Policy Gradient (DDPG)等方法。通过之前的历史（可是自己的也可以是别人的）进行学习，学习者和决策者不需要相同。[^3]而off-policy方法则更容易出现不稳定性但收敛速度较快。包括Q Learning ， Deep Q Network。一般有两个策略，其中一个策略(最常见的是ϵ−贪婪法)用于选择新的动作，另一个策略(最常见的是贪婪法)用于更新价值函数。[^15]
  3. 基于执行者/评论者的智能体（actor-critic agent）：该类智能体结合了值函数和策略的思想。它包含一个执行者（actor）网络和一个评论者（critic）网络，执行者网络用于生成动作，而评论者网络用于估计值函数。[^8]!在Actor-Critic 基础上扩展的 DDPN (Deep Deterministic Policy Gradient)、A3C (Asynchronous Advantage Actor-Critic)、DPPO (Distributed Proximal Policy Optimization)。[17][](img\A+C.png)
- 基于模型的智能体 （model-based agent）：该类智能体尝试学习环境的动态模型，即预测从给定状态和动作转移到下一个状态的概率。然后，智能体可以使用学习到的环境模型来提前规划决策。（model + policy and/or + value function）但缺点是如果模型跟真实世界不一致，那么在实际使用场景下会表现的不好。预测从给定状态和动作转移到下一个状态的概率: $$\mathbf{P}_{s}^{a}=P\left[S_{t+1}=s^{\prime} \mid S_{t}=s, A_{t}=a\right], \mathbf{R}$$ 环境模型一般可以从数学上抽象为状态转移函数 (transition function) 和奖励函数 (reward function)。 在学习得到环境模型后，理想情况下，智能体可以不与真实环境进行交互，而只在模拟的环境中，通过RL算法最大化累积折扣奖励，得到最优策略。[28]
- 模仿学习智能体（imitation learning agent）：该类智能体不是直接学习环境奖励，而是尝试模仿人类或其他专家的决策。模仿学习可以提供一种简单而有效的方式，使智能体学习到正确的行为。

#### 基于价值函数

##### 价值(Value)

价值(Value): 价值(Value)是对未来折扣累积回报的预测值

$$V_{\pi}(s)=E_{\pi}\left[R_{t+1}+\gamma R_{t+2}+\gamma^{2} R_{t+3}+\ldots \mid S_{t}=s\right]$$

预测下一个即时的奖励: $\mathbf{R}_{}=E\left[R_{t+1} \mid S_{t}=s, A_{t}=a\right]_{\circ}$

##### 价值函数(Value function)

价值函数是一个将状态或状态-行动对映射到预期未来的**累计折扣回报**的函数。价值函数的值是对未来累计折扣回报的预测，我们用它来评估状态的好坏。

价值函数通常有两种形式：

- 状态价值函数（state value function）表示在某个状态下的价值函数。$$V_{\pi}(s) = \mathbb{E}\pi \left[ G_t | S_t = s \right]$$ 其中，$V_{\pi}$ 是在状态 $s$ 下，根据策略 $\pi$ 执行后的预期累积回报，$G_t$ 是从时刻 $t$ 开始的累积回报，$\mathbb{E}\pi$ 是在策略 $\pi$ 下的期望。
- 动作价值函数（action value function）表示在某个状态-行动对下的价值函数，又叫Q函数。$$Q_{\pi}(s, a) = \mathbb{E}\pi \left[ G_t | S_t = s, A_t = a \right]$$ 其中，$Q_{\pi}$ 是在状态 $s$ 下执行动作 $a$，根据策略 $\pi$ 执行后的预期累积回报。$G_t$ 是从时刻 $t$ 开始的累积回报，$\mathbb{E}\pi$ 是在策略 $\pi$ 下的期望。注意，动作值函数 $Q^\pi(s, a)$ 是状态 $s$ 和动作 $a$ 的函数。可以通过 Q 函数得到进入某个状态要采取的最优动作。

#### 基于策略(Policy)

##### 策略(Policy)

策略(Policy)：策略$\pi(a \mid s)$是指智能体在每个状态下应该采取的行动，即是 State 到 Action 的一个映射。强化学习通过学习来改进策略来最大化总奖励。

- 确定性策略（deterministic policy）就是智能体直接采取最有可能的动作，即$a^*=\underset{a}{\arg \max } \pi(a \mid s)$。
- 随机性策略（stochastic policy）$\pi(a \mid s)=P\left[A_{t}=a \mid S_{t}=s\right]$）通过采样该概率分布就可以得到智能体将采取的动作。

以雅达利游戏为例子，策略函数的输入就是游戏的一帧，它的输出决定智能体向左移动或者向右移动。

通常情况下，强化学习一般使用随机性策略，随机性策略有很多优点。比如，在学习时可以通过引入一定的随机性来更好地探索环境； 随机性策略的动作具有多样性，这一点在多个智能体博弈时非常重要。采用确定性策略的智能体总是对同样的状态采取相同的动作，这会导致它的策略很容易被对手预测。[30]

- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。
- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此寻找最优策略对应着寻找最优占用度量。[31]

#### 选择动作的策略，以N-Armed Bandit (N = 10)问题为例：

见MAB.md


- 使用 $\epsilon$-贪心算法（$\epsilon$-greedy algorithm）。该算法在进行动作选择时，以 $1-\epsilon$ 的概率进行利用，以 $\epsilon$ 的概率进行探索，选择该非最优动作的概率为 $\epsilon$ /(总动作数-最有动作数)。通过调节 $\epsilon$ 的值，我们可以在探索和利用之间进行权衡。当 $\epsilon$ 值很小时，智能体更倾向于利用已知策略；当 $\epsilon$ 值很大时，智能体更倾向于探索新的动作。假设epsilon = 0.1，则有90%的概率选择最优估计动作，剩下的9个动作的选择概率*均*约为1%，缺点就在于次优动作与负收益或低收益动作的选择概率相同，这样对于总收益是不利的。[27]

##### UCB 上置信界算法



使用UCB，每次选择置信上限最优的动作，这样是可以提升次优动作和较高收益动作的选择率，但却*大大降低了最优动作的选择率*。此缺陷也决定了UCB的适用范围。


且看这两种策略，缺陷都在于不能够很好的优化动作的选择概率。那么对于动作选择，我们较为理想的情况是高收益的动作选择概率就要高，低收益或负收益的动作选择概率就要低。这是比较笼统的说法，可以描述的更加细致一点，用权重来替代动作选择概率，高收益的动作分配的权重就高，反之亦然。最简单的权重计算方法就是比重，各自动作统计上的平均收益与所有动作的总平均收益的比重，就可以视为权重。

- 根据Softmax,动作选择概率的一般表达式，可以写为：
- Gradient策略可以说是Softmax的进化后脱离出的新的策略


###### MDP

MDP（Markov Decision Process）是一种用于描述序贯决策问题的数学框架。在MDP中，决策是在一个由状态、动作、奖励和转移概率组成的环境中进行的。

在现实环境中，从一个状态转化到下一个状态s′的概率不仅与当前状态s和动作a有关，还与之前的状态有关。如果考虑这么多的状态，会导致环境转化模型非常复杂，复杂到难以建模。马尔可夫性简化

MDP的解决方案是描述MDP中每个状态的最佳操作的策略，称为最优策略。这种最优策略可以通过各种方法找到，比如动态规划、蒙特卡洛和时序差分。

Q-Learning

### 贝尔曼方程

表示当前状态的值函数可以通过下个状态的值函数来计算。贝尔曼方程因其提出者、美国国家科学院院士、动态规划创始人理查德·贝尔曼（RichardBellman，1920～1984）而得名，也叫作“动态规划方程”。如果给定策略𝜋(𝑎|𝑠)，状态转移概率𝑝(𝑠′|𝑠,𝑎)和奖励𝑟(𝑠,𝑎,𝑠′)，我们就可以通过迭代的方式来计算𝑉𝜋(𝑠)。由于存在折扣率，迭代一定步数后，每个状态的值函数就会固定不变。

### 例子

https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1?id=_112-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0%e7%9a%84%e4%be%8b%e5%ad%90

从DP、MC、TD
基础算法类似于Sarsa、Q-learning、REINFORCE
有系统介绍TRPO、PPO、DDPG、SAC等经典DRL算法
印像深刻的是TRPO算法不少网上材料对论文中提到的共轭梯度、线性搜索等都是一笔带过（感觉对优化算法有了解的人来说不太友好），而且论文附录中Computing the Fisher-Vector Product部分自己到现在都没还能完全理解如何高效计算Fisher信息矩阵和某一向量的乘积。[21]


## 优势函数（Advantage Functions）

强化学习中，有些时候我们不需要描述一个行动的绝对好坏，而只需要知道它相对于平均水平的优势。也就是说，我们只想知道一个行动的相对 优势 。这就是优势函数的概念。

一个服从策略 \pi 的优势函数，描述的是它在状态 s 下采取行为 a 比随机选择一个行为好多少（假设之后一直服从策略 \pi ）。数学角度上，优势函数的定义为：


## 探索和利用（Exploration and Exploitation）[6]

探索率ϵ，这个比率主要用在强化学习训练迭代过程中，由于我们一般会选择使当前轮迭代价值最大的动作，但是这会导致一些较好的但我们没有执行过的动作被错过。因此我们在训练选择最优动作时，会有一定的概率ϵ不选择使当前轮迭代价值最大的动作，而选择其他的动作。[9]

假如我们去餐馆吃饭：探索就是去一个新的餐馆。利用就是在去过的餐馆中挑一个最好吃的。探索有可能找到一个更好吃的，但也有不好吃的风险。
假如我们做广告：利用就是用当下最成功的打广告方案。探索就是想一个全新的方案。

在上面的算法中，我们可以看到需要使用某一个policy来生成动作，也就是说这个policy不是优化的那个policy, 所以Q-Learning算法叫做Off-policy的算法。另一方面, 因为Q-Learning完全不考虑model模 型也就是环境的具体情况, 只考虑看到的环境及reward, 因此是model-free的方法。
回到policy的问题, 那么要选择怎样的policy来生成action呢? 有两种做法:
随机的生成一个动作
根据当前的Q值计算出一个最优的动作, 这个policy $\pi$ 称之为greedy policy, 也就是
$$
\pi\left(S_{t+1}\right)=\arg \max _{a} Q\left(S_{t+1}, a\right)
$$
使用随机的动作就是exploration, 也就是探索未知的动作会产生的效果，有利于更新Q值, 获得更好 的policy。而使用greedy policy也就是target policy则是exploitation, 利用policy, 这个相对来说就不好 更新出更好的Q值, 但可以得到更好的测试效果用于判断算法是否有效。

将两者结合起来就是所谓的 $\epsilon-$ greedy policy, $\epsilon$ 一般是一个很小的值, 作为选取随机动作的概率值。 可以更改 $\epsilon$ 的值从而得到不同的exploration和exploitation的比例。


##  Baseline的使用

Q：我们最后是根据 $R\left(\tau^{n}\right)$ 的正负来调整 $\theta,$ 但如果在某个游戏当中, 所有的Reward都是正数该怎么 解决? (Space invader其实就属于这种情况, 因为这个游戏的分数永远不会是负值) A：使用如下的式子来计算 $\nabla \bar{R}_{\theta}$ :
$$
\nabla \bar{R}_{\theta} \approx \frac{1}{N} \sum_{n=1}^{N} \sum_{t=1}^{T_{n}}\left(R\left(\tau^{n}\right)-b\right) \nabla \log p\left(a_{t}^{n} \mid s_{t}^{n}, \theta\right)
$$
除了给 $R\left(\tau^{n}\right)$ 减去一个b以外没有其他的不同, 这个b让 $R\left(\tau^{n}\right)$ 的值有正有负，我们将它成为 Baseline, Baseline的值是自己定义的)

## challenge[4]

• Humans can learn incredibly quickly
• Deep RL methods are usually slow • Humans can reuse past knowledge
• Transfer learning in deep RL is an open problem
• Not clear what the reward function should be
• Not clear what the role of prediction should be

## RL监督信息的来源

- 模仿的学习方式（imitation learning）
  - 从专家示教中学习（Learning from demonstration）
    - 通过SL来克隆label过的专家行为（behavior cloning）
    - 通过IRL从demonstration中推断专家行为背后的reward function（IRL）
- 观察环境的学习方式 (Learning from observing the world)
  - 从环境中无意识地获取数据中的结构信息
    - 无监督学习发掘环境中的结构信息(Unsupervised Learning)
    - 根据结构信息对环境的动态变化进行某种预测（Learning to predict)
- 借鉴经验的学习方式 (Learning from other tasks)
  - 迁移其它任务的经验到本任务中来 (transfer learning)
  - 从其它任务的经验中习得学习方法，再利用该学习方法对本任务进行学习 (meta learning = learning to learn)

### 深度强化学习

强化学习是有一定的历史的，早期的强化学习，我们称其为标准强化学习。最近业界把强化学习与深度学习结合起来，就形成了深度强化学习（deep reinforcemet learning），因此，深度强化学习 = 深度学习 + 强化学习。


深度强化学习中最常见的两种随机策略是 绝对策略 (Categorical Policies) 和 对角高斯策略 (Diagonal Gaussian Policies)。

策略 智能体的策略（Policy）就是智能体如何根据环境状态𝑠来决定下一步的 动作𝑎，即从状态空间到行为空间的一个映射 $\pi: S \rightarrow A$；通常可以分为确定性策略（Deterministic Policy）和随机性策略（StochasticPolicy）两种[2]

而对于一个给定的策略，其值函数 (Value function) 为:

- $V^{\pi}\left(s_{1}\right)=E\left[R\left(s_{1}\right)+\gamma R\left(s_{2}\right)+\gamma^{2} R\left(s_{3}\right)+\cdots \mid \pi\right]$

其中，所有状态服从某个状态序列 $\left(s_{1}, s_{2}, s_{3}, \ldots\right)$ 。这个序列是我们使用该策略进行决策时, 从 s1开始走过的状态路径。
另外, 也给出质量函数（Quality function） 的定义：

- $Q^{\pi}(s, a)=R(s)+\gamma E_{s^{\prime} \sim P_{s a}(\cdot)}\left[V^{\pi}\left(s^{\prime}\right)\right]$

其中, $\quad s^{\prime} \sim P_{s a}(\cdot)$ 的意思是下一个状态s'服从分布 $P_{s a}(\cdot)$ 。
进而，最优值函数 $V^{*}(s)=s u p_{\pi} V^{\pi}(s)$
且, 最优质量函数 $Q^{*}(s, a)=\sup _{\pi} Q^{\pi}(s, a)$

对于有限离散状态空间 $S=\left\{s_{1}, s_{2}, \ldots, s_{n}\right\}, \quad R, V^{\pi}, P_{a}$ 都可以用向量来表示，依次变成n维向 量、n维向量和n $\times$ n矩阵。最后，我们用 $\preceq$ 和 $\prec$ 来表示严格和非严格向量不等式, 比如, $x \prec y$ 成立意味着 $\forall i, x_{i}<y_{i}$ 。

## RL面临的一些挑战[10]

人学习的速度很快，无论是模仿还是借鉴，但deep RL却很慢？
借鉴的学习方式在deep RL中是一个open problem，经验该如何借鉴才能学起来更快速有效呢？
每个人的行为action动机背后的奖赏函数都不太一样，所以很明显reward function是可变的，那该如何表示单任务或多任务中的reward function呢？
观察环境，根据环境的结构信息进行某种预测（prediction），从而辅助决策，那prediction究竟是什么？如何表示？作用显著吗？

和深度学习类似，强化学习中的关键问题也是贡献度分配问题[Minsky, 1961]，每一个动作*并不能直接得到监督信息*，需要通过整个模型的最终监督信息（奖励）得到，并且有一定的延时性。
![主流的分类](img/fenlei.jpg)

### 贡献度分配问题（ChatGPT）

贡献度分配问题通常是指在一个团队或组织中，如何公平地分配成员的贡献度或奖励。这个问题通常出现在以下情况：

- 团队成员贡献度不均：有些成员可能比其他成员更努力、更出色，因此他们应该获得更多的奖励或认可。
- 目标不明确：如果团队的目标没有明确规定，那么很难确定哪些成员做出了最大的贡献。
- 成员间合作度不够：如果团队中的成员没有足够的合作精神，那么他们可能会互相指责或争夺奖励。

解决这个问题的一个方法是

1. 建立一个公平的评估系统，该系统应该明确规定团队目标和每个成员的职责，并且应该评估每个成员的贡献。评估可以基于量化的指标，如销售额、产品质量、生产率等等。如果评估是主观的，那么评估者应该是经过培训和有资格的人员。
2. 团队应该制定一个公平的奖励机制，该机制应该与成员的贡献成比例。例如，如果一个成员做出了更大的贡献，那么他应该获得更多的奖励。奖励可以是奖金、晋升、荣誉称号等等。
3. 为了避免不必要的纠纷，团队应该明确规定评估和奖励机制，并将其在团队内公布，让每个成员都了解规则和标准。

### 在强化学习中（ChatGPT）

在强化学习中，贡献度分配问题通常指的是如何公平地分配奖励或信用，以表彰在任务中表现出色的智能体。通常情况下，这种情况会出现在多个智能体协作完成任务的情况下。

在强化学习中，一个标准的解决方案是使用合作博弈论的概念，为每个智能体分配一个所谓的“Shapley值”。Shapley值是一个用于度量参与博弈的每个参与者所作出的贡献的方法，该值是按照参与者参与的不同子集计算得出的。

为了计算每个智能体的Shapley值，需要首先确定每个智能体参与了多少个子集，并根据其参与的每个子集的收益计算出该子集对该智能体的边际贡献。然后可以计算出每个智能体的Shapley值，即通过将其边际贡献相加，并将所有智能体的边际贡献进行平均化得出的值。

虽然这种方法可以用于分配奖励或信用，但它对计算资源和计算时间的需求很高，因此在某些情况下可能不实际。此外，对于某些任务，特别是非合作任务，可能没有一个清晰的Shapley值分配方案。因此，在应用强化学习解决贡献度分配问题时，需要根据具体情况选择最适合的方法。




- 预测问题：即给定强化学习的5个要素：状态集$S$, 动作集$A$, 即时奖励$R$，衰减因子$γ$, 给定策略$π$， 求解该策略的状态价值函数$V^\pi(S)$。
- 控制问题：也就是求解最优的价值函数和策略。给定强化学习的5个要素：状态集$S$, 动作集$A$, 即时奖励$R$，衰减因子$γ$, 探索率$ϵ$, 求解最优的动作价值函数$q^*$, - 和最优策略$\pi^*$。[^16]


[1]: https://spinningup.readthedocs.io/zh_CN/latest/spinningup/rl_intro.html#bellman-equations
[2]: https://weread.qq.com/web/reader/62332d007190b92f62371aek92c3210025c92cc22753209
[3]: https://easyai.tech/ai-definition/reinforcement-learning/
[4]: http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf
[5]: https://github.com/NLP-LOVE/ML-NLP/tree/master/Deep%20Learning/14.%20Reinforcement%20Learning
[6]: https://zhuanlan.zhihu.com/p/316339517
[7]: https://rl.qiwihui.com/zh_CN/latest/chapter1/introduction.html#id4
[8]: https://github.com/applenob/rl_learn/blob/master/class_note.ipynb
[9]: https://www.cnblogs.com/pinard/p/9385570.html
[10]: https://blog.csdn.net/weixin_40056577/article/details/104109073
[11]: https://tianshou.readthedocs.io/zh/latest/docs/2-impl.html#id31
[12]: https://nndl.github.io/ 的ch14
[13]: https://anesck.github.io/M-D-R_learning_notes/RLTPI/notes_html/1.chapter_one.html
[14]: https://echenshe.com/class/ml-intro/4-02-RL-methods.html
[15]: https://blog.csdn.net/weixin_42022175/article/details/99676753
[16]: https://blog.csdn.net/weixin_42022175/article/details/99676753
[17]: https://blog.csdn.net/Hansry/article/details/80808097
[18]: https://spinningup.qiwihui.com/zh_CN/latest/spinningup/rl_intro.html
[19]: https://www.huoban.com/news/post/2237.html
[20]: https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/6171383?fromModule=search-result_lemma-recommend
[21]: http://www.deeprlhub.com/d/722/42
[22]: https://baike.baidu.com/item/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/2971075
[23]: https://baike.baidu.com/item/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/810193?fromModule=lemma_search-box
[24]: https://leovan.me/cn/2020/05/introduction-of-reinforcement-learning/
[25]: https://blog.sciencenet.cn/blog-2374-1351757.html
[26]: https://zh.wikipedia.org/wiki/%E6%93%8D%E4%BD%9C%E5%88%B6%E7%B4%84
[27]: https://blog.sciencenet.cn/blog-3189881-1122463.html
[28]: https://opendilab.github.io/DI-engine/02_algo/model_based_rl_zh.html
[29]: https://blog.csdn.net/qq_38962621/article/details/103951014
[30]: https://datawhalechina.github.io/easy-rl/#/chapter1/chapter1?id=_123-%e5%ba%8f%e5%88%97%e5%86%b3%e7%ad%96
[31]: https://hrl.boyuai.com/chapter/1/%E5%88%9D%E6%8E%A2%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0
